 | date | expiration | valid | name | url | domain | focus | keywords | summary | task_types | ai_capability_measured | metrics | models | notes | cite | ratings.specification.rating | ratings.specification.reason | ratings.dataset.rating | ratings.dataset.reason | ratings.metrics.rating | ratings.metrics.reason | ratings.reference_solution.rating | ratings.reference_solution.reason | ratings.documentation.rating | ratings.documentation.reason | 
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
2024-05-01 |  | yes | Jet Classification | https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify | Particle Physics | Real-time classification of particle jets using HL-LHC simulation features | classification, real-time ML, jet tagging, QKeras | This benchmark evaluates ML models for real-time classification of particle jets using high-level features derived from simulated LHC data. It includes both full-precision \nand quantized models optimized for FPGA deployment.  | Classification | Real-time inference, model compression performance | Accuracy, AUC | Keras DNN, QKeras quantized DNN | Includes both float and quantized models using QKeras | [^1] | 9.0 | Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified. | 9.0 | Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries. | 9.0 | Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation. | 8.0 | Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not bundled as an official starter kit. | 7.0 | Clear paper, GitHub repo, and integration with HuggingFace Datasets; full reproducibility requires manually connecting models to dataset. | 
2024-05-01 |  | yes | Irregular Sensor Data Compression | https://github.com/fastmachinelearning/fastml-science/tree/main/sensor-data-compression | Particle Physics | Real-time compression of sparse sensor data with autoencoders | compression, autoencoder, sparse data, irregular sampling | This benchmark addresses lossy compression of irregularly sampled sensor data from \nparticle detectors using real-time autoencoder architectures, targeting latency-critical \napplications in physics experiments.  | Compression | Reconstruction quality, compression efficiency | MSE, Compression ratio | Autoencoder, Quantized autoencoder | Based on synthetic but realistic physics sensor data | [^2] | 8.0 | Classification is clearly defined for real-time inference on simulated LHC jets. Input features (HLFs) are documented, though exact latency or resource constraints are not numerically specified. | 9.0 | Two datasets (OpenML and Zenodo) are public, well-formatted, and documented; FAIR principles are followed, though richer metadata would raise confidence to a 10. | 9.0 | AUC and Accuracy are standard, quantitative, and well-aligned with goals of jet tagging and inference efficiency. | 8.0 | Float and quantized Keras/QKeras models are provided with results. Reproducibility is good, though full automation and documentation could be improved. | 8.0 | GitHub contains baseline code, data loaders, and references, but setup for deployment (e.g., FPGA pipeline) requires familiarity with the tooling. | 
2024-05-01 |  | yes | Beam Control | https://github.com/fastmachinelearning/fastml-science/tree/main/beam-control | Accelerators and Magnets | Reinforcement learning control of accelerator beam position | RL, beam stabilization, control systems, simulation | Beam Control explores real-time reinforcement learning strategies for maintaining  stable beam trajectories in particle accelerators. The benchmark is based on the  BOOSTR environment for accelerator simulation.  | Control | Policy performance in simulated accelerator control | Stability, Control loss | DDPG, PPO (planned) | Environment defined, baseline RL implementation is in progress | [^3], [^4] | 9.0 | Task is well defined (real-time compression of sparse, irregular sensor data using autoencoders); latency constraints are implied but not fully quantified. | 8.0 | Dataset is custom and synthetic but described well; FAIR-compliance is partial (reusable and accessible, but not externally versioned with rich metadata). | 9.0 | Uses standard quantitative metrics (MSE, compression ratio) clearly aligned with compression and reconstruction goals. | 7.0 | Baseline (autoencoder and quantized variant) is provided, but training/inference pipeline is minimally documented and needs user setup. | 8.0 | GitHub repo contains core components, but more structured setup instructions and pretrained weights would improve usability. | 
2024-07-08 |  | yes | Ultrafast jet classification at the HL-LHC | https://arxiv.org/pdf/2402.01876 | Particle Physics | FPGA-optimized real-time jet origin classification at the HL-LHC | jet classification, FPGA, quantization-aware training, Deep Sets, Interaction Networks | Demonstrates three ML models (MLP, Deep Sets, Interaction Networks) optimized for FPGA deployment with O(100 ns) inference using quantized models and hls4ml, targeting real-time jet tagging in the L1 trigger environment at the high-luminosity LHC. Data is available on Zenodo DOI:10.5281/zenodo.3602260. :contentReference[oaicite:1]{index=1}  | Classification | Real-time inference under FPGA constraints | Accuracy, Latency, Resource utilization | MLP, Deep Sets, Interaction Network | Uses quantization-aware training; hardware synthesis evaluated via hls4ml | [^5] | 8.0 | Task is clear (RL control of beam stability), with BOOSTR-based simulator; control objectives are well motivated, but system constraints and reward structure are still under refinement. | 7.0 | BOOSTR dataset exists and is cited, but integration into the benchmark is in early stages; metadata and FAIR structure are limited. | 7.0 | Stability and control loss are mentioned, but metrics are not yet formalized with clear definitions or baselines. | 5.5 | DDPG baseline mentioned; PPO planned; implementation is still in progress with no reproducible results available yet. | 6.0 | GitHub has a defined structure but is incomplete; setup and execution instructions for training/evaluation are not fully established. | 
2024-10-15 |  | yes | Quench detection | https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf | Accelerators and Magnets | Real-time detection of superconducting magnet quenches using ML | quench detection, autoencoder, anomaly detection, real-time | Exploration of real-time quench detection using unsupervised and RL approaches, combining multi-modal sensor data (BPM, power supply, acoustic), operating on kHz-MHz streams with anomaly detection and frequency-domain features. :contentReference[oaicite:2]{index=2}  | Anomaly detection, Quench localization | Real-time anomaly detection with multi-modal sensors | ROC-AUC, Detection latency | Autoencoder, RL agents (in development) | Precursor detection in progress; multi-modal and dynamic weighting methods | [^6] | 10.0 | Real-time jet origin classification under FPGA constraints is clearly defined, with explicit latency targets (~100 ns) and I/O formats. | 9.0 | Data available on Zenodo with DOI, includes constituent-level jets; accessible and well-documented, though not deeply versioned with full FAIR metadata. | 10.0 | Accuracy, latency, and hardware resource usage (LUTs, DSPs) are rigorously measured and aligned with real-time goals. | 9.0 | Includes models (MLP, Deep Sets, Interaction Networks) with quantization-aware training and synthesis results via hls4ml; reproducible but tightly coupled with specific toolchains. | 8.0 | Paper and code (via hls4ml) are sufficient, but a centralized, standalone repo for reproducing all models would enhance accessibility. | 
2024-10-15 |  | yes | DUNE | https://indico.fnal.gov/event/66520/contributions/301423/attachments/182439/250508/fast_ml_dunedaq_sonic_10_15_24.pdf | Particle Physics | Real-time ML for DUNE DAQ time-series data | DUNE, time-series, real-time, trigger | Applying real-time ML methods to time-series data from DUNE detectors, exploring trigger-level anomaly detection and event selection with low latency constraints.  | Trigger selection, Time-series anomaly detection | Low-latency event detection | Detection efficiency, Latency | CNN, LSTM (planned) | Prototype models demonstrated on SONIC platform | [^7] | 8.0 | Task (quench detection via anomaly detection) is clearly described; multi-modal sensors, streaming rates, and objective are provided, but constraints (latency thresholds) are qualitative. | 7.0 | Custom dataset using real data from BNL; HDF5 formatted and structured, but access may be internal or limited, and not versioned for public FAIR use. | 8.0 | ROC-AUC and detection latency are defined; relevant and quantitative but not yet paired with benchmark baselines. | 6.0 | Autoencoder prototype exists; RL methods are in development; no fully reproducible pipeline is available yet. | 7.0 | Slides and GDocs outline results; implementation is in progress with limited setup/code release. | 
2025-01-08 |  | yes | Intelligent experiments through real-time AI | https://arxiv.org/pdf/2501.04845 | Instrumentation and Detectors; Nuclear Physics; Particle Physics | Real-time FPGA-based triggering and detector control for sPHENIX and future EIC | FPGA, Graph Neural Network, hls4ml, real-time inference, detector control | Resaerch and Development demonstrator for real-time processing of high-rate tracking data from the sPHENIX detector (RHIC) and future EIC systems. Uses GNNs with hls4ml for FPGA-based trigger generation to identify rare events (heavy flavor, DIS electrons) within 10 µs latency. Demonstrated improved accuracy and latency on Alveo/FELIX platforms.  | Trigger classification, Detector control, Real-time inference | Low-latency GNN inference on FPGA | Accuracy (charm and beauty detection), Latency (µs), Resource utilization (LUT/FF/BRAM/DSP) | Bipartite Graph Network with Set Transformers (BGN-ST), GarNet (edge-classifier) | Achieved ~97.4% accuracy for beauty decay triggers; sub-10 µs latency on Alveo U280; hit-based FPGA design via hls4ml and FlowGNN. | [^8] | 8.0 | Task (trigger-level anomaly detection) is clearly defined for low-latency streaming input, but the problem framing lacks complete architectural/system specs. | 6.0 | Internal DUNE SONIC data; not publicly released and no formal FAIR support; replicability is institutionally gated. | 7.0 | Metrics include detection efficiency and latency, which are relevant, but only lightly supported by baselines or formal eval scripts. | 5.0 | One CNN prototype demonstrated; LSTM planned. No public implementation or ready-to-run example yet. | 6.0 | Slides and some internal documentation exist, but no full pipeline or public GitHub repo yet. | 
2025-01-09 |  | yes | Neural Architecture Codesign for Fast Physics Applications | https://arxiv.org/abs/2501.05515 | Physics; Materials Science; Particle Physics | Automated neural architecture search and hardware-efficient model codesign for fast physics applications | neural architecture search, FPGA deployment, quantization, pruning, hls4ml | Introduces a two-stage neural architecture codesign (NAC) pipeline combining global and local search, quantization-aware training, and pruning to design efficient models for fast Bragg peak finding and jet classification, synthesized for FPGA deployment with hls4ml. Achieves >30x reduction in BOPs and sub-100 ns inference latency on FPGA.  | Classification, Peak finding | Hardware-aware model optimization; low-latency inference | Accuracy, Latency, Resource utilization | NAC-based BraggNN, NAC-optimized Deep Sets (jet) | Demonstrated two case studies (materials science, HEP); pipeline and code open-sourced. | [^9] | 10.0 | Task is clearly defined (triggering on rare events with sub-10 µs latency); architecture, constraints, and system context (FPGA, Alveo) are well detailed. | 7.0 | Simulated tracking data from sPHENIX and EIC; internally structured but not yet released in a public FAIR-compliant format. | 10.0 | Accuracy, latency, and hardware resource utilization (LUTs, DSPs) are clearly defined and used in evaluation. | 9.0 | Graph-based models (BGN-ST, GarNet) are implemented and tested on real hardware; reproducibility possible with hls4ml but full scripts not bundled. | 8.0 | Paper is detailed and tool usage (FlowGNN, hls4ml) is described, but repo release and dataset access remain in progress. | 
2024-06-24 |  | yes | Smart Pixels for LHC | https://arxiv.org/abs/2406.14860 | Particle Physics; Instrumentation and Detectors | On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors | smart pixel, on-sensor inference, data reduction, trigger | Presents a 256x256-pixel ROIC in 28 nm CMOS with embedded 2-layer NN for cluster filtering at 25 ns, achieving 54-75% data reduction while maintaining noise and latency constraints. Prototype consumes ~300 µW/pixel and operates in combinatorial digital logic.  | Image Classification, Data filtering | On-chip, low-power inference; data reduction | Data rejection rate, Power per pixel | 2-layer pixel NN | Prototype in CMOS 28 nm; proof-of-concept for Phase III pixel upgrades. | [^10] | 9.0 | Task (automated neural architecture search for real-time physics) is well formulated with clear latency, model compression, and deployment goals. | 6.0 | Internal Bragg and jet datasets used; not publicly hosted or FAIR-compliant, though mentioned in the paper. | 10.0 | BOP reduction, latency, and accuracy are all quantitatively evaluated. | 8.0 | NAC-generated models for Bragg peak and jet classification are described, but pipeline requires integration of several tools and is not fully packaged. | 7.0 | NAC pipeline, hls4ml usage, and results are discussed; code (e.g., nac-opt) referenced, but replication requires stitching together toolchain and data. | 
2023-10-03 |  | yes | HEDM BraggNN | https://arxiv.org/abs/2008.08198 | Material Science | Fast Bragg peak analysis using deep learning in diffraction microscopy | BraggNN, diffraction, peak finding, HEDM | Uses BraggNN, a deep neural network, for rapid Bragg peak localization in high-energy diffraction microscopy, achieving ~13x speedup compared to Voigt-based methods while maintaining sub-pixel accuracy.  | Peak detection | High-throughput peak localization | Localization accuracy, Inference time | BraggNN | Enables real-time HEDM workflows; basis for NAC case study. | [^11] | 10.0 | Fully specified: describes task (data filtering/classification), system design (on-sensor inference), latency (25 ns), and power constraints. | 8.0 | In-pixel charge cluster data used, but dataset release info is minimal; FAIR metadata/versioning limited. | 9.0 | Data rejection rate and power per pixel are clearly defined and directly tied to hardware goals. | 9.0 | 2-layer NN implementation is evaluated in hardware; reproducible via hls4ml flow with results in paper. | 8.0 | Paper is clear; Zenodo asset is referenced, but additional GitHub or setup repo would improve reproducibility. | 
2023-12-03 |  | yes | 4D-STEM | https://openreview.net/pdf?id=7yt3N0o0W9 | Material Science | Real-time ML for scanning transmission electron microscopy | 4D-STEM, electron microscopy, real-time, image processing | Proposes ML methods for real-time analysis of 4D scanning transmission electron microscopy datasets; framework details in progress.  | Image Classification, Streamed data inference | Real-time large-scale microscopy inference | Classification accuracy, Throughput | CNN models (prototype) | In-progress; model design under development. | [^12] | 9.0 | Peak localization task is well-defined for diffraction images; input/output described clearly, but no system constraints. | 8.0 | Simulated diffraction images provided; reusable and downloadable, but not externally versioned or FAIR-structured. | 9.0 | Inference speed and localization accuracy are standard and quantitatively reported. | 8.0 | BraggNN model and training pipeline exist, but need stitching from separate repositories. | 8.0 | Paper and codebase are available and usable, though not fully turnkey. | 
2023-12-05 |  | yes | In-Situ High-Speed Computer Vision | https://arxiv.org/abs/2312.00128 | Fusion/Plasma | Real-time image classification for in-situ plasma diagnostics | plasma, in-situ vision, real-time ML | Applies low-latency CNN models for image classification of plasma diagnostics streams; supports deployment on embedded platforms.  | Image Classification | Real-time diagnostic inference | Accuracy, FPS | CNN | Embedded/deployment details in progress. | [^13] | 7.0 | General task defined (real-time microscopy inference), but no standardized I/O format, latency constraint, or complete problem framing yet. | 0.0 | Dataset not provided or described in any formal way. | 6.0 | Mentions throughput and accuracy, but metrics are not formally defined or benchmarked. | 2.0 | Prototype CNNs described; no baseline or implementation released. | 5.0 | OpenReview paper and Gemini doc give some insight, but no working code, environment, or example. | 
2020-01-01 |  | yes | BenchCouncil AIBench | https://www.benchcouncil.org/AIBench/ | General | End-to-end AI benchmarking across micro, component, and application levels | benchmarking, AI systems, application-level evaluation | AIBench is a comprehensive benchmark suite that evaluates AI workloads at different levels (micro, component, application) across hardware systems—covering image generation, object detection, translation, recommendation, video prediction, etc. | Training, Inference, End-to-end AI workloads | System-level AI workload performance | Throughput, Latency, Accuracy | ResNet, BERT, GANs, Recommendation systems | Covers scenario-distilling, micro, component, and end-to-end benchmarks. | [^14] | 8.0 | Task (plasma diagnostic classification) and real-time deployment described; system specs (FPS targets) implied but not fully quantified. | 6.0 | Dataset is sensor stream-based but not shared or FAIR-documented. | 8.0 | FPS and classification accuracy reported and relevant. | 7.0 | CNN model described and evaluated, but public implementation and benchmarks are not available yet. | 6.0 | Paper and Gemini doc exist, but full setup instructions and tools are still in progress. | 
2020-01-01 |  | yes | BenchCouncil BigDataBench | https://www.benchcouncil.org/BigDataBench/ | General | Big data and AI benchmarking across structured, semi-structured, and unstructured data workloads | big data, AI benchmarking, data analytics | BigDataBench provides benchmarks for evaluating big data and AI workloads with realistic datasets (13 sources) and pipelines across analytics, graph, warehouse, NoSQL, streaming, and AI. | Data preprocessing, Inference, End-to-end data pipelines | Data processing and AI model inference performance at scale | Data throughput, Latency, Accuracy | CNN, LSTM, SVM, XGBoost | Built on eight data motifs; provides Hadoop, Spark, Flink, MPI implementations. | [^15] | 9.0 | Evaluates AI at multiple levels (micro to end-to-end); tasks and workloads are clearly defined, though specific I/O formats and constraints vary. | 9.0 | Realistic datasets across diverse domains; FAIR structure for many components, but individual datasets may not all be versioned or richly annotated. | 9.0 | Latency, throughput, and accuracy clearly defined for end-to-end tasks; consistent across models and setups. | 8.0 | Reference implementations for several tasks exist, but setup across all tasks is complex and not fully streamlined. | 8.0 | Central documentation exists, with detailed component breakdowns; environment setup across platforms (e.g., hardware variations) can require manual adjustment. | 
2021-10-20 |  | yes | MLPerf HPC | https://github.com/mlcommons/hpc | Cosmology, Climate, Protein Structure, Catalysis | Scientific ML training and inference on HPC systems | HPC, training, inference, scientific ML | MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow, DeepCAM) aimed at large-scale HPC evaluation with >10x performance scaling through system-level optimizations. | Training, Inference | Scaling efficiency, training time, model accuracy on HPC | Training time, Accuracy, GPU utilization | CosmoFlow, DeepCAM, OpenCatalyst | Shared framework with MLCommons Science; reference implementations included. | [^16] | 9.0 | Focused on structured/unstructured data pipelines; clearly defined tasks spanning analytics to AI; some scenarios lack hardware constraint modeling. | 9.0 | Built from 13 real-world sources; structured for realistic big data scenarios; partially FAIR-compliant with documented data motifs. | 9.0 | Covers data throughput, latency, and accuracy; quantitative and benchmark-ready. | 8.0 | Many pipeline and model examples provided using Hadoop/Spark/Flink; setup effort varies by task and platform. | 8.0 | Strong documentation with examples and task specifications; centralized support exists, but task-specific tuning may require domain expertise. | 
2023-06-01 |  | yes | MLCommons Science | https://github.com/mlcommons/science | Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD | AI benchmarks for scientific applications including time-series, imaging, and simulation | science AI, benchmark, MLCommons, HPC | MLCommons Science assembles benchmark tasks with datasets, targets, and implementations across earthquake forecasting, satellite imagery, drug screening, electron microscopy, and CFD to drive scientific ML reproducibility. | Time-series analysis, Image classification, Simulation surrogate modeling | Inference accuracy, simulation speed-up, generalization | MAE, Accuracy, Speedup vs simulation | CNN, GNN, Transformer | Joint national-lab effort under Apache-2.0 license. | [^17] | 10.0 | Scientific ML tasks (e.g., CosmoFlow, DeepCAM) are clearly defined with HPC system-level constraints and targets. | 9.0 | Public scientific datasets (e.g., cosmology, weather); used consistently, though FAIR-compliance of individual datasets varies slightly. | 10.0 | Training time, GPU utilization, and accuracy are all directly measured and benchmarked across HPC systems. | 9.0 | Reference implementations available and actively maintained; HPC setup may require domain-specific environment. | 9.0 | GitHub repo and papers provide detailed instructions; reproducibility supported across multiple institutions. | 
2021-07-05 |  | yes | LHC New Physics Dataset | https://arxiv.org/pdf/2107.02157 | Particle Physics; Real-time Triggering | Real-time LHC event filtering for anomaly detection using proton collision data | anomaly detection, proton collision, real-time inference, event filtering, unsupervised ML | A dataset of proton-proton collision events emulating a 40 MHz real-time data stream from LHC detectors, pre-filtered on electron or muon presence. Designed for unsupervised new-physics detection algorithms under latency/bandwidth constraints. | Anomaly detection, Event classification | Unsupervised signal detection under latency and bandwidth constraints | ROC-AUC, Detection efficiency | Autoencoder, Variational autoencoder, Isolation forest | Includes electron/muon-filtered background and black-box signal benchmarks; 1M events per black box. | [^18] | 7.0 | The problem (anomaly detection for new physics at LHC) is clearly described with goals and background, but lacks a formal task specification or constraints. | 8.0 | Large-scale, public dataset derived from LHC simulations; well-documented and available via Zenodo. | 7.0 | Provides AUROC, accuracy, and anomaly detection metrics but lacks standardized evaluation script. | 5.0 | Baseline models (autoencoders, GANs) are described in associated papers, but implementations vary across papers. | 6.0 | Publicly available papers and datasets with descriptions, but no unified README or training setup. | 
2023-07-17 |  | yes | MLCommons Medical AI | https://github.com/mlcommons/medical | Healthcare; Medical AI | Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data | medical AI, federated evaluation, privacy-preserving, fairness, healthcare benchmarks | The MLCommons Medical AI working group develops benchmarks, best practices, and platforms (MedPerf, GaNDLF, COFE) to accelerate robust, privacy-preserving AI development for healthcare. MedPerf enables federated testing of clinical models on diverse datasets, improving generalizability and equity while keeping data onsite :contentReference[oaicite:1]{index=1}.  | Federated evaluation, Model validation | Clinical accuracy, fairness, generalizability, privacy compliance | ROC AUC, Accuracy, Fairness metrics | MedPerf-validated CNNs, GaNDLF workflows | Open-source platform under Apache-2.0; used across 20+ institutions and hospitals :contentReference[oaicite:2]{index=2}. | [^19] | 9.0 | Diverse scientific tasks (earthquake, CFD, microscopy) with detailed problem statements and goals; system constraints not uniformly applied. | 9.0 | Domain-specific datasets (e.g., microscopy, climate); mostly public and structured, but FAIR annotations are not always explicit. | 9.0 | Task-specific metrics (MAE, speedup, accuracy) are clear and reproducible. | 9.0 | Reference models (CNN, GNN, Transformer) provided with training/evaluation pipelines. | 9.0 | Well-documented, open-sourced, and maintained with examples; strong community support and reproducibility focus. | 
2024-10-28 |  | yes | CaloChallenge 2022 | http://arxiv.org/abs/2410.21611 | LHC Calorimeter; Particle Physics | Fast generative-model-based calorimeter shower simulation evaluation | calorimeter simulation, generative models, surrogate modeling, LHC, fast simulation | The Fast Calorimeter Simulation Challenge 2022 assessed 31 generative-model submissions (VAEs, GANs, Flows, Diffusion) on four calorimeter shower datasets; benchmarking shower quality, generation speed, and model complexity :contentReference[oaicite:3]{index=3}.  | Surrogate modeling | Simulation fidelity, speed, efficiency | Histogram similarity, Classifier AUC, Generation latency | VAE variants, GAN variants, Normalizing flows, Diffusion models | The most comprehensive survey to date on ML-based calorimeter simulation; 31 submissions over different dataset sizes. | [^20] | 9.0 | Task is clearly defined: real-time anomaly detection from high-rate LHC collisions. Latency and bandwidth constraints are mentioned, though not numerically enforced. | 9.0 | Publicly available via Zenodo, with structured signal/background splits, and rich metadata; nearly fully FAIR. | 9.0 | ROC-AUC and detection efficiency are clearly defined and appropriate for unsupervised anomaly detection. | 8.0 | Several baseline methods (autoencoder, VAE, isolation forest) are evaluated; runnable versions available via community repos but not tightly bundled. | 8.0 | Paper and data documentation are clear, and the dataset is widely reused. Setup requires some manual effort to reproduce full pipelines. | 
ongoing |  | yes | Papers With Code- SOTA Platform | https://paperswithcode.com/sota | General ML; All domains | Open platform tracking state-of-the-art results, benchmarks, and implementations across ML tasks and papers | leaderboard, benchmarking, reproducibility, open-source | Papers With Code (PWC) aggregates benchmark suites, tasks, and code across ML research: 12,423 benchmarks, 5,358 unique tasks, and 154,766 papers with code links. It tracks SOTA metrics and fosters reproducibility.  | Multiple (Classification, Detection, NLP, etc.) | Model performance across tasks (accuracy, F1, BLEU, etc.) | Task-specific (Accuracy, F1, BLEU, etc.) | All published models with code | Community-driven open platform; automatic data extraction and versioning. | [^21] | 9.0 | Evaluation setting (federated clinical benchmarking) is well-defined; I/O interfaces vary slightly by task but are standardized in MedPerf platform. | 8.0 | Uses distributed, real-world clinical datasets across institutions; FAIR compliance varies across hospitals and data hosts. | 9.0 | ROC AUC, accuracy, and fairness metrics are explicitly defined and task-dependent; consistently tracked across institutions. | 8.0 | Validated CNNs and GaNDLF pipelines are used and shared via the MedPerf tool, but some implementations are abstracted behind the platform. | 9.0 | Excellent documentation across MedPerf, GaNDLF, and COFE; reproducibility handled via containerized flows and task templates. | 
2022-01-01 |  | yes | Codabench | https://www.codabench.org/ | General ML; Multiple | Open-source platform for organizing reproducible AI benchmarks and competitions | benchmark platform, code submission, competitions, meta-benchmark | Codabench (successor to CodaLab) is a flexible, easy-to-use, reproducible API platform for hosting AI benchmarks and code-submission challenges. It supports custom scoring, inverted benchmarks, and scalable public or private queues :contentReference[oaicite:1]{index=1}.  | Multiple | Model reproducibility, performance across datasets | Submission count, Leaderboard ranking, Task-specific metrics | Arbitrary code submissions | Hosts 51 public competitions, ~26 k users, 177 k submissions :contentReference[oaicite:2]{index=2} | [^22] | 0 | Not a benchmark. It's a hosting site for benchmarks. | 0 | Not a benchmark. It's a hosting site for benchmarks. | 0 | Not a benchmark. It's a hosting site for benchmarks. | 0 | Not a benchmark. It's a hosting site for benchmarks. | 0 | Not a benchmark. It's a hosting site for benchmarks. | 
2021-09-27 |  | yes | Sabath - SBI-FAIR | https://sbi-fair.github.io/docs/software/sabath/ | Systems; Metadata | FAIR metadata framework for ML-driven surrogate workflows in HPC systems | meta-benchmark, metadata, HPC, surrogate modeling | Sabath is a metadata framework from the SBI-FAIR group (UTK, Argonne, Virginia) facilitating FAIR-compliant benchmarking and surrogate execution logging across HPC systems :contentReference[oaicite:3]{index=3}.  | Systems benchmarking | Metadata tracking, reproducible HPC workflows | Metadata completeness, FAIR compliance | N/A | Developed by PI Piotr Luszczek at UTK; integrates with MiniWeatherML, AutoPhaseNN, Cosmoflow, etc. :contentReference[oaicite:4]{index=4} | [^23] | 8.0 | The benchmark defines simulation-based inference (SBI) tasks clearly with FAIR principles applied to particle physics datasets. | 8.0 | Data is well-structured for SBI and publicly available with clear licensing. | 8.0 | Includes likelihood and posterior accuracy; metrics well-matched to SBI. | 7.0 | Baseline SBI models are implemented and reproducible. | 6.0 | GitHub repo includes code and instructions, but lacks full tutorials or walkthroughs. | 
2022-10-13 |  | yes | PDEBench | https://github.com/pdebench/PDEBench | CFD; Weather Modeling | Benchmark suite for ML-based surrogates solving time-dependent PDEs | PDEs, CFD, scientific ML, surrogate modeling, NeurIPS | PDEBench offers forward/inverse PDE tasks with large ready-to-use datasets and baselines (FNO, U-Net, PINN), packaged via a unified API. It won the SimTech Best Paper Award 2023 :contentReference[oaicite:5]{index=5}.  | Supervised Learning | Time-dependent PDE modeling; physical accuracy | RMSE, boundary RMSE, Fourier RMSE | FNO, U-Net, PINN, Gradient-Based inverse methods | Datasets hosted on DaRUS (DOI:10.18419/darus-2986); contact maintainers by email :contentReference[oaicite:6]{index=6} | [^24] | 9.0 | Clearly defined PDE-solving tasks with well-specified constraints and solution formats. | 9.0 | Includes synthetic and real-world PDE datasets with detailed format descriptions. | 8.0 | Uses L2 error and other norms relevant to PDE solutions. | 7.0 | Includes baseline solvers and trained models across multiple PDE tasks. | 8.0 | Well-organized GitHub with examples, dataset loading scripts, and training configs. | 
2024-12-03 |  | yes | The Well | https://polymathic-ai.org/the_well/ | biological systems, fluid dynamics, acoustic scattering, astrophysical MHD | Foundation model + surrogate dataset spanning 16 physical simulation domains | surrogate modeling, foundation model, physics simulations, spatiotemporal dynamics | A 15 TB collection of ML-ready physics simulation datasets (HDF5), covering 16 domains—from biology to astrophysical magnetohydrodynamic simulations—with unified API and metadata. Ideal for training surrogate and foundation models on scientific data. :contentReference[oaicite:1]{index=1}  | Supervised Learning | Surrogate modeling, physics-based prediction | Dataset size, Domain breadth | FNO baselines, U-Net baselines | Includes unified API and dataset metadata; see 2025 NeurIPS paper for full benchmark details. Size: 15 TB. :contentReference[oaicite:2]{index=2} | [^25] | 7.0 | Explores LLM understanding of mental health scenarios; framing is creative but loosely defined. | 6.0 | Dataset is described in concept but not released; privacy limits public access though synthetic proxies are referenced. | 7.0 | Uses manual annotation and quality scores, but lacks standardized automatic metrics. | 6.0 | Provides few-shot prompt examples and human rating calibration details. | 5.0 | Paper gives use cases, but code and data are not yet public. | 
2024-10-31 |  | yes | LLM-Inference-Bench | https://github.com/argonne-lcf/LLM-Inference-Bench | LLM; HPC/inference | Hardware performance benchmarking of LLMs on AI accelerators | LLM, inference benchmarking, GPU, accelerator, throughput | A suite evaluating inference performance of LLMs (LLaMA, Mistral, Qwen) across diverse accelerators (NVIDIA, AMD, Intel, SambaNova) and frameworks (vLLM, DeepSpeed-MII, etc.), with an interactive dashboard and per-platform metrics. :contentReference[oaicite:3]{index=3}  | Inference Benchmarking | Inference throughput, latency, hardware utilization | Token throughput (tok/s), Latency, Framework-hardware mix performance | LLaMA-2-7B, LLaMA-2-70B, Mistral-7B, Qwen-7B | Licensed under BSD-3, maintained by Argonne; supports GPUs and accelerators. :contentReference[oaicite:4]{index=4} | [^26] | 9.0 | PDE tasks (forward/inverse) and I/O structures are clearly specified with detailed PDE context and constraints. | 10.0 | Hosted via DaRUS with a DOI, well-documented, versioned, and FAIR-compliant. | 9.0 | Uses RMSE variants and Fourier-based errors. | 10.0 | Baselines (FNO, U-Net, PINN) implemented and ready-to-run; strong community adoption. | 9.0 | Clean GitHub with usage, dataset links, and tutorial notebooks. | 
2023-12-12 |  | yes | SGLang Framework | https://github.com/sgl-project/sglang/tree/main/benchmark | LLM Vision | Fast serving framework for LLMs and vision-language models | LLM serving, vision-language, RadixAttention, performance, JSON decoding | A high-performance open-source serving framework combining efficient backend runtime (RadixAttention, batching, quantization) and expressive frontend language, boosting LLM/VLM inference throughput up to ~3x over alternatives. :contentReference[oaicite:5]{index=5}  | Model serving framework | Serving throughput, JSON/task-specific latency | Tokens/sec, Time-to-first-token, Throughput gain vs baseline | LLaVA, DeepSeek, Llama | Deployed in production (xAI, NVIDIA, Google Cloud); v0.4.8 release June 2025. :contentReference[oaicite:6]{index=6} | [^27] | 8.0 | Clearly framed around surrogate learning across 16 domains, but not all tasks are formally posed or constrained in a unified benchmark protocol. Paper mentions performance on NVIDIA H100. | 9.0 | FAIR-compliant physics simulation dataset, structured in HDF5 with unified metadata. | 7.0 | Metrics like dataset size and domain coverage are listed, but standardized quantitative model evaluation metrics (e.g., RMSE, MAE) are not enforced. | 9.0 | FNO and U-Net baselines available; full benchmarking implementations pending NeurIPS paper code release. | 10.0 | Site and GitHub offer a unified API, metadata standards, and dataset loading tools; NeurIPS paper adds detailed design context. | 
2023-09-12 |  | yes | vLLM Inference and Serving Engine | https://github.com/vllm-project/vllm/tree/main/benchmarks | LLM; HPC/inference | High-throughput, memory-efficient inference and serving engine for LLMs | LLM inference, PagedAttention, CUDA graph, streaming API, quantization | vLLM is a fast, high-throughput, memory-efficient inference and serving engine for large language models,  featuring PagedAttention, continuous batching, and support for quantized and pipelined model execution.  Benchmarks compare it to TensorRT-LLM, SGLang, and others. :contentReference[oaicite:1]{index=1}  | Inference Benchmarking | Throughput, latency, memory efficiency | Tokens/sec, Time to First Token (TTFT), Memory footprint | LLaMA, Mixtral, FlashAttention-based models | Incubated by LF AI and Data; achieves up to 24x throughput over HuggingFace Transformers | [^28] | 9.0 | Benchmarks hardware performance of LLM inference across multiple platforms with well-defined input/output and platform constraints. | 7.0 | Uses structured log files and configs instead of conventional datasets; suitable for inference benchmarking. | 9.0 | Clear throughput, latency, and utilization metrics; platform comparison dashboard enhances evaluation. | 8.0 | Includes reproducible scripts and example runs; models like LLaMA and Mistral are referenced with platform-specific configs. | 8.0 | GitHub contains clear instructions, platform details, and framework comparisons. | 
2022-06-22 |  | yes | vLLM Performance Dashboard | https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/ | LLM; HPC/inference | Interactive dashboard showing inference performance of vLLM | Dashboard, Throughput visualization, Latency analysis, Metric tracking | A live visual dashboard for vLLM showcasing throughput, latency, and other inference metrics across models and hardware configurations.  | Performance visualization | Throughput, latency, hardware utilization | Tokens/sec, TTFT, Memory usage | LLaMA-2, Mistral, Qwen | Built using ObservableHQ; integrates live data from vLLM benchmarks. | [^29] | 8.0 | Framed as a model-serving tool rather than a benchmark, but includes benchmark configurations and real model tasks. | 6.0 | Mostly uses dummy configs or external model endpoints for evaluation; not designed around a formal dataset. | 8.0 | Well-defined serving metrics: tokens/sec, time-to-first-token, and gain over baselines. | 9.0 | Core framework includes full reproducible serving benchmarks and code; multiple deployment case studies. | 9.0 | High-quality usage guides, examples, and performance tuning docs. | 
2022-04-01 |  | yes | Nixtla NeuralForecast | https://github.com/Nixtla/neuralforecast | Time-series forecasting; General ML | High-performance neural forecasting library with >30 models | time-series, neural forecasting, NBEATS, NHITS, TFT, probabilistic forecasting, usability | NeuralForecast offers scalable, user-friendly implementations of over 30 neural forecasting models (NBEATS, NHITS, TFT, DeepAR, etc.), emphasizing quality, usability, interpretability, and performance. :contentReference[oaicite:3]{index=3}  | Time-series forecasting | Forecast accuracy, interpretability, speed | RMSE, MAPE, CRPS | NBEATS, NHITS, TFT, DeepAR | AutoModel supports hyperparameter tuning and distributed execution via Ray and Optuna. First official NHITS implementation. | [^30] | 9.0 | Targets high-throughput LLM inference via PagedAttention and memory-optimized serving; benchmarks cover many configs. | 7.0 | Focuses on model configs and streaming input/output pipelines rather than classical datasets. | 9.0 | Strong token/sec, memory usage, and TTFT metrics; comparative plots and logs included. | 9.0 | Benchmarks reproducible via script with support for multiple models and hardware types. | 9.0 | Excellent GitHub docs, CLI/API usage, and deployment walkthroughs. | 
2023-06-01 |  | yes | Nixtla Neural Forecast NHITS | https://github.com/Nixtla/neuralforecast | Time-series; General ML | Official NHITS implementation for long-horizon time series forecasting | NHITS, long-horizon forecasting, neural interpolation, time-series | NHITS (Neural Hierarchical Interpolation for Time Series) is a state-of-the-art model that improved accuracy by ~25% and reduced compute by 50x compared to Transformer baselines, using hierarchical interpolation and multi-rate sampling :contentReference[oaicite:1]{index=1}.  | Time-series forecasting | Accuracy, compute efficiency for long series | RMSE, MAPE | NHITS | Official implementation in NeuralForecast, included since its AAAI 2023 release. | [^31] | 7.0 | Primarily a visualization frontend; underlying benchmark definitions come from vLLM project. | 6.0 | No traditional dataset; displays live or logged benchmark metrics. | 9.0 | Live throughput, memory, latency, and TTFT displayed interactively; highly informative for performance analysis. | 7.0 | Dashboard built on vLLM benchmarks but not itself a complete experiment package. | 8.0 | Observable notebooks are intuitive; customization instructions are minimal but UI is self-explanatory. | 
2023-10-03 |  | yes | Nixtla Neural Forecast TimeLLM | https://github.com/Nixtla/neuralforecast | Time-series; General ML | Reprogramming LLMs for time series forecasting | Time-LLM, language model, time-series, reprogramming | Time-LLM uses reprogramming layers to adapt frozen LLMs for time series forecasting, treating forecasting as a language task :contentReference[oaicite:2]{index=2}.  | Time-series forecasting | Model reuse via LLM, few-shot forecasting | RMSE, MAPE | Time-LLM | Fully open-source; transforms forecasting using LLM text reconstruction. | [^32] | 7.0 | Describes forecasting with LLMs, but less formal on input/output or task framing. | 6.0 | Uses open time series datasets, but lacks a consolidated data release or splits. | 7.0 | Reports metrics like MASE and SMAPE, standard in forecasting. | 6.0 | Provides TimeLLM with open source, but no other baselines included. | 6.0 | GitHub readme with installation and example usage; lacks API or extensive tutorials. | 
2023-10-05 |  | yes | Nixtla Neural Forecast TimeGPT | https://github.com/Nixtla/neuralforecast | Time-series; General ML | Time-series foundation model "TimeGPT" for forecasting and anomaly detection | TimeGPT, foundation model, time-series, generative model | TimeGPT is a transformer-based generative pretrained model on 100B+ time series data for zero-shot forecasting and anomaly detection via API :contentReference[oaicite:3]{index=3}.  | Time-series forecasting, Anomaly detection | Zero-shot forecasting, anomaly detection | RMSE, Anomaly detection metrics | TimeGPT | Offered via Nixtla API and Azure Studio; enterprise-grade support available. | [^33] | 7.0 | Describes forecasting with LLMs, but less formal on input/output or task framing. | 6.0 | Uses open time series datasets, but lacks a consolidated data release or splits. | 7.0 | Reports metrics like MASE and SMAPE, standard in forecasting. | 6.0 | Provides TimeLLM with open source, but no other baselines included. | 6.0 | GitHub readme with installation and example usage; lacks API or extensive tutorials. | 
2025-03-03 |  | yes | HDR ML Anomaly Challenge- Gravitational Waves | https://www.codabench.org/competitions/2626/ | Astrophysics; Time-series | Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets | anomaly detection, gravitational waves, astrophysics, time-series | A benchmark for detecting anomalous transient gravitational-wave signals, including “unknown-unknowns,” using preprocessed LIGO time-series at 4096 Hz. Competitors submit inference models on Codabench for continuous 50 ms segments from dual interferometers. :contentReference[oaicite:1]{index=1}  | Anomaly detection | Novel event detection in physical signals | ROC-AUC, Precision/Recall | Deep latent CNNs, Autoencoders | NSF HDR A3D3 sponsored; prize pool and starter kit provided on Codabench. :contentReference[oaicite:2]{index=2} | [^34] | 8.0 | Novel approach treating forecasting as text generation is explained; framing is less conventional. | 9.0 | Compatible with standard forecasting datasets (e.g., M4, electricity). | 8.0 | RMSE and MAPE are included, but less emphasis on interpretability or time-series domain constraints. | 9.0 | Open-source with reprogramming layers, LLM interface scripts provided. | 8.0 | Model and architecture overview present, though usability guide is slightly lighter than others. | 
2025-03-03 |  | yes | HDR ML Anomaly Challenge- Butterfly | https://www.codabench.org/competitions/3764/ | Genomics; Image/CV | Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset | anomaly detection, computer vision, genomics, butterfly hybrids | Image-based challenge for detecting butterfly hybrids in microscopy-driven species data. Participants evaluate models on Codabench using image segmentation/classification. :contentReference[oaicite:3]{index=3}  | Anomaly detection | Hybrid detection in biological systems | Classification accuracy, F1 score | CNN-based detectors | Hybrid detection benchmarks hosted on Codabench. :contentReference[oaicite:4]{index=4} | [^35] | 8.0 | Task of detecting rare anomalies in butterfly physics is well-described with physics motivation. | 7.0 | Real detector data with injected anomalies is available, but requires NDA for full access. | 7.0 | Uses ROC, F1, and anomaly precision, standard in challenge evaluations. | 4.0 | Partial baselines described, but no codebase or reproducible runs. | 6.0 | Challenge site includes overview and metrics, but limited in walkthrough or examples. | 
2025-03-03 |  | yes | HDR ML Anomaly Challenge- Sea Level Rise | https://www.codabench.org/competitions/3223/ | Climate Science; Time-series, Image/CV | Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery | anomaly detection, climate science, sea-level rise, time-series, remote sensing | A challenge combining North Atlantic sea-level time-series and satellite imagery to detect flooding anomalies. Models submitted via Codabench. :contentReference[oaicite:5]{index=5}  | Anomaly detection | Detection of environmental anomalies | ROC-AUC, Precision/Recall | CNNs, RNNs, Transformers | Sponsored by NSF HDR; integrates sensor and satellite data. :contentReference[oaicite:6]{index=6} | [^36] | 9.0 | Clear anomaly detection objective framed for physical signal discovery (LIGO/Virgo). | 10.0 | Preprocessed waveform data from dual interferometers, public and well-structured. | 9.0 | ROC-AUC, Precision/Recall, and confusion-based metrics are standardized. | 1.0 | No starter model or baseline code linked | 9.0 | Codabench page, GitHub starter kit, and related papers provide strong guidance. | 
2025-01-24 |  | yes | Single Qubit Readout on QICK System | https://github.com/fastmachinelearning/ml-quantum-readout | Quantum Computing | Real-time single-qubit state classification using FPGA firmware | qubit readout, hls4ml, FPGA, QICK | Implements real-time ML models for single-qubit readout on the Quantum Instrumentation Control Kit (QICK), using hls4ml to deploy quantized neural networks on RFSoC FPGAs. Offers high-fidelity, low-latency quantum state discrimination.  | Classification | Single-shot fidelity, inference latency | Accuracy, Latency | hls4ml quantized NN | Achieves ~96% fidelity with ~32 ns latency and low FPGA resource utilization. :contentReference[oaicite:1]{index=1} | [^37] | 8.0 | Task clearly framed around detecting hybrid species via images, but exact labeling methods and hybrid definitions may need elaboration. | 8.0 | Dataset hosted on Codabench; appears structured but details on image sourcing and labeling pipeline are limited. | 9.0 | Classification accuracy and F1 are standard and appropriate. | 1.0 | No starter model or baseline code linked | 7.5 | Codabench task page describes dataset and evaluation method but lacks full API/docs. | 
2023-11-20 |  | yes | GPQA A Graduate Level Google Proof Question and Answer Benchmark | https://arxiv.org/abs/2311.12022 | Science (Biology, Physics, Chemistry) | Graduate-level, expert-validated multiple-choice questions hard even with web access | Google-proof, multiple-choice, expert reasoning, science QA | Contains 448 challenging questions written by domain experts, with expert accuracy at 65% (74% discounting clear errors) and non-experts reaching just 34%. GPT-4 baseline scores ~39%—designed for scalable oversight evaluation.  | Multiple choice | Scientific reasoning, knowledge probing | Accuracy | GPT-4 baseline | “Google-proof”; supports oversight research. | [^38] | 9.0 | Clear dual-modality task (image + time-series); environmental focus is well described. | 9.0 | Time-series and satellite imagery data provided; sensor info and collection intervals are explained. | 9.0 | ROC-AUC, Precision/Recall are appropriate and robust. | 1.0 | No starter model or baseline code linked | 6.5 | Moderate Codabench documentation with climate context; lacks pipeline-level walkthrough. | 
2024-12-13 |  | yes | SeafloorAI | https://neurips.cc/virtual/2024/poster/97432 | Marine Science; Vision-Language | Large-scale vision-language dataset for seafloor mapping and geological classification | sonar imagery, vision-language, seafloor mapping, segmentation, QA | A first-of-its-kind dataset covering 17,300 sq km of seafloor with 696K sonar images, 827K segmentation masks, and 696K natural-language descriptions plus ~7M QA pairs—designed for both vision and language-based ML models in marine science  | Image segmentation, Vision-language QA | Geospatial understanding, multimodal reasoning | Segmentation pixel accuracy, QA accuracy | SegFormer, ViLT-style multimodal models | Data processing code publicly available, covering five geological layers; curated with marine scientists :contentReference[oaicite:2]{index=2}. | [^39] | 9.0 | Real-time qubit classification task clearly defined in quantum instrumentation context. | 9.0 | Dataset available on Zenodo with signal traces; compact and reproducible. | 9.0 | Accuracy and latency are well defined and crucial in this setting. | 9.0 | GitHub repo has reproducible code and HLS firmware targeting FPGA. | 8.0 | Good setup instructions, but no interactive visualization or starter notebook. | 
2024-12-13 |  | yes | SuperCon3D | https://neurips.cc/virtual/2024/poster/97553 | Materials Science; Superconductivity | Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures | superconductivity, crystal structures, equivariant GNN, generative models | SuperCon3D introduces 3D crystal structures with associated critical temperatures (Tc) and two deep-learning models: SODNet (equivariant graph model) and DiffCSP-SC (diffusion generator) designed to screen and synthesize high-Tc candidates :contentReference[oaicite:3]{index=3}.  | Regression (Tc prediction), Generative modeling | Structure-to-property prediction, structure generation | MAE (Tc), Validity of generated structures | SODNet, DiffCSP-SC | Demonstrates advantage of combining ordered and disordered structural data in model design :contentReference[oaicite:4]{index=4}. | [^40] | 10.0 | Multimodal task (segmentation + natural language QA pairs);. | 10.0 | sonar imagery + masks + descriptions, georeferenced and labeled with QA | 9.0 | Pixel accuracy and QA metrics clearly defined; tasks split by modality. | 8.0 | Baseline models (SegFormer, ViLT) are cited, partial configs likely available. | 8.5 | Paper + GitHub metadata and processing details are comprehensive, though full dataset is not yet available. | 
2024-12-13 |  | yes | GeSS | https://neurips.cc/virtual/2024/poster/97816 | Scientific ML; Geometric Deep Learning | Benchmark suite evaluating geometric deep learning models under real-world distribution shifts | geometric deep learning, distribution shift, OOD robustness, scientific applications | GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access :contentReference[oaicite:5]{index=5}.  | Classification, Regression | OOD performance in scientific settings | Accuracy, RMSE, OOD robustness delta | GCN, EGNN, DimeNet++ | Includes no-OOD, unlabeled-OOD, and few-label scenarios :contentReference[oaicite:6]{index=6}. | [^41] | 9.0 | Well-defined problem (Tc prediction, generation) with strong scientific motivation (high-Tc materials), but no formal hardware constraints. | 9.0 | Includes curated 3D crystal structures and Tc data; readily downloadable and used in paper models. | 9.0 | MAE and structural validity used, well-established in materials modeling. | 8.0 | Provides two reference models (SODNet, DiffCSP-SC) with results. Code likely available post-conference. | 8.0 | Paper and poster explain design choices well; software availability confirms reproducibility but limited external documentation. | 
2024-12-13 |  | yes | Vocal Call Locator | https://neurips.cc/virtual/2024/poster/97470 | Neuroscience; Bioacoustics | Benchmarking sound-source localization of rodent vocalizations from multi-channel audio | source localization, bioacoustics, time-series, SSL | The first large-scale benchmark (767K sounds across 9 conditions) for localizing rodent vocal calls using synchronized audio and video in standard lab environments, enabling systematic evaluation of sound-source localization algorithms in bioacoustics :contentReference[oaicite:1]{index=1}.  | Sound source localization | Source localization accuracy in bioacoustic settings | Localization error (cm), Recall/Precision | CNN-based SSL models | Dataset spans real, simulated, and mixed audio; supports benchmarking across data types :contentReference[oaicite:2]{index=2}. | [^42] | 9.0 | Clear benchmark scenarios across GDL tasks under multiple real-world shift settings; OOD settings precisely categorized. | 8.0 | Scientific graph datasets provided in multiple shift regimes; standardized splits across domains. Exact format of data not specified. | 9.0 | Includes base metrics (accuracy, RMSE) plus OOD delta robustness for evaluation under shifts. | 9.0 | Multiple baselines (11 algorithms x 3 backbones) evaluated; setup supports reproducible comparison. | 2.0 | Paper, poster, and source code provide thorough access to methodology and implementation. Setup instructions and accompanying code not present. | 
2024-12-13 |  | yes | MassSpecGym | https://neurips.cc/virtual/2024/poster/97823 | Cheminformatics; Molecular Discovery | Benchmark suite for discovery and identification of molecules via MS/MS | mass spectrometry, molecular structure, de novo generation, retrieval, dataset | MassSpecGym curates the largest public MS/MS dataset with three standardized tasks—de novo structure generation, molecule retrieval, and spectrum simulation—using challenging generalization splits to propel ML-driven molecule discovery :contentReference[oaicite:3]{index=3}.  | De novo generation, Retrieval, Simulation | Molecular identification and generation from spectral data | Structure accuracy, Retrieval precision, Simulation MSE | Graph-based generative models, Retrieval baselines | Dataset~>1M spectra; open-source GitHub repo; widely cited as a go-to benchmark for MS/MS tasks :contentReference[oaicite:4]{index=4}. | [^43] | 9.0 | Focused on sound source localization for rodent vocalizations in lab settings; well-scoped. | 9.5 | 767000 annotated audio segments across diverse conditions. Minor deduction for no train/test/valid split. | 9.5 | Localization error, precision/recall used | 7.0 | CNN-based baselines referenced but unclear whether pretrained models or training code are available. | 2.0 | Poster and paper outline benchmark intent and setup; repo expected but not confirmed in dataset card. | 
2024-12-13 |  | yes | Urban Data Layer | https://neurips.cc/virtual/2024/poster/97837 | Urban Computing; Data Engineering | Unified data pipeline for multi-modal urban science research | data pipeline, urban science, multi-modal, benchmark | UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks.  | Prediction, Classification | Multi-modal urban inference, standardization | Task-specific accuracy or RMSE | Baseline regression/classification pipelines | Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models :contentReference[oaicite:6]{index=6}. | [^44] | 9.0 | Three tasks (de novo generation, retrieval, simulation) are clearly defined for MS/MS molecule discovery. | 10.0 | Over 1 million spectra with structure annotations; dataset is open-source and well-documented. | 9.0 | Task-appropriate metrics (structure accuracy, precision, MSE) are specified and used consistently. | 8.0 | Baseline models are available (graph-based and retrieval), though not exhaustive. | 9.0 | GitHub repo and poster provide code and reproducibility guidance. | 
2024-12-13 |  | yes | Delta Squared-DFT | https://neurips.cc/virtual/2024/poster/97788 | Computational Chemistry; Materials Science | Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies | density functional theory, Delta Squared-ML correction, reaction energetics, quantum chemistry | Introduces the Delta Squared-ML paradigm—using ML corrections to DFT to predict reaction energies with accuracy comparable to CCSD(T), while training on small CC datasets. Evaluated across 10 reaction datasets covering organic and organometallic transformations.  | Regression | High-accuracy energy prediction, DFT correction | Mean Absolute Error (eV), Energy ranking accuracy | Delta Squared-ML correction networks, Kernel ridge regression | Demonstrates CC-level accuracy with ~1% of high-level data. Benchmarks publicly included for reproducibility. | [^45] | 8.0 | Clear goals around unifying urban data formats and tasks (e.g., air quality prediction), though some specifics could be more formal. | 9.0 | Multi-modal data is standardized and accessible; GitHub repo available. | 8.0 | Uses common task metrics like accuracy/RMSE, though varies by task. | 7.0 | Baseline regression/classification models included. | 8.0 | Source code supports pipeline reuse, but formal evaluation splits may vary. | 
2024-12-13 |  | yes | LLMs for Crop Science | https://neurips.cc/virtual/2024/poster/97570 | Agricultural Science; NLP | Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts | crop science, prompt engineering, domain adaptation, question answering | Establishes a benchmark of 3,500 expert-annotated prompts and QA pairs covering crop traits, growth stages, and environmental interactions. Tests GPT-style LLMs on accuracy and domain reasoning using in-context, chain-of-thought, and retrieval-augmented prompts.  | Question Answering, Inference | Scientific knowledge, crop reasoning | Accuracy, F1 score | GPT-4, LLaMA-2-13B, T5-XXL | Includes examples with retrieval-augmented and chain-of-thought prompt templates; supports few-shot adaptation. | [^46] | 9.0 | The task of ML correction to DFT energy predictions is well-specified. | 9.0 | 10 public reaction datasets with DFT and CC references; well-documented. | 8.0 | Uses MAE and ranking accuracy, suitable for this task. | 8.0 | Includes both Δ²-ML and KRR baselines. | 9.0 | Public benchmarks and clear reproducibility via datasets and model code. | 
2024-12-13 |  | yes | SPIQA LLM | https://neurips.cc/virtual/2024/poster/97575 | Multimodal Scientific QA; Computer Vision | Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter performance) | multimodal QA, scientific figures, image+text, chain-of-thought prompting | A workshop version of SPIQA comparing 10 LLM adapter methods on the SPIQA benchmark with scientific diagram/questions. Highlights performance differences between chain-of-thought and end-to-end adapter models.  | Multimodal QA | Visual reasoning, scientific figure understanding | Accuracy, F1 score | LLaVA, MiniGPT-4, Owl-LLM adapter variants | Companion to SPIQA main benchmark; compares adapter strategies using same images and QA pairs. | [^47] | 6.0 | Task of QA over scientific figures is interesting but not fully formalized in input/output terms. | 6.0 | Uses SPIQA dataset with ~10 adapters; figures and questions are included, but not fully open. | 7.0 | Reports accuracy and F1; fair but no visual reasoning-specific metric. | 6.0 | 10 LLM adapter baselines; results included. | 5.0 | Poster paper and limited documentation; no reproducibility instructions. | 

[^1]: Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.
[^2]: Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.
[^3]: Diana Kafkes and Jason St. John. Boostr: a dataset for accelerator control systems. 2021. URL: https://arxiv.org/abs/2101.08359, arXiv:2101.08359.
[^4]: Javier Duarte, Nhan Tran, Ben Hawks, Christian Herwig, Jules Muhizi, Shvetank Prakash, and Vijay Janapa Reddi. Fastml science benchmarks: accelerating real-time scientific edge machine learning. 2022. URL: https://arxiv.org/abs/2207.07958, arXiv:2207.07958.
[^5]: Could not parse citation: 
[^6]: Could not parse citation: 
[^7]: Could not parse citation: 
[^8]: J. Kvapil, G. Borca-Tasciuc, H. Bossi, K. Chen, Y. Chen, Y. Corrales Morales, H. Da Costa, C. Da Silva, C. Dean, J. Durham, S. Fu, C. Hao, P. Harris, O. Hen, H. Jheng, Y. Lee, P. Li, X. Li, Y. Lin, M. X. Liu, V. Loncar, J. P. Mitrevski, A. Olvera, M. L. Purschke, J. S. Renck, G. Roland, J. Schambach, Z. Shi, N. Tran, N. Wuerfel, B. Xu, D. Yu, and H. Zhang. Intelligent experiments through real-time ai: fast data processing and autonomous detector control for sphenix and future eic detectors. 2025. URL: https://arxiv.org/abs/2501.04845, arXiv:2501.04845.
[^9]: Jason Weitz, Dmitri Demler, Luke McDermott, Nhan Tran, and Javier Duarte. Neural architecture codesign for fast physics applications. 2025. URL: https://arxiv.org/abs/2501.05515, arXiv:2501.05515.
[^10]: Benjamin Parpillon, Chinar Syal, Jieun Yoo, Jennet Dickinson, Morris Swartz, Giuseppe Di Guglielmo, Alice Bean, Douglas Berry, Manuel Blanco Valentin, Karri DiPetrillo, Anthony Badea, Lindsey Gray, Petar Maksimovic, Corrinne Mills, Mark S. Neubauer, Gauri Pradhan, Nhan Tran, Dahai Wen, and Farah Fahim. Smart pixels: in-pixel ai for on-sensor data filtering. 2024. URL: https://arxiv.org/abs/2406.14860, arXiv:2406.14860.
[^11]: Zhengchun Liu, Hemant Sharma, Jun-Sang Park, Peter Kenesei, Antonino Miceli, Jonathan Almer, Rajkumar Kettimuthu, and Ian Foster. Braggnn: fast x-ray bragg peak analysis using deep learning. 2021. URL: https://arxiv.org/abs/2008.08198, arXiv:2008.08198.
[^12]: Shuyu Qin, Joshua Agar, and Nhan Tran. Extremely noisy 4d-tem strain mapping using cycle consistent spatial transforming autoencoders. In AI for Accelerated Materials Design - NeurIPS 2023 Workshop. 2023. URL: https://openreview.net/forum?id=7yt3N0o0W9.
[^13]: Y. Wei, R. F. Forelli, C. Hansen, J. P. Levesque, N. Tran, J. C. Agar, G. Di Guglielmo, M. E. Mauel, and G. A. Navratil. Low latency optical-based mode tracking with machine learning deployed on fpgas on a tokamak. Review of Scientific Instruments, July 2024. URL: http://dx.doi.org/10.1063/5.0190354, doi:10.1063/5.0190354.
[^14]: Wanling Gao, Fei Tang, Lei Wang, Jianfeng Zhan, Chunxin Lan, Chunjie Luo, Yunyou Huang, Chen Zheng, Jiahui Dai, Zheng Cao, Daoyi Zheng, Haoning Tang, Kunlin Zhan, Biao Wang, Defei Kong, Tong Wu, Minghe Yu, Chongkang Tan, Huan Li, Xinhui Tian, Yatao Li, Junchao Shao, Zhenyu Wang, Xiaoyu Wang, and Hainan Ye. Aibench: an industry standard internet service ai benchmark suite. 2019. URL: https://arxiv.org/abs/1908.08998, arXiv:1908.08998.
[^15]: Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Daoyi Zheng, Xu Wen, Rui Ren, Chen Zheng, Xiwen He, Hainan Ye, Haoning Tang, Zheng Cao, Shujie Zhang, and Jiahui Dai. Bigdatabench: a scalable and unified big data and ai benchmark suite. 2018. URL: https://arxiv.org/abs/1802.08254, arXiv:1802.08254.
[^16]: Steven Farrell, Murali Emani, Jacob Balma, Lukas Drescher, Aleksandr Drozd, Andreas Fink, Geoffrey Fox, David Kanter, Thorsten Kurth, Peter Mattson, Dawei Mu, Amit Ruhela, Kento Sato, Koichi Shirahata, Tsuguchika Tabaru, Aristeidis Tsaris, Jan Balewski, Ben Cumming, Takumi Danjo, Jens Domke, Takaaki Fukai, Naoto Fukumoto, Tatsuya Fukushi, Balazs Gerofi, Takumi Honda, Toshiyuki Imamura, Akihiko Kasagi, Kentaro Kawakami, Shuhei Kudo, Akiyoshi Kuroda, Maxime Martinasso, Satoshi Matsuoka, Henrique Mendonça, Kazuki Minami, Prabhat Ram, Takashi Sawada, Mallikarjun Shankar, Tom St. John, Akihiro Tabuchi, Venkatram Vishwanath, Mohamed Wahib, Masafumi Yamazaki, and Junqi Yin. Mlperf hpc: a holistic benchmark suite for scientific machine learning on hpc systems. 2021. URL: https://arxiv.org/abs/2110.11466, arXiv:2110.11466.
[^17]: MLCommons Science Working Group. Mlcommons science working group benchmarks. 2023. URL: https://github.com/mlcommons/science.
[^18]: Thea Aarrestad, Ekaterina Govorkova, Jennifer Ngadiuba, Ema Puljak, Maurizio Pierini, and Kinga Anna Wozniak. Unsupervised new physics detection at 40 mhz: training dataset. June 2021. URL: https://doi.org/10.5281/zenodo.5046428, doi:10.5281/zenodo.5046428.
[^19]: Alex Karargyris, Micah J Sheller, and others. Federated benchmarking of medical artificial intelligence with medperf. Nature Machine Intelligence, 2023. URL: https://www.nature.com/articles/s42256-023-00652-2.
[^20]: Claudius Krause, Michele Faucci Giannelli, Gregor Kasieczka, Benjamin Nachman, Dalila Salamani, David Shih, Anna Zaborowska, Oz Amram, Kerstin Borras, Matthew R. Buckley, Erik Buhmann, Thorsten Buss, Renato Paulo Da Costa Cardoso, Anthony L. Caterini, Nadezda Chernyavskaya, Federico A. G. Corchia, Jesse C. Cresswell, Sascha Diefenbacher, Etienne Dreyer, Vijay Ekambaram, Engin Eren, Florian Ernst, Luigi Favaro, Matteo Franchini, Frank Gaede, Eilam Gross, Shih-Chieh Hsu, Kristina Jaruskova, Benno Käch, Jayant Kalagnanam, Raghav Kansal, Taewoo Kim, Dmitrii Kobylianskii, Anatolii Korol, William Korcari, Dirk Krücker, Katja Krüger, Marco Letizia, Shu Li, Qibin Liu, Xiulong Liu, Gabriel Loaiza-Ganem, Thandikire Madula, Peter McKeown, Isabell-A. Melzer-Pellmann, Vinicius Mikuni, Nam Nguyen, Ayodele Ore, Sofia Palacios Schweitzer, Ian Pang, Kevin Pedro, Tilman Plehn, Witold Pokorski, Huilin Qu, Piyush Raikwar, John A. Raine, Humberto Reyes-Gonzalez, Lorenzo Rinaldi, Brendan Leigh Ross, Moritz A. W. Scham, Simon Schnake, Chase Shimmin, Eli Shlizerman, Nathalie Soybelman, Mudhakar Srivatsa, Kalliopi Tsolaki, Sofia Vallecorsa, Kyongmin Yeo, and Rui Zhang. Calochallenge 2022: a community challenge for fast calorimeter simulation. 2024. URL: https://arxiv.org/abs/2410.21611, arXiv:2410.21611.
[^21]: Papers With Code. Papers with code: open machine learning benchmarks and leaderboards. 2025. URL: https://paperswithcode.com.
[^22]: Zhen Xu, Sergio Escalera, and others. Codabench: flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns, 3(7):100543, 2022. doi:10.1016/j.patter.2022.100543.
[^23]: Piotr Luszczek and others. Sabath: fair metadata technology for surrogate benchmarks. Technical Report, University of Tennessee, 2021.
[^24]: Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani, Dirk Pflüger, and Mathias Niepert. Pdebench: an extensive benchmark for scientific machine learning. 2024. URL: https://arxiv.org/abs/2210.07182, arXiv:2210.07182.
[^25]: Ruben Ohana, Michael McCabe, Lucas Meyer, and others. The well: a large-scale collection of diverse physics simulations for machine learning. NeurIPS, 37:44989–45037, 2024.
[^26]: Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale, Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali Emani, and Venkatram Vishwanath. Llm-inference-bench: inference benchmarking of large language models on ai accelerators. 2024. URL: https://arxiv.org/abs/2411.00136, arXiv:2411.00136.
[^27]: Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: efficient execution of structured language model programs. 2024. URL: https://arxiv.org/abs/2312.07104, arXiv:2312.07104.
[^28]: Woosuk Kwon and others. Efficient memory management for large language model serving with pagedattention. In SOSP 2023. 2023.
[^29]: Simon Mo. Vllm performance dashboard. 2024. URL: https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/.
[^30]: Kin G. Olivares, Cristian Challú, and others. Neuralforecast: user friendly state-of-the-art neural forecasting models. PyCon US, 2022. URL: https://github.com/Nixtla/neuralforecast.
[^31]: Cristian Challu, Kin G. Olivares, and others. Nhits: neural hierarchical interpolation for time series forecasting. In AAAI 2023. 2023.
[^32]: Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: time series forecasting by reprogramming large language models. 2024. URL: https://arxiv.org/abs/2310.01728, arXiv:2310.01728.
[^33]: Azul Garza, Cristian Challu, and Max Mergenthaler-Canseco. Timegpt-1. 2024. URL: https://arxiv.org/abs/2310.03589, arXiv:2310.03589.
[^34]: Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Saúl Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.
[^35]: Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Saúl Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.
[^36]: Elizabeth G. Campolongo, Yuan-Tang Chou, Ekaterina Govorkova, Wahid Bhimji, Wei-Lun Chao, Chris Harris, Shih-Chieh Hsu, Hilmar Lapp, Mark S. Neubauer, Josephine Namayanja, Aneesh Subramanian, Philip Harris, Advaith Anand, David E. Carlyn, Subhankar Ghosh, Christopher Lawrence, Eric Moreno, Ryan Raikman, Jiaman Wu, Ziheng Zhang, Bayu Adhi, Mohammad Ahmadi Gharehtoragh, Saúl Alonso Monsalve, Marta Babicz, Furqan Baig, Namrata Banerji, William Bardon, Tyler Barna, Tanya Berger-Wolf, Adji Bousso Dieng, Micah Brachman, Quentin Buat, David C. Y. Hui, Phuong Cao, Franco Cerino, Yi-Chun Chang, Shivaji Chaulagain, An-Kai Chen, Deming Chen, Eric Chen, Chia-Jui Chou, Zih-Chen Ciou, Miles Cochran-Branson, Artur Cordeiro Oudot Choi, Michael Coughlin, Matteo Cremonesi, Maria Dadarlat, Peter Darch, Malina Desai, Daniel Diaz, Steven Dillmann, Javier Duarte, Isla Duporge, Urbas Ekka, Saba Entezari Heravi, Hao Fang, Rian Flynn, Geoffrey Fox, Emily Freed, Hang Gao, Jing Gao, Julia Gonski, Matthew Graham, Abolfazl Hashemi, Scott Hauck, James Hazelden, Joshua Henry Peterson, Duc Hoang, Wei Hu, Mirco Huennefeld, David Hyde, Vandana Janeja, Nattapon Jaroenchai, Haoyi Jia, Yunfan Kang, Maksim Kholiavchenko, Elham E. Khoda, Sangin Kim, Aditya Kumar, Bo-Cheng Lai, Trung Le, Chi-Wei Lee, JangHyeon Lee, Shaocheng Lee, Suzan van der Lee, Charles Lewis, Haitong Li, Haoyang Li, Henry Liao, Mia Liu, Xiaolin Liu, Xiulong Liu, Vladimir Loncar, Fangzheng Lyu, Ilya Makarov, Abhishikth Mallampalli Chen-Yu Mao, Alexander Michels, Alexander Migala, Farouk Mokhtar, Mathieu Morlighem, Min Namgung, Andrzej Novak, Andrew Novick, Amy Orsborn, Anand Padmanabhan, Jia-Cheng Pan, Sneh Pandya, Zhiyuan Pei, Ana Peixoto, George Percivall, Alex Po Leung, Sanjay Purushotham, Zhiqiang Que, Melissa Quinnan, Arghya Ranjan, Dylan Rankin, Christina Reissel, Benedikt Riedel, Dan Rubenstein, Argyro Sasli, Eli Shlizerman, Arushi Singh, Kim Singh, Eric R. Sokol, Arturo Sorensen, Yu Su, Mitra Taheri, Vaibhav Thakkar, Ann Mariam Thomas, Eric Toberer, Chenghan Tsai, Rebecca Vandewalle, Arjun Verma, Ricco C. Venterea, He Wang, Jianwu Wang, Sam Wang, Shaowen Wang, Gordon Watts, Jason Weitz, Andrew Wildridge, Rebecca Williams, Scott Wolf, Yue Xu, Jianqi Yan, Jai Yu, Yulei Zhang, Haoran Zhao, Ying Zhao, and Yibo Zhong. Building machine learning challenges for anomaly detection in science. 2025. URL: https://arxiv.org/abs/2503.02112, arXiv:2503.02112.
[^37]: Giuseppe Di Guglielmo, Botao Du, Javier Campos, Alexandra Boltasseva, Akash V. Dixit, Farah Fahim, Zhaxylyk Kudyshev, Santiago Lopez, Ruichao Ma, Gabriel N. Perdue, Nhan Tran, Omer Yesilyurt, and Daniel Bowring. End-to-end workflow for machine learning-based qubit readout with qick and hls4ml. 2025. URL: https://arxiv.org/abs/2501.14663, arXiv:2501.14663.
[^38]: David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: a graduate-level google-proof q&a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022, arXiv:2311.12022.
[^39]: Kien X. Nguyen, Fengchun Qiao, Arthur Trembanis, and Xi Peng. Seafloorai: a large-scale vision-language dataset for seafloor geological survey. 2024. URL: https://arxiv.org/abs/2411.00172, arXiv:2411.00172.
[^40]: Zhong Zuo and others. Supercon3d: learning superconductivity from ordered and disordered material structures. 2024. NeurIPS Poster.
[^41]: Deyu Zou, Shikun Liu, and others. Gess: benchmarking geometric deep learning under scientific applications with distribution shifts. 2024. NeurIPS Poster.
[^42]: Ralph Peterson, Aramis Tanelus, and others. Vocal call locator benchmark for localizing rodent vocalizations. 2024. NeurIPS Poster. URL: https://neurips.cc/virtual/2024/poster/97470.
[^43]: Roman Bushuiev, Anton Bushuiev, and others. Massspecgym: a benchmark for the discovery and identification of molecules. 2024. NeurIPS Spotlight Poster. URL: https://neurips.cc/virtual/2024/poster/97823.
[^44]: Yiheng Wang, Tianyu Wang, and others. Urbandatalayer: a unified data pipeline for urban science. 2024. NeurIPS Poster. URL: https://neurips.cc/virtual/2024/poster/97837.
[^45]: Wei Liu, Rong Chen, and others. Delta squared-dft: machine-learning corrected density functional theory for reaction  energetics. 2024. NeurIPS Poster. URL: https://neurips.cc/virtual/2024/poster/97788.
[^46]: Deepak Patel, Lan Zhao, and others. Large language models for crop science: benchmarking domain reasoning and qa. 2024. NeurIPS Poster. URL: https://neurips.cc/virtual/2024/poster/97570.
[^47]: Xiaoyan Zhong, Yijian Gao, and others. Spiqa-llm: evaluating llm adapters on scientific figure qa. 2024. NeurIPS Poster. URL: https://neurips.cc/virtual/2024/poster/97575.
