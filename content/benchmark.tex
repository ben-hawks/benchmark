\begin{table}[h!]
\centering
\begin{tabular}{|l | l | l | l | l | l | l | l | l | l | l | l | l | l | l|}
\hline
date & expiration & valid & name & url & domain & focus & keyword & description & task_types & ai_capability_measured & metrics & models & notes & cite \\ \hline
2020-09-07 & None & yes & MMLU (Massive Multitask Language Understanding) & https://paperswithcode.com/dataset/mmlu & Multidomain & Academic knowledge and reasoning across 57 subjects & ['multitask', 'multiple-choice', 'zero-shot', 'few-shot', 'knowledge probing'] & Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 
multiple-choice tasks covering elementary mathematics, US history, computer science, 
law, and more, designed to evaluate a model’s breadth and depth of knowledge in 
zero-shot and few-shot settings.
 & ['Multiple choice'] & General reasoning, subject-matter understanding & ['Accuracy'] & ['GPT-4o', 'Gemini 1.5 Pro', 'o1', 'DeepSeek-R1'] & Good & ['@article{hendrycks2021measuring, title={Measuring Massive Multitask Language Understanding}, author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and others}, journal={arXiv preprint arXiv:2009.03300}, year={2021}, url={https://arxiv.org/abs/2009.03300} }'] \\ \hline
2023-11-20 & None & yes & GPQA Diamond & https://arxiv.org/abs/2311.12022 & Science & Graduate-level scientific reasoning & ['Google-proof', 'graduate-level', 'science QA', 'chemistry', 'physics'] & GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,
and chemistry, written by domain experts. It is “Google-proof”—experts score 65 percent 
(74 percent after error correction) while skilled non-experts with web access score only 34 percent. 
State-of-the-art LLMs like GPT-4 reach around 39 percent accuracy.
 & ['Multiple choice', 'Multi-step QA'] & Scientific reasoning, deep knowledge & ['Accuracy'] & ['o1', 'DeepSeek-R1'] & Good & ['@misc{rein2023gpqagraduatelevelgoogleproofqa, title={GPQA: A Graduate-Level Google-Proof Q&A Benchmark}, author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and others}, year={2023}, url={https://arxiv.org/abs/2311.12022} }'] \\ \hline
2018-03-14 & None & yes & ARC-Challenge (Advanced Reasoning Challenge) & https://allenai.org/data/arc & Science & Grade-school science with reasoning emphasis & ['grade-school', 'science QA', 'challenge set', 'reasoning'] & The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school
science questions that retrieval-based and word co-occurrence algorithms both fail, 
requiring advanced reasoning over a 14-million-sentence corpus.
 & ['Multiple choice'] & Commonsense and scientific reasoning & ['Accuracy'] & ['GPT-4', 'Claude'] & Good & ['@inproceedings{clark2018think, title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge}, author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and others}, booktitle={EMNLP 2018}, pages={237–248}, year={2018}, url={https://allenai.org/data/arc} }'] \\ \hline
2025-01-24 & None & yes & Humanity's Last Exam & https://arxiv.org/abs/2501.14249 & Multidomain & Broad cross-domain academic reasoning & ['cross-domain', 'academic exam', 'multiple-choice', 'multidisciplinary'] & Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000
questions across diverse academic disciplines, designed to evaluate LLMs' ability to
reason across domains without external resources.
 & ['Multiple choice'] & Cross-domain academic reasoning & ['Accuracy'] & [] & Good & ["@misc{phan2025humanitys, title={Humanity's Last Exam}, author={Phan, Long and Gatti, Alice and Han, Ziwen and others}, year={2025}, url={https://arxiv.org/abs/2501.14249} }"] \\ \hline
2024-11-07 & None & yes & FrontierMath & https://arxiv.org/abs/2411.04872 & Mathematics & Challenging advanced mathematical reasoning & ['symbolic reasoning', 'number theory', 'algebraic geometry', 'category theory'] & FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning
number theory, real analysis, algebraic geometry, and category theory, measuring LLMs’ 
ability to solve problems requiring deep abstract reasoning.
 & ['Problem solving'] & Symbolic and abstract mathematical reasoning & ['Accuracy'] & [] & Good & ['@misc{glazer2024frontiermath, title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI}, author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and others}, year={2024}, url={https://arxiv.org/abs/2411.04872} }'] \\ \hline
2024-07-18 & None & yes & SciCode & https://arxiv.org/abs/2407.13168 & Scientific Programming & Scientific code generation & problem solving & ['code synthesis', 'scientific computing', 'programming benchmark'] & SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80
real research tasks across 16 scientific subfields, evaluating models on knowledge recall, 
reasoning, and code synthesis for scientific computing tasks.
 & ['Coding'] & Program synthesis, scientific computing & ['Solve rate (percent)'] & ['Claude3.5-Sonnet'] & Good & ['@misc{tian2024scicode, title={SciCode: A Research Coding Benchmark Curated by Scientists}, author={Tian, Minyang and Gao, Luyu and Zhang, Shizhuo and others}, year={2024}, url={https://arxiv.org/abs/2407.13168} }'] \\ \hline
2025-03-13 & None & yes & AIME (American Invitational Mathematics Examination) & https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions & Mathematics & Pre-college advanced problem solving & ['algebra', 'combinatorics', 'number theory', 'geometry'] & The AIME is a 15-question, 3-hour exam for high-school students featuring challenging
short-answer math problems in algebra, number theory, geometry, and combinatorics, 
assessing depth of problem-solving ability.
 & ['Problem solving'] & Mathematical problem-solving and reasoning & ['Accuracy'] & [] & No formal paper; summary at https://www.vals.ai/benchmarks/aime-2025-03-13 & ['aime_website'] \\ \hline
2025-02-15 & None & yes & MATH-500 & https://huggingface.co/datasets/HuggingFaceH4/MATH-500 & Mathematics & Math reasoning generalization & ['calculus', 'algebra', 'number theory', 'geometry'] & MATH-500 is a curated subset of 500 problems from the OpenAI MATH dataset, spanning
high-school to advanced levels, designed to evaluate LLMs’ mathematical reasoning and 
generalization.
 & ['Problem solving'] & Math reasoning and generalization & ['Accuracy'] & [] & Dataset hosted on Hugging Face & ['@misc{huggingface2025math500, title={MATH-500}, author={HuggingFaceH4}, year={2025}, url={https://huggingface.co/datasets/HuggingFaceH4/MATH-500} }'] \\ \hline
2024-04-02 & None & yes & CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction) & https://arxiv.org/abs/2404.02029 & Multidomain Science & Long-context scientific reasoning & ['long-context', 'information extraction', 'multimodal'] & CURIE is a benchmark of 580 problems across six scientific disciplines—materials
science, quantum computing, biology, chemistry, climate science, and astrophysics—
designed to evaluate LLMs on long-context understanding, reasoning, and information 
extraction in realistic scientific workflows.
 & ['Information extraction', 'Reasoning', 'Concept tracking', 'Aggregation', 'Algebraic manipulation', 'Multimodal comprehension'] & Long-context understanding and scientific reasoning & ['Accuracy'] & [] & Good & ['@misc{curie2024, title={Scientific Reasoning Benchmarks from the CURIE Dataset}, author={TODO: Add authors}, year={2024}, url={https://arxiv.org/abs/2404.02029} }'] \\ \hline
2023-01-26 & None & yes & FEABench (Finite Element Analysis Benchmark) & https://github.com/alleninstitute/feabench & Computational Engineering & FEA simulation accuracy & performance & ['finite element', 'simulation', 'PDE'] & FEABench is a suite evaluating finite element analysis tools on standardized 
PDE-based simulation tasks with complex geometries and boundary conditions, 
measuring both accuracy and runtime performance.
 & ['Simulation', 'Performance evaluation'] & Numerical simulation accuracy and efficiency & ['Solve time', 'Error norm'] & ['FEniCS', 'deal.II'] & Good & ['@misc{allen2023feabench, title={FEABench: A Finite Element Analysis Benchmark}, author={Allen Institute}, year={2023}, url={https://github.com/alleninstitute/feabench} }'] \\ \hline
2024-07-12 & None & yes & SPIQA (Scientific Paper Image Question Answering) & https://arxiv.org/abs/2407.09413 & Computer Science & Multimodal QA on scientific figures & ['multimodal QA', 'figure understanding', 'table comprehension', 'chain-of-thought'] & SPIQA assesses AI models’ ability to interpret and answer questions about figures
and tables in scientific papers by integrating visual and textual modalities 
with chain-of-thought reasoning.
 & ['Question answering', 'Multimodal QA', 'Chain-of-Thought evaluation'] & Visual-textual reasoning in scientific contexts & ['Accuracy', 'F1 score'] & ['Chain-of-Thought models', 'Multimodal QA systems'] & Good & ['@article{zhong2024spiqa, title={SPIQA: Scientific Paper Image Question Answering}, author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin}, year={2024}, url={https://arxiv.org/abs/2407.09413} }'] \\ \hline
2020-09-28 & None & yes & MedQA & https://arxiv.org/abs/2009.13081 & Medical Question Answering & Medical board exam QA & ['USMLE', 'diagnostic QA', 'medical knowledge', 'multilingual'] & MedQA is a large-scale multiple-choice dataset drawn from professional medical
board exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge 
questions in English and Chinese.
 & ['Multiple choice'] & Medical diagnosis and knowledge retrieval & ['Accuracy'] & ['Neural reader', 'Retrieval-based QA systems'] & Multilingual (English, Simplified & Traditional Chinese) & ['@article{jin2020what, title={What Disease Does This Patient Have? A Large-scale Open-domain Question Answering Dataset from Medical Exams}, author={Jin, Di and Li, Ying and Zhang, Yichong and others}, year={2020}, url={https://arxiv.org/abs/2009.13081} }'] \\ \hline
2025-05-13 & None & yes & BaisBench (Biological AI Scientist Benchmark) & https://arxiv.org/abs/2505.08341 & Computational Biology & Omics-driven AI research tasks & ['single-cell annotation', 'biological QA', 'autonomous discovery'] & BaisBench evaluates AI scientists’ ability to perform data-driven biological research
by annotating cell types in single-cell datasets and answering MCQs derived from 
biological study insights, measuring autonomous scientific discovery.
 & ['Cell type annotation', 'Multiple choice'] & Autonomous biological research capabilities & ['Annotation accuracy', 'QA accuracy'] & ['LLM-based AI scientist agents'] & Underperforms human experts; aims to advance AI-driven discovery & ['@misc{luo2025benchmarkingaiscientistsomics, title={Benchmarking AI scientists in omics data-driven biological research}, author={Luo, Erpai and Jia, Jinmeng and Xiong, Yifan and others}, year={2025}, url={https://arxiv.org/abs/2505.08341} }'] \\ \hline
2023-01-26 & None & yes & MOLGEN & https://github.com/zjunlp/MolGen & Computational Chemistry & Molecular generation & optimization & ['SELFIES', 'GAN', 'property optimization'] & MolGen is a pre-trained molecular language model that generates chemically valid
molecules using SELFIES and reinforcement learning, guided by chemical feedback 
to optimize properties such as logP, QED, and docking score.
 & ['Distribution learning', 'Goal-oriented generation'] & Generation of valid and optimized molecular structures & ['Validity percent', 'Novelty percent', 'QED', 'Docking score'] & ['MolGen'] & Domain-agnostic prefix tuning; SELFIES-based & ['@article{fang2023domain, title={Domain-Agnostic Molecular Generation with Chemical Feedback}, author={Fang, Yin and Zhang, Ningyu and Chen, Zhuo and others}, year={2023}, url={https://arxiv.org/abs/2301.11259} }'] \\ \hline
2020-05-02 & None & yes & Open Graph Benchmark (OGB) – Biology & https://ogb.stanford.edu/docs/home/ & Graph ML & Biological graph property prediction & ['node prediction', 'link prediction', 'graph classification'] & OGB-Biology is a suite of large-scale biological network datasets (protein–protein
interaction, drug–target, etc.) with standardized splits and evaluation protocols 
for node, link, and graph property prediction tasks.
 & ['Node property prediction', 'Link property prediction', 'Graph property prediction'] & Scalability & generalization in graph ML for biology & ['Accuracy', 'ROC-AUC'] & ['GCN', 'GraphSAGE', 'GAT'] & Community-driven updates & ['@misc{hu2020ogb, title={Open Graph Benchmark: Datasets for Machine Learning on Graphs}, author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and others}, year={2020}, url={https://arxiv.org/abs/2005.00687} }'] \\ \hline
2011-10-01 & None & yes & Materials Project & https://materialsproject.org/ & Materials Science & DFT-based property prediction & ['DFT', 'materials genome', 'high-throughput'] & The Materials Project provides an open-access database of computed properties for
inorganic materials via high-throughput density functional theory (DFT), accelerating 
materials discovery.
 & ['Property prediction'] & Prediction of inorganic material properties & ['MAE', 'R²'] & ['Automatminer', 'Crystal Graph Neural Networks'] & Core component of the Materials Genome Initiative & ['@article{jain2013materials, title={The Materials Project: A materials genome approach...}, author={Jain, Anubhav and Ong, Shyue Ping and others}, journal={APL Materials}, year={2013}, url={https://materialsproject.org/} }'] \\ \hline
2020-10-20 & None & yes & OCP (Open Catalyst Project) & https://opencatalystproject.org/ & Chemistry; Materials Science & Catalyst adsorption energy prediction & ['DFT relaxations', 'adsorption energy', 'graph neural networks'] & The Open Catalyst Project (OC20 & OC22) provides DFT-calculated catalyst–adsorbate 
relaxation datasets, challenging ML models to predict energies and forces for 
renewable energy applications.
 & ['Energy prediction', 'Force prediction'] & Prediction of adsorption energies and forces & ['MAE (energy)', 'MAE (force)'] & ['CGCNN', 'SchNet', 'DimeNet++', 'GemNet-OC'] & Public leaderboards; active community development & ['@article{chanussot2020open, title={The Open Catalyst 2020 (OC20) Dataset...}, author={Chanussot, Loïk and Das, Abhishek and others}, year={2020}, url={https://arxiv.org/abs/2010.09990} }', '@article{tran2022open, title={The Open Catalyst 2022 (OC22) Dataset...}, author={Tran, Richard and Lan, Janice and others}, year={2022}, url={https://arxiv.org/abs/2206.08917} }'] \\ \hline
2023-06-20 & None & yes & JARVIS-Leaderboard & https://arxiv.org/abs/2306.11688 & Materials Science; Benchmarking & Comparative evaluation of materials design methods & ['leaderboards', 'materials methods', 'simulation'] & JARVIS-Leaderboard is a community-driven platform benchmarking AI, electronic
structure, force-fields, quantum computing, and experimental methods across hundreds
of materials science tasks.
 & ['Method benchmarking', 'Leaderboard ranking'] & Performance comparison across diverse materials design methods & ['MAE', 'RMSE', 'Accuracy'] & [] & 1,281 contributions across 274 benchmarks & ['@article{choudhary2023jarvis, title={JARVIS-Leaderboard: A Large Scale Benchmark...}, author={Choudhary, Kamal and Wines, Daniel and others}, year={2023}, url={https://arxiv.org/abs/2306.11688} }'] \\ \hline
2022-02-22 & None & yes & Quantum Computing Benchmarks (QML) & ['https://github.com/XanaduAI/qml-benchmarks', 'https://pennylane.ai/datasets/collection/qml-benchmarks'] & Quantum Computing & Quantum algorithm performance evaluation & ['quantum circuits', 'state preparation', 'error correction'] & A suite of benchmarks evaluating quantum hardware and algorithms on tasks such as state 
preparation, circuit optimization, and error correction across multiple platforms.
 & ['Circuit benchmarking', 'State classification'] & Quantum algorithm performance and fidelity & ['Fidelity', 'Success probability'] & ['IBM Q', 'IonQ', 'AQT@LBNL'] & Hardware-agnostic, application-level metrics & ['@misc{tomesh2022supermarq,...}'] \\ \hline
2024-10-01 & None & yes & CFDBench (Fluid Dynamics) & https://arxiv.org/abs/2310.05963 & Fluid Dynamics; Scientific ML & Neural operator surrogate modeling & ['neural operators', 'CFD', 'FNO', 'DeepONet'] & CFDBench provides large-scale CFD data for four canonical fluid flow problems, 
assessing neural operators’ ability to generalize to unseen PDE parameters and domains.
 & ['Surrogate modeling'] & Generalization of neural operators for PDEs & ['L2 error', 'MAE'] & ['FNO', 'DeepONet', 'U-Net'] & 302K frames across 739 cases & ['@misc{luo2024cfdbenchlargescalebenchmarkmachine, title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics}, author={Luo, Yining and Chen, Yingfa and Zhang, Zhen}, year={2024}, url={https://arxiv.org/abs/2310.05963} }'] \\ \hline
None & None & yes & SatImgNet & None & Remote Sensing & Satellite imagery classification & ['land-use', 'zero-shot', 'multi-task'] & SATIN (sometimes referred to as SatImgNet) is a multi-task metadataset of 27 satellite
imagery classification datasets evaluating zero-shot transfer of vision–language models
across diverse remote sensing tasks.
 & ['Image classification'] & Zero-shot land-use classification & ['Accuracy'] & [] & Public leaderboard available & ['@article{roberts2023satin,...}'] \\ \hline
2023-07-19 & None & yes & ClimateLearn & https://arxiv.org/abs/2307.01909 & Climate Science; Forecasting & ML for weather & climate modeling & ['medium-range forecasting', 'ERA5', 'data-driven'] & ClimateLearn provides standardized datasets and evaluation protocols for machine 
learning models in medium-range weather and climate forecasting using ERA5 reanalysis.
 & ['Forecasting'] & Global weather prediction (3–5 days) & ['RMSE', 'Anomaly correlation'] & ['CNN baselines', 'ResNet variants'] & Includes physical and ML baselines & ['@misc{nguyen2023climatelearnbenchmarkingmachinelearning,...}'] \\ \hline
2022-06-09 & None & yes & BIG-Bench (Beyond the Imitation Game Benchmark) & https://github.com/google/BIG-bench & NLP; AI Evaluation & Diverse reasoning & generalization tasks & ['few-shot', 'multi-task', 'bias analysis'] & BIG-Bench is a collaborative suite of 204 tasks designed to probe LLMs’ reasoning, 
knowledge, and bias across diverse domains and difficulty levels beyond simple imitation.
 & ['Few-shot evaluation', 'Multi-task evaluation'] & Reasoning and generalization across diverse tasks & ['Accuracy', 'Task-specific metrics'] & ['GPT-3', 'Dense Transformers', 'Sparse Transformers'] & Human baselines included & ['@article{srivastava2022beyond,...}'] \\ \hline
2019-11-20 & None & yes & CommonSenseQA & https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge & NLP; Commonsense & Commonsense question answering & ['ConceptNet', 'multiple-choice', 'adversarial'] & CommonsenseQA is a challenging multiple-choice QA dataset built from ConceptNet,
requiring models to apply commonsense knowledge to select the correct answer 
among five choices.
 & ['Multiple choice'] & Commonsense reasoning and knowledge integration & ['Accuracy'] & ['BERT-large', 'RoBERTa', 'GPT-3'] & Baseline 56 percent, human 89 percent & ['@inproceedings{talmor2019commonsenseqa,...}'] \\ \hline
2019-07-24 & None & yes & Winogrande & https://leaderboard.allenai.org/winogrande/submissions/public & NLP; Commonsense & Winograd Schema-style pronoun resolution & ['adversarial', 'pronoun resolution'] & WinoGrande is a large-scale adversarial dataset of 44,000 Winograd Schema-style 
questions with reduced bias using AFLite, serving as both a benchmark and transfer 
learning resource.
 & ['Pronoun resolution'] & Robust commonsense reasoning & ['Accuracy', 'AUC'] & ['RoBERTa', 'BERT', 'GPT-2'] & Human ~94 percent & ['@article{sakaguchi2019winogrande,...}'] \\ \hline
\end{tabular}
\end{table}
