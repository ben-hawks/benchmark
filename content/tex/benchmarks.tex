
\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{pdflscape}
\usepackage{wasysym}
\usepackage{longtable}
\usepackage[style=ieee, url=true]{biblatex}
\addbibresource{benchmarks.bib}

\begin{document}


\begin{landscape}
{\footnotesize
\begin{longtable}{|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{2.5cm}|p{2.5cm}|p{2.0cm}|p{2.0cm}|p{2.5cm}|p{4.0cm}|p{3.0cm}|p{3.0cm}|p{2.0cm}|p{2.0cm}|p{3.0cm}|p{1.0cm}|p{1.0cm}|p{3.0cm}|p{1.0cm}|p{3.0cm}|p{1.0cm}|p{3.0cm}|p{1.0cm}|p{3.0cm}|p{1.0cm}|p{3.0cm}|}
\hline
{\bf Date} & {\bf Expiration} & {\bf Valid} & {\bf Name} & {\bf URL} & {\bf Domain} & {\bf Focus} & {\bf Keywords} & {\bf Description} & {\bf Task Types} & {\bf AI Capability} & {\bf Metrics} & {\bf Models} & {\bf Notes} & {\bf Citation} & {\bf Specification Rating} & {\bf Specification Reason} & {\bf Dataset Rating} & {\bf Dataset Reason} & {\bf Metrics Rating} & {\bf Metrics Reason} & {\bf Reference Solution Rating} & {\bf Reference Solution Reason} & {\bf Documentation Rating} \\\\ \hline
\endfirsthead
\hline
{\bf Date} & {\bf Expiration} & {\bf Valid} & {\bf Name} & {\bf URL} & {\bf Domain} & {\bf Focus} & {\bf Keywords} & {\bf Description} & {\bf Task Types} & {\bf AI Capability} & {\bf Metrics} & {\bf Models} & {\bf Notes} & {\bf Citation} & {\bf Specification Rating} & {\bf Specification Reason} & {\bf Dataset Rating} & {\bf Dataset Reason} & {\bf Metrics Rating} & {\bf Metrics Reason} & {\bf Reference Solution Rating} & {\bf Reference Solution Reason} & {\bf Documentation Rating} \\\\ \hline
\endhead
\hline
\multicolumn{25}{r}{Continued on next page} \\
\endfoot
\hline
\endlastfoot
2024-05-01 & yes & Jet Classification & \href{https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify}{link} & Particle Physics & Real-time classification of particle jets using HL-LHC simulation features & classification, real-time ML, jet tagging, QKeras & This benchmark evaluates ML models for real-time classification of particle jets using high-level features derived from simulated LHC data. It includes both full-precision {\textbackslash}nand quantized models optimized for FPGA deployment.  & Classification & Real-time inference, model compression performance & Accuracy, AUC & Keras DNN, QKeras quantized DNN & Includes both float and quantized models using QKeras & \cite{duarte2022fastmlsciencebenchmarksaccelerating} \href{https://arxiv.org/abs/2207.07958}{$\Rightarrow$ } & 9.0 & Task and format (multiple-choice QA with 5 options) are clearly defined; grounded in ConceptNet with consistent structure, though no hardware/system constraints are specified. & 9.0 & Public, versioned, and FAIR-compliant; includes metadata, splits, and licensing; well-integrated with HuggingFace and other ML libraries. & 9.0 & Accuracy is a simple, reproducible metric aligned with task goals; no ambiguity in evaluation. & 8.0 & Several baseline models (e.g., BERT, RoBERTa) are reported with scores; implementations exist in public repos, but not bundled as an official starter kit. & 7.0 & Clear paper, GitHub repo, and integration with HuggingFace Datasets; full reproducibility requires manually connecting models to dataset. \\ \hline
2024-05-01 & yes & Irregular Sensor Data Compression & \href{https://github.com/fastmachinelearning/fastml-science/tree/main/sensor-data-compression}{link} & Particle Physics & Real-time compression of sparse sensor data with autoencoders & compression, autoencoder, sparse data, irregular sampling & This benchmark addresses lossy compression of irregularly sampled sensor data from {\textbackslash}nparticle detectors using real-time autoencoder architectures, targeting latency-critical {\textbackslash}napplications in physics experiments.  & Compression & Reconstruction quality, compression efficiency & MSE, Compression ratio & Autoencoder, Quantized autoencoder & Based on synthetic but realistic physics sensor data & \cite{duarte2022fastmlsciencebenchmarksaccelerating2} \href{https://arxiv.org/abs/2207.07958}{$\Rightarrow$ } & 8.0 & Classification is clearly defined for real-time inference on simulated LHC jets. Input features (HLFs) are documented, though exact latency or resource constraints are not numerically specified. & 9.0 & Two datasets (OpenML and Zenodo) are public, well-formatted, and documented; FAIR principles are followed, though richer metadata would raise confidence to a 10. & 9.0 & AUC and Accuracy are standard, quantitative, and well-aligned with goals of jet tagging and inference efficiency. & 8.0 & Float and quantized Keras/QKeras models are provided with results. Reproducibility is good, though full automation and documentation could be improved. & 8.0 & GitHub contains baseline code, data loaders, and references, but setup for deployment (e.g., FPGA pipeline) requires familiarity with the tooling. \\ \hline
2024-05-01 & yes & Beam Control & \href{https://github.com/fastmachinelearning/fastml-science/tree/main/beam-control}{link} & Accelerators and Magnets & Reinforcement learning control of accelerator beam position & RL, beam stabilization, control systems, simulation & Beam Control explores real-time reinforcement learning strategies for maintaining  stable beam trajectories in particle accelerators. The benchmark is based on the  BOOSTR environment for accelerator simulation.  & Control & Policy performance in simulated accelerator control & Stability, Control loss & DDPG, PPO (planned) & Environment defined, baseline RL implementation is in progress & \cite{kafkes2021boostrdatasetacceleratorcontrol, duarte2022fastmlsciencebenchmarksaccelerating3} \href{https://arxiv.org/abs/2101.08359}{$\Rightarrow$ } & 9.0 & Task is well defined (real-time compression of sparse, irregular sensor data using autoencoders); latency constraints are implied but not fully quantified. & 8.0 & Dataset is custom and synthetic but described well; FAIR-compliance is partial (reusable and accessible, but not externally versioned with rich metadata). & 9.0 & Uses standard quantitative metrics (MSE, compression ratio) clearly aligned with compression and reconstruction goals. & 7.0 & Baseline (autoencoder and quantized variant) is provided, but training/inference pipeline is minimally documented and needs user setup. & 8.0 & GitHub repo contains core components, but more structured setup instructions and pretrained weights would improve usability. \\ \hline
2024-07-08 & yes & Ultrafast jet classification at the HL-LHC & \href{https://arxiv.org/pdf/2402.01876}{link} & Particle Physics & FPGA-optimized real-time jet origin classification at the HL-LHC & jet classification, FPGA, quantization-aware training, Deep Sets, Interaction Networks & Demonstrates three ML models (MLP, Deep Sets, Interaction Networks) optimized for FPGA deployment with O(100 ns) inference using quantized models and hls4ml, targeting real-time jet tagging in the L1 trigger environment at the high-luminosity LHC. Data is available on Zenodo DOI:10.5281/zenodo.3602260.  & Classification & Real-time inference under FPGA constraints & Accuracy, Latency, Resource utilization & MLP, Deep Sets, Interaction Network & Uses quantization-aware training; hardware synthesis evaluated via hls4ml & \cite{odiagiu2024} \href{http://dx.doi.org/10.1088/2632-2153/ad5f10}{$\Rightarrow$ } & 8.0 & Task is clear (RL control of beam stability), with BOOSTR-based simulator; control objectives are well motivated, but system constraints and reward structure are still under refinement. & 7.0 & BOOSTR dataset exists and is cited, but integration into the benchmark is in early stages; metadata and FAIR structure are limited. & 7.0 & Stability and control loss are mentioned, but metrics are not yet formalized with clear definitions or baselines. & 5.5 & DDPG baseline mentioned; PPO planned; implementation is still in progress with no reproducible results available yet. & 6.0 & GitHub has a defined structure but is incomplete; setup and execution instructions for training/evaluation are not fully established. \\ \hline
2024-10-15 & yes & Quench detection & \href{https://indico.cern.ch/event/1387540/contributions/6153618/attachments/2948441/5182077/fast_ml_magnets_2024_final.pdf}{link} & Accelerators and Magnets & Real-time detection of superconducting magnet quenches using ML & quench detection, autoencoder, anomaly detection, real-time & Exploration of real-time quench detection using unsupervised and RL approaches, combining multi-modal sensor data (BPM, power supply, acoustic), operating on kHz-MHz streams with anomaly detection and frequency-domain features.  & Anomaly detection, Quench localization & Real-time anomaly detection with multi-modal sensors & ROC-AUC, Detection latency & Autoencoder, RL agents (in development) & Precursor detection in progress; multi-modal and dynamic weighting methods & 10.0 & Real-time jet origin classification under FPGA constraints is clearly defined, with explicit latency targets ({\textasciitilde}100 ns) and I/O formats. & 9.0 & Data available on Zenodo with DOI, includes constituent-level jets; accessible and well-documented, though not deeply versioned with full FAIR metadata. & 10.0 & Accuracy, latency, and hardware resource usage (LUTs, DSPs) are rigorously measured and aligned with real-time goals. & 9.0 & Includes models (MLP, Deep Sets, Interaction Networks) with quantization-aware training and synthesis results via hls4ml; reproducible but tightly coupled with specific toolchains. & 8.0 & Paper and code (via hls4ml) are sufficient, but a centralized, standalone repo for reproducing all models would enhance accessibility. \\ \hline
2024-10-15 & yes & DUNE & \href{https://indico.fnal.gov/event/66520/contributions/301423/attachments/182439/250508/fast_ml_dunedaq_sonic_10_15_24.pdf}{link} & Particle Physics & Real-time ML for DUNE DAQ time-series data & DUNE, time-series, real-time, trigger & Applying real-time ML methods to time-series data from DUNE detectors, exploring trigger-level anomaly detection and event selection with low latency constraints.  & Trigger selection, Time-series anomaly detection & Low-latency event detection & Detection efficiency, Latency & CNN, LSTM (planned) & Prototype models demonstrated on SONIC platform & 8.0 & Task (quench detection via anomaly detection) is clearly described; multi-modal sensors, streaming rates, and objective are provided, but constraints (latency thresholds) are qualitative. & 7.0 & Custom dataset using real data from BNL; HDF5 formatted and structured, but access may be internal or limited, and not versioned for public FAIR use. & 8.0 & ROC-AUC and detection latency are defined; relevant and quantitative but not yet paired with benchmark baselines. & 6.0 & Autoencoder prototype exists; RL methods are in development; no fully reproducible pipeline is available yet. & 7.0 & Slides and GDocs outline results; implementation is in progress with limited setup/code release. \\ \hline
2025-01-08 & yes & Intelligent experiments through real-time AI & \href{https://arxiv.org/pdf/2501.04845}{link} & Instrumentation and Detectors; Nuclear Physics; Particle Physics & Real-time FPGA-based triggering and detector control for sPHENIX and future EIC & FPGA, Graph Neural Network, hls4ml, real-time inference, detector control & Resaerch and Development demonstrator for real-time processing of high-rate tracking data from the sPHENIX detector (RHIC) and future EIC systems. Uses GNNs with hls4ml for FPGA-based trigger generation to identify rare events (heavy flavor, DIS electrons) within 10 {\textmu}s latency. Demonstrated improved accuracy and latency on Alveo/FELIX platforms.  & Trigger classification, Detector control, Real-time inference & Low-latency GNN inference on FPGA & Accuracy (charm and beauty detection), Latency ({\textmu}s), Resource utilization (LUT/FF/BRAM/DSP) & Bipartite Graph Network with Set Transformers (BGN-ST), GarNet (edge-classifier) & Achieved {\textasciitilde}97.4\% accuracy for beauty decay triggers; sub-10 {\textmu}s latency on Alveo U280; hit-based FPGA design via hls4ml and FlowGNN. & \cite{kvapil2025intelligentexperimentsrealtimeai} \href{https://arxiv.org/abs/2501.04845}{$\Rightarrow$ } & 8.0 & Task (trigger-level anomaly detection) is clearly defined for low-latency streaming input, but the problem framing lacks complete architectural/system specs. & 6.0 & Internal DUNE SONIC data; not publicly released and no formal FAIR support; replicability is institutionally gated. & 7.0 & Metrics include detection efficiency and latency, which are relevant, but only lightly supported by baselines or formal eval scripts. & 5.0 & One CNN prototype demonstrated; LSTM planned. No public implementation or ready-to-run example yet. & 6.0 & Slides and some internal documentation exist, but no full pipeline or public GitHub repo yet. \\ \hline
2025-01-09 & yes & Neural Architecture Codesign for Fast Physics Applications & \href{https://arxiv.org/abs/2501.05515}{link} & Physics; Materials Science; Particle Physics & Automated neural architecture search and hardware-efficient model codesign for fast physics applications & neural architecture search, FPGA deployment, quantization, pruning, hls4ml & Introduces a two-stage neural architecture codesign (NAC) pipeline combining global and local search, quantization-aware training, and pruning to design efficient models for fast Bragg peak finding and jet classification, synthesized for FPGA deployment with hls4ml. Achieves \ensuremath{>}30x reduction in BOPs and sub-100 ns inference latency on FPGA.  & Classification, Peak finding & Hardware-aware model optimization; low-latency inference & Accuracy, Latency, Resource utilization & NAC-based BraggNN, NAC-optimized Deep Sets (jet) & Demonstrated two case studies (materials science, HEP); pipeline and code open-sourced. & \cite{weitz2025neuralarchitecturecodesignfast} \href{https://arxiv.org/abs/2501.05515}{$\Rightarrow$ } & 10.0 & Task is clearly defined (triggering on rare events with sub-10 {\textmu}s latency); architecture, constraints, and system context (FPGA, Alveo) are well detailed. & 7.0 & Simulated tracking data from sPHENIX and EIC; internally structured but not yet released in a public FAIR-compliant format. & 10.0 & Accuracy, latency, and hardware resource utilization (LUTs, DSPs) are clearly defined and used in evaluation. & 9.0 & Graph-based models (BGN-ST, GarNet) are implemented and tested on real hardware; reproducibility possible with hls4ml but full scripts not bundled. & 8.0 & Paper is detailed and tool usage (FlowGNN, hls4ml) is described, but repo release and dataset access remain in progress. \\ \hline
2024-06-24 & yes & Smart Pixels for LHC & \href{https://arxiv.org/abs/2406.14860}{link} & Particle Physics; Instrumentation and Detectors & On-sensor, in-pixel ML filtering for high-rate LHC pixel detectors & smart pixel, on-sensor inference, data reduction, trigger & Presents a 256x256-pixel ROIC in 28 nm CMOS with embedded 2-layer NN for cluster filtering at 25 ns, achieving 54-75\% data reduction while maintaining noise and latency constraints. Prototype consumes {\textasciitilde}300 {\textmu}W/pixel and operates in combinatorial digital logic.  & Image Classification, Data filtering & On-chip, low-power inference; data reduction & Data rejection rate, Power per pixel & 2-layer pixel NN & Prototype in CMOS 28 nm; proof-of-concept for Phase III pixel upgrades. & \cite{parpillon2024smartpixelsinpixelai} \href{https://arxiv.org/abs/2406.14860}{$\Rightarrow$ } & 9.0 & Task (automated neural architecture search for real-time physics) is well formulated with clear latency, model compression, and deployment goals. & 6.0 & Internal Bragg and jet datasets used; not publicly hosted or FAIR-compliant, though mentioned in the paper. & 10.0 & BOP reduction, latency, and accuracy are all quantitatively evaluated. & 8.0 & NAC-generated models for Bragg peak and jet classification are described, but pipeline requires integration of several tools and is not fully packaged. & 7.0 & NAC pipeline, hls4ml usage, and results are discussed; code (e.g., nac-opt) referenced, but replication requires stitching together toolchain and data. \\ \hline
2023-10-03 & yes & HEDM BraggNN & \href{https://arxiv.org/abs/2008.08198}{link} & Material Science & Fast Bragg peak analysis using deep learning in diffraction microscopy & BraggNN, diffraction, peak finding, HEDM & Uses BraggNN, a deep neural network, for rapid Bragg peak localization in high-energy diffraction microscopy, achieving {\textasciitilde}13x speedup compared to Voigt-based methods while maintaining sub-pixel accuracy.  & Peak detection & High-throughput peak localization & Localization accuracy, Inference time & BraggNN & Enables real-time HEDM workflows; basis for NAC case study. & \cite{liu2021braggnnfastxraybragg} \href{https://arxiv.org/abs/2008.08198}{$\Rightarrow$ } & 10.0 & Fully specified: describes task (data filtering/classification), system design (on-sensor inference), latency (25 ns), and power constraints. & 8.0 & In-pixel charge cluster data used, but dataset release info is minimal; FAIR metadata/versioning limited. & 9.0 & Data rejection rate and power per pixel are clearly defined and directly tied to hardware goals. & 9.0 & 2-layer NN implementation is evaluated in hardware; reproducible via hls4ml flow with results in paper. & 8.0 & Paper is clear; Zenodo asset is referenced, but additional GitHub or setup repo would improve reproducibility. \\ \hline
2023-12-03 & yes & 4D-STEM & \href{https://openreview.net/pdf?id=7yt3N0o0W9}{link} & Material Science & Real-time ML for scanning transmission electron microscopy & 4D-STEM, electron microscopy, real-time, image processing & Proposes ML methods for real-time analysis of 4D scanning transmission electron microscopy datasets; framework details in progress.  & Image Classification, Streamed data inference & Real-time large-scale microscopy inference & Classification accuracy, Throughput & CNN models (prototype) & In-progress; model design under development. & \cite{qin2023extremely} \href{https://openreview.net/forum?id=7yt3N0o0W9}{$\Rightarrow$ } & 9.0 & Peak localization task is well-defined for diffraction images; input/output described clearly, but no system constraints. & 8.0 & Simulated diffraction images provided; reusable and downloadable, but not externally versioned or FAIR-structured. & 9.0 & Inference speed and localization accuracy are standard and quantitatively reported. & 8.0 & BraggNN model and training pipeline exist, but need stitching from separate repositories. & 8.0 & Paper and codebase are available and usable, though not fully turnkey. \\ \hline
2023-12-05 & yes & In-Situ High-Speed Computer Vision & \href{https://arxiv.org/abs/2312.00128}{link} & Fusion/Plasma & Real-time image classification for in-situ plasma diagnostics & plasma, in-situ vision, real-time ML & Applies low-latency CNN models for image classification of plasma diagnostics streams; supports deployment on embedded platforms.  & Image Classification & Real-time diagnostic inference & Accuracy, FPS & CNN & Embedded/deployment details in progress. & \cite{wei2024} \href{http://dx.doi.org/10.1063/5.0190354}{$\Rightarrow$ } & 7.0 & General task defined (real-time microscopy inference), but no standardized I/O format, latency constraint, or complete problem framing yet. & 0.0 & Dataset not provided or described in any formal way. & 6.0 & Mentions throughput and accuracy, but metrics are not formally defined or benchmarked. & 2.0 & Prototype CNNs described; no baseline or implementation released. & 5.0 & OpenReview paper and Gemini doc give some insight, but no working code, environment, or example. \\ \hline
2020-01-01 & yes & BenchCouncil AIBench & \href{https://www.benchcouncil.org/AIBench/}{link} & General & End-to-end AI benchmarking across micro, component, and application levels & benchmarking, AI systems, application-level evaluation & AIBench is a comprehensive benchmark suite that evaluates AI workloads at different levels (micro, component, application) across hardware systems{\textemdash}covering image generation, object detection, translation, recommendation, video prediction, etc. & Training, Inference, End-to-end AI workloads & System-level AI workload performance & Throughput, Latency, Accuracy & ResNet, BERT, GANs, Recommendation systems & Covers scenario-distilling, micro, component, and end-to-end benchmarks. & \cite{gao2019aibenchindustrystandardinternet} \href{https://arxiv.org/abs/1908.08998}{$\Rightarrow$ } & 8.0 & Task (plasma diagnostic classification) and real-time deployment described; system specs (FPS targets) implied but not fully quantified. & 6.0 & Dataset is sensor stream-based but not shared or FAIR-documented. & 8.0 & FPS and classification accuracy reported and relevant. & 7.0 & CNN model described and evaluated, but public implementation and benchmarks are not available yet. & 6.0 & Paper and Gemini doc exist, but full setup instructions and tools are still in progress. \\ \hline
2020-01-01 & yes & BenchCouncil BigDataBench & \href{https://www.benchcouncil.org/BigDataBench/}{link} & General & Big data and AI benchmarking across structured, semi-structured, and unstructured data workloads & big data, AI benchmarking, data analytics & BigDataBench provides benchmarks for evaluating big data and AI workloads with realistic datasets (13 sources) and pipelines across analytics, graph, warehouse, NoSQL, streaming, and AI. & Data preprocessing, Inference, End-to-end data pipelines & Data processing and AI model inference performance at scale & Data throughput, Latency, Accuracy & CNN, LSTM, SVM, XGBoost & Built on eight data motifs; provides Hadoop, Spark, Flink, MPI implementations. & \cite{gao2018bigdatabenchscalableunifiedbig} \href{https://arxiv.org/abs/1802.08254}{$\Rightarrow$ } & 9.0 & Evaluates AI at multiple levels (micro to end-to-end); tasks and workloads are clearly defined, though specific I/O formats and constraints vary. & 9.0 & Realistic datasets across diverse domains; FAIR structure for many components, but individual datasets may not all be versioned or richly annotated. & 9.0 & Latency, throughput, and accuracy clearly defined for end-to-end tasks; consistent across models and setups. & 8.0 & Reference implementations for several tasks exist, but setup across all tasks is complex and not fully streamlined. & 8.0 & Central documentation exists, with detailed component breakdowns; environment setup across platforms (e.g., hardware variations) can require manual adjustment. \\ \hline
2021-10-20 & yes & MLPerf HPC & \href{https://github.com/mlcommons/hpc}{link} & Cosmology, Climate, Protein Structure, Catalysis & Scientific ML training and inference on HPC systems & HPC, training, inference, scientific ML & MLPerf HPC introduces scientific model benchmarks (e.g., CosmoFlow, DeepCAM) aimed at large-scale HPC evaluation with \ensuremath{>}10x performance scaling through system-level optimizations. & Training, Inference & Scaling efficiency, training time, model accuracy on HPC & Training time, Accuracy, GPU utilization & CosmoFlow, DeepCAM, OpenCatalyst & Shared framework with MLCommons Science; reference implementations included. & \cite{farrell2021mlperfhpcholisticbenchmark} \href{https://arxiv.org/abs/2110.11466}{$\Rightarrow$ } & 9.0 & Focused on structured/unstructured data pipelines; clearly defined tasks spanning analytics to AI; some scenarios lack hardware constraint modeling. & 9.0 & Built from 13 real-world sources; structured for realistic big data scenarios; partially FAIR-compliant with documented data motifs. & 9.0 & Covers data throughput, latency, and accuracy; quantitative and benchmark-ready. & 8.0 & Many pipeline and model examples provided using Hadoop/Spark/Flink; setup effort varies by task and platform. & 8.0 & Strong documentation with examples and task specifications; centralized support exists, but task-specific tuning may require domain expertise. \\ \hline
2023-06-01 & yes & MLCommons Science & \href{https://github.com/mlcommons/science}{link} & Earthquake, Satellite Image, Drug Discovery, Electron Microscope, CFD & AI benchmarks for scientific applications including time-series, imaging, and simulation & science AI, benchmark, MLCommons, HPC & MLCommons Science assembles benchmark tasks with datasets, targets, and implementations across earthquake forecasting, satellite imagery, drug screening, electron microscopy, and CFD to drive scientific ML reproducibility. & Time-series analysis, Image classification, Simulation surrogate modeling & Inference accuracy, simulation speed-up, generalization & MAE, Accuracy, Speedup vs simulation & CNN, GNN, Transformer & Joint national-lab effort under Apache-2.0 license. & \cite{mlcommons_science2023} \href{https://github.com/mlcommons/science}{$\Rightarrow$ } & 10.0 & Scientific ML tasks (e.g., CosmoFlow, DeepCAM) are clearly defined with HPC system-level constraints and targets. & 9.0 & Public scientific datasets (e.g., cosmology, weather); used consistently, though FAIR-compliance of individual datasets varies slightly. & 10.0 & Training time, GPU utilization, and accuracy are all directly measured and benchmarked across HPC systems. & 9.0 & Reference implementations available and actively maintained; HPC setup may require domain-specific environment. & 9.0 & GitHub repo and papers provide detailed instructions; reproducibility supported across multiple institutions. \\ \hline
2021-07-05 & yes & LHC New Physics Dataset & \href{https://arxiv.org/pdf/2107.02157}{link} & Particle Physics; Real-time Triggering & Real-time LHC event filtering for anomaly detection using proton collision data & anomaly detection, proton collision, real-time inference, event filtering, unsupervised ML & A dataset of proton-proton collision events emulating a 40 MHz real-time data stream from LHC detectors, pre-filtered on electron or muon presence. Designed for unsupervised new-physics detection algorithms under latency/bandwidth constraints. & Anomaly detection, Event classification & Unsupervised signal detection under latency and bandwidth constraints & ROC-AUC, Detection efficiency & Autoencoder, Variational autoencoder, Isolation forest & Includes electron/muon-filtered background and black-box signal benchmarks; 1M events per black box. & \cite{thea_aarrestad_2021_5046428} \href{https://doi.org/10.5281/zenodo.5046428}{$\Rightarrow$ } & 7.0 & The problem (anomaly detection for new physics at LHC) is clearly described with goals and background, but lacks a formal task specification or constraints. & 8.0 & Large-scale, public dataset derived from LHC simulations; well-documented and available via Zenodo. & 7.0 & Provides AUROC, accuracy, and anomaly detection metrics but lacks standardized evaluation script. & 5.0 & Baseline models (autoencoders, GANs) are described in associated papers, but implementations vary across papers. & 6.0 & Publicly available papers and datasets with descriptions, but no unified README or training setup. \\ \hline
2023-07-17 & yes & MLCommons Medical AI & \href{https://github.com/mlcommons/medical}{link} & Healthcare; Medical AI & Federated benchmarking and evaluation of medical AI models across diverse real-world clinical data & medical AI, federated evaluation, privacy-preserving, fairness, healthcare benchmarks & The MLCommons Medical AI working group develops benchmarks, best practices, and platforms (MedPerf, GaNDLF, COFE) to accelerate robust, privacy-preserving AI development for healthcare. MedPerf enables federated testing of clinical models on diverse datasets, improving generalizability and equity while keeping data onsite.  & Federated evaluation, Model validation & Clinical accuracy, fairness, generalizability, privacy compliance & ROC AUC, Accuracy, Fairness metrics & MedPerf-validated CNNs, GaNDLF workflows & Open-source platform under Apache-2.0; used across 20+ institutions and hospitals. & \cite{karargyris2023federated} \href{https://www.nature.com/articles/s42256-023-00652-2}{$\Rightarrow$ } & 9.0 & Diverse scientific tasks (earthquake, CFD, microscopy) with detailed problem statements and goals; system constraints not uniformly applied. & 9.0 & Domain-specific datasets (e.g., microscopy, climate); mostly public and structured, but FAIR annotations are not always explicit. & 9.0 & Task-specific metrics (MAE, speedup, accuracy) are clear and reproducible. & 9.0 & Reference models (CNN, GNN, Transformer) provided with training/evaluation pipelines. & 9.0 & Well-documented, open-sourced, and maintained with examples; strong community support and reproducibility focus. \\ \hline
2024-10-28 & yes & CaloChallenge 2022 & \href{http://arxiv.org/abs/2410.21611}{link} & LHC Calorimeter; Particle Physics & Fast generative-model-based calorimeter shower simulation evaluation & calorimeter simulation, generative models, surrogate modeling, LHC, fast simulation & The Fast Calorimeter Simulation Challenge 2022 assessed 31 generative-model submissions (VAEs, GANs, Flows, Diffusion) on four calorimeter shower datasets; benchmarking shower quality, generation speed, and model complexity.  & Surrogate modeling & Simulation fidelity, speed, efficiency & Histogram similarity, Classifier AUC, Generation latency & VAE variants, GAN variants, Normalizing flows, Diffusion models & The most comprehensive survey to date on ML-based calorimeter simulation; 31 submissions over different dataset sizes. & \cite{krause2024calochallenge2022communitychallenge} \href{https://arxiv.org/abs/2410.21611}{$\Rightarrow$ } & 9.0 & Task is clearly defined: real-time anomaly detection from high-rate LHC collisions. Latency and bandwidth constraints are mentioned, though not numerically enforced. & 9.0 & Publicly available via Zenodo, with structured signal/background splits, and rich metadata; nearly fully FAIR. & 9.0 & ROC-AUC and detection efficiency are clearly defined and appropriate for unsupervised anomaly detection. & 8.0 & Several baseline methods (autoencoder, VAE, isolation forest) are evaluated; runnable versions available via community repos but not tightly bundled. & 8.0 & Paper and data documentation are clear, and the dataset is widely reused. Setup requires some manual effort to reproduce full pipelines. \\ \hline
ongoing & yes & Papers With Code- SOTA Platform & \href{https://paperswithcode.com/sota}{link} & General ML; All domains & Open platform tracking state-of-the-art results, benchmarks, and implementations across ML tasks and papers & leaderboard, benchmarking, reproducibility, open-source & Papers With Code (PWC) aggregates benchmark suites, tasks, and code across ML research: 12,423 benchmarks, 5,358 unique tasks, and 154,766 papers with code links. It tracks SOTA metrics and fosters reproducibility.  & Multiple (Classification, Detection, NLP, etc.) & Model performance across tasks (accuracy, F1, BLEU, etc.) & Task-specific (Accuracy, F1, BLEU, etc.) & All published models with code & Community-driven open platform; automatic data extraction and versioning. & \cite{pwc2025} \href{https://paperswithcode.com}{$\Rightarrow$ } & 9.0 & Evaluation setting (federated clinical benchmarking) is well-defined; I/O interfaces vary slightly by task but are standardized in MedPerf platform. & 8.0 & Uses distributed, real-world clinical datasets across institutions; FAIR compliance varies across hospitals and data hosts. & 9.0 & ROC AUC, accuracy, and fairness metrics are explicitly defined and task-dependent; consistently tracked across institutions. & 8.0 & Validated CNNs and GaNDLF pipelines are used and shared via the MedPerf tool, but some implementations are abstracted behind the platform. & 9.0 & Excellent documentation across MedPerf, GaNDLF, and COFE; reproducibility handled via containerized flows and task templates. \\ \hline
2022-01-01 & yes & Codabench & \href{https://www.codabench.org/}{link} & General ML; Multiple & Open-source platform for organizing reproducible AI benchmarks and competitions & benchmark platform, code submission, competitions, meta-benchmark & Codabench (successor to CodaLab) is a flexible, easy-to-use, reproducible API platform for hosting AI benchmarks and code-submission challenges. It supports custom scoring, inverted benchmarks, and scalable public or private queues  & Multiple & Model reproducibility, performance across datasets & Submission count, Leaderboard ranking, Task-specific metrics & Arbitrary code submissions & Hosts 51 public competitions, {\textasciitilde}26k users, 177k submissions & \cite{xu2021codabench} & 0 & Not a benchmark. It's a hosting site for benchmarks. & 0 & Not a benchmark. It's a hosting site for benchmarks. & 0 & Not a benchmark. It's a hosting site for benchmarks. & 0 & Not a benchmark. It's a hosting site for benchmarks. & 0 & Not a benchmark. It's a hosting site for benchmarks. \\ \hline
2021-09-27 & yes & Sabath - SBI-FAIR & \href{https://sbi-fair.github.io/docs/software/sabath/}{link} & Systems; Metadata & FAIR metadata framework for ML-driven surrogate workflows in HPC systems & meta-benchmark, metadata, HPC, surrogate modeling & Sabath is a metadata framework from the SBI-FAIR group (UTK, Argonne, Virginia) facilitating FAIR-compliant benchmarking and surrogate execution logging across HPC systems  & Systems benchmarking & Metadata tracking, reproducible HPC workflows & Metadata completeness, FAIR compliance & N/A & Developed by PI Piotr Luszczek at UTK; integrates with MiniWeatherML, AutoPhaseNN, Cosmoflow, etc. & \cite{luszczek2021sabath} & 8.0 & The benchmark defines simulation-based inference (SBI) tasks clearly with FAIR principles applied to particle physics datasets. & 8.0 & Data is well-structured for SBI and publicly available with clear licensing. & 8.0 & Includes likelihood and posterior accuracy; metrics well-matched to SBI. & 7.0 & Baseline SBI models are implemented and reproducible. & 6.0 & GitHub repo includes code and instructions, but lacks full tutorials or walkthroughs. \\ \hline
2022-10-13 & yes & PDEBench & \href{https://github.com/pdebench/PDEBench}{link} & CFD; Weather Modeling & Benchmark suite for ML-based surrogates solving time-dependent PDEs & PDEs, CFD, scientific ML, surrogate modeling, NeurIPS & PDEBench offers forward/inverse PDE tasks with large ready-to-use datasets and baselines (FNO, U-Net, PINN), packaged via a unified API. It won the SimTech Best Paper Award 2023.  & Supervised Learning & Time-dependent PDE modeling; physical accuracy & RMSE, boundary RMSE, Fourier RMSE & FNO, U-Net, PINN, Gradient-Based inverse methods & Datasets hosted on DaRUS (DOI:10.18419/darus-2986); contact maintainers by email & \cite{takamoto2024pdebenchextensivebenchmarkscientific} \href{https://arxiv.org/abs/2210.07182}{$\Rightarrow$ } & 9.0 & Clearly defined PDE-solving tasks with well-specified constraints and solution formats. & 9.0 & Includes synthetic and real-world PDE datasets with detailed format descriptions. & 8.0 & Uses L2 error and other norms relevant to PDE solutions. & 7.0 & Includes baseline solvers and trained models across multiple PDE tasks. & 8.0 & Well-organized GitHub with examples, dataset loading scripts, and training configs. \\ \hline
2024-12-03 & yes & The Well & \href{https://polymathic-ai.org/the_well/}{link} & biological systems, fluid dynamics, acoustic scattering, astrophysical MHD & Foundation model + surrogate dataset spanning 16 physical simulation domains & surrogate modeling, foundation model, physics simulations, spatiotemporal dynamics & A 15 TB collection of ML-ready physics simulation datasets (HDF5), covering 16 domains{\textemdash}from biology to astrophysical magnetohydrodynamic simulations{\textemdash}with unified API and metadata. Ideal for training surrogate and foundation models on scientific data.  & Supervised Learning & Surrogate modeling, physics-based prediction & Dataset size, Domain breadth & FNO baselines, U-Net baselines & Includes unified API and dataset metadata; see 2025 NeurIPS paper for full benchmark details. Size: 15 TB. & \cite{ohana2024well} & 7.0 & Explores LLM understanding of mental health scenarios; framing is creative but loosely defined. & 6.0 & Dataset is described in concept but not released; privacy limits public access though synthetic proxies are referenced. & 7.0 & Uses manual annotation and quality scores, but lacks standardized automatic metrics. & 6.0 & Provides few-shot prompt examples and human rating calibration details. & 5.0 & Paper gives use cases, but code and data are not yet public. \\ \hline
2024-10-31 & yes & LLM-Inference-Bench & \href{https://github.com/argonne-lcf/LLM-Inference-Bench}{link} & LLM; HPC/inference & Hardware performance benchmarking of LLMs on AI accelerators & LLM, inference benchmarking, GPU, accelerator, throughput & A suite evaluating inference performance of LLMs (LLaMA, Mistral, Qwen) across diverse accelerators (NVIDIA, AMD, Intel, SambaNova) and frameworks (vLLM, DeepSpeed-MII, etc.), with an interactive dashboard and per-platform metrics.  & Inference Benchmarking & Inference throughput, latency, hardware utilization & Token throughput (tok/s), Latency, Framework-hardware mix performance & LLaMA-2-7B, LLaMA-2-70B, Mistral-7B, Qwen-7B & Licensed under BSD-3, maintained by Argonne; supports GPUs and accelerators. & \cite{chittyvenkata2024llminferencebenchinferencebenchmarkinglarge} \href{https://arxiv.org/abs/2411.00136}{$\Rightarrow$ } & 9.0 & PDE tasks (forward/inverse) and I/O structures are clearly specified with detailed PDE context and constraints. & 10.0 & Hosted via DaRUS with a DOI, well-documented, versioned, and FAIR-compliant. & 9.0 & Uses RMSE variants and Fourier-based errors. & 10.0 & Baselines (FNO, U-Net, PINN) implemented and ready-to-run; strong community adoption. & 9.0 & Clean GitHub with usage, dataset links, and tutorial notebooks. \\ \hline
2023-12-12 & yes & SGLang Framework & \href{https://github.com/sgl-project/sglang/tree/main/benchmark}{link} & LLM Vision & Fast serving framework for LLMs and vision-language models & LLM serving, vision-language, RadixAttention, performance, JSON decoding & A high-performance open-source serving framework combining efficient backend runtime (RadixAttention, batching, quantization) and expressive frontend language, boosting LLM/VLM inference throughput up to {\textasciitilde}3x over alternatives.  & Model serving framework & Serving throughput, JSON/task-specific latency & Tokens/sec, Time-to-first-token, Throughput gain vs baseline & LLaVA, DeepSeek, Llama & Deployed in production (xAI, NVIDIA, Google Cloud); v0.4.8 release June 2025. & \cite{zheng2024sglangefficientexecutionstructured} \href{https://arxiv.org/abs/2312.07104}{$\Rightarrow$ } & 8.0 & Clearly framed around surrogate learning across 16 domains, but not all tasks are formally posed or constrained in a unified benchmark protocol. Paper mentions performance on NVIDIA H100. & 9.0 & FAIR-compliant physics simulation dataset, structured in HDF5 with unified metadata. & 7.0 & Metrics like dataset size and domain coverage are listed, but standardized quantitative model evaluation metrics (e.g., RMSE, MAE) are not enforced. & 9.0 & FNO and U-Net baselines available; full benchmarking implementations pending NeurIPS paper code release. & 10.0 & Site and GitHub offer a unified API, metadata standards, and dataset loading tools; NeurIPS paper adds detailed design context. \\ \hline
2023-09-12 & yes & vLLM Inference and Serving Engine & \href{https://github.com/vllm-project/vllm/tree/main/benchmarks}{link} & LLM; HPC/inference & High-throughput, memory-efficient inference and serving engine for LLMs & LLM inference, PagedAttention, CUDA graph, streaming API, quantization & vLLM is a fast, high-throughput, memory-efficient inference and serving engine for large language models, featuring PagedAttention, continuous batching, and support for quantized and pipelined model execution. Benchmarks compare it to TensorRT-LLM, SGLang, and others & Inference Benchmarking & Throughput, latency, memory efficiency & Tokens/sec, Time to First Token (TTFT), Memory footprint & LLaMA, Mixtral, FlashAttention-based models & Incubated by LF AI and Data; achieves up to 24x throughput over HuggingFace Transformers & \cite{kwon2023efficient} & 9.0 & Benchmarks hardware performance of LLM inference across multiple platforms with well-defined input/output and platform constraints. & 7.0 & Uses structured log files and configs instead of conventional datasets; suitable for inference benchmarking. & 9.0 & Clear throughput, latency, and utilization metrics; platform comparison dashboard enhances evaluation. & 8.0 & Includes reproducible scripts and example runs; models like LLaMA and Mistral are referenced with platform-specific configs. & 8.0 & GitHub contains clear instructions, platform details, and framework comparisons. \\ \hline
2022-06-22 & yes & vLLM Performance Dashboard & \href{https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}{link} & LLM; HPC/inference & Interactive dashboard showing inference performance of vLLM & Dashboard, Throughput visualization, Latency analysis, Metric tracking & A live visual dashboard for vLLM showcasing throughput, latency, and other inference metrics across models and hardware configurations.  & Performance visualization & Throughput, latency, hardware utilization & Tokens/sec, TTFT, Memory usage & LLaMA-2, Mistral, Qwen & Built using ObservableHQ; integrates live data from vLLM benchmarks. & \cite{mo2024vllm_dashboard} \href{https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}{$\Rightarrow$ } & 8.0 & Framed as a model-serving tool rather than a benchmark, but includes benchmark configurations and real model tasks. & 6.0 & Mostly uses dummy configs or external model endpoints for evaluation; not designed around a formal dataset. & 8.0 & Well-defined serving metrics: tokens/sec, time-to-first-token, and gain over baselines. & 9.0 & Core framework includes full reproducible serving benchmarks and code; multiple deployment case studies. & 9.0 & High-quality usage guides, examples, and performance tuning docs. \\ \hline
2022-04-01 & yes & Nixtla NeuralForecast & \href{https://github.com/Nixtla/neuralforecast}{link} & Time-series forecasting; General ML & High-performance neural forecasting library with \ensuremath{>}30 models & time-series, neural forecasting, NBEATS, NHITS, TFT, probabilistic forecasting, usability & NeuralForecast offers scalable, user-friendly implementations of over 30 neural forecasting models (NBEATS, NHITS, TFT, DeepAR, etc.), emphasizing quality, usability, interpretability, and performance.  & Time-series forecasting & Forecast accuracy, interpretability, speed & RMSE, MAPE, CRPS & NBEATS, NHITS, TFT, DeepAR & AutoModel supports hyperparameter tuning and distributed execution via Ray and Optuna. First official NHITS implementation. & \cite{olivares2022library_neuralforecast} \href{https://github.com/Nixtla/neuralforecast}{$\Rightarrow$ } & 9.0 & Targets high-throughput LLM inference via PagedAttention and memory-optimized serving; benchmarks cover many configs. & 7.0 & Focuses on model configs and streaming input/output pipelines rather than classical datasets. & 9.0 & Strong token/sec, memory usage, and TTFT metrics; comparative plots and logs included. & 9.0 & Benchmarks reproducible via script with support for multiple models and hardware types. & 9.0 & Excellent GitHub docs, CLI/API usage, and deployment walkthroughs. \\ \hline
2023-06-01 & yes & Nixtla Neural Forecast NHITS & \href{https://github.com/Nixtla/neuralforecast}{link} & Time-series; General ML & Official NHITS implementation for long-horizon time series forecasting & NHITS, long-horizon forecasting, neural interpolation, time-series & NHITS (Neural Hierarchical Interpolation for Time Series) is a state-of-the-art model that improved accuracy by {\textasciitilde}25\% and reduced compute by 50x compared to Transformer baselines, using hierarchical interpolation and multi-rate sampling  & Time-series forecasting & Accuracy, compute efficiency for long series & RMSE, MAPE & NHITS & Official implementation in NeuralForecast, included since its AAAI 2023 release. & \cite{challu2023nhits} & 7.0 & Primarily a visualization frontend; underlying benchmark definitions come from vLLM project. & 6.0 & No traditional dataset; displays live or logged benchmark metrics. & 9.0 & Live throughput, memory, latency, and TTFT displayed interactively; highly informative for performance analysis. & 7.0 & Dashboard built on vLLM benchmarks but not itself a complete experiment package. & 8.0 & Observable notebooks are intuitive; customization instructions are minimal but UI is self-explanatory. \\ \hline
2023-10-03 & yes & Nixtla Neural Forecast TimeLLM & \href{https://github.com/Nixtla/neuralforecast}{link} & Time-series; General ML & Reprogramming LLMs for time series forecasting & Time-LLM, language model, time-series, reprogramming & Time-LLM uses reprogramming layers to adapt frozen LLMs for time series forecasting, treating forecasting as a language task.  & Time-series forecasting & Model reuse via LLM, few-shot forecasting & RMSE, MAPE & Time-LLM & Fully open-source; transforms forecasting using LLM text reconstruction. & \cite{jin2024timellmtimeseriesforecasting} \href{https://arxiv.org/abs/2310.01728}{$\Rightarrow$ } & 7.0 & Describes forecasting with LLMs, but less formal on input/output or task framing. & 6.0 & Uses open time series datasets, but lacks a consolidated data release or splits. & 7.0 & Reports metrics like MASE and SMAPE, standard in forecasting. & 6.0 & Provides TimeLLM with open source, but no other baselines included. & 6.0 & GitHub readme with installation and example usage; lacks API or extensive tutorials. \\ \hline
2023-10-05 & yes & Nixtla Neural Forecast TimeGPT & \href{https://github.com/Nixtla/neuralforecast}{link} & Time-series; General ML & Time-series foundation model ''TimeGPT'' for forecasting and anomaly detection & TimeGPT, foundation model, time-series, generative model & TimeGPT is a transformer-based generative pretrained model on 100B+ time series data for zero-shot forecasting and anomaly detection via API.  & Time-series forecasting, Anomaly detection & Zero-shot forecasting, anomaly detection & RMSE, Anomaly detection metrics & TimeGPT & Offered via Nixtla API and Azure Studio; enterprise-grade support available. & \cite{garza2024timegpt1} \href{https://arxiv.org/abs/2310.03589}{$\Rightarrow$ } & 7.0 & Describes forecasting with LLMs, but less formal on input/output or task framing. & 6.0 & Uses open time series datasets, but lacks a consolidated data release or splits. & 7.0 & Reports metrics like MASE and SMAPE, standard in forecasting. & 6.0 & Provides TimeLLM with open source, but no other baselines included. & 6.0 & GitHub readme with installation and example usage; lacks API or extensive tutorials. \\ \hline
2025-03-03 & yes & HDR ML Anomaly Challenge- Gravitational Waves & \href{https://www.codabench.org/competitions/2626/}{link} & Astrophysics; Time-series & Detecting anomalous gravitational-wave signals from LIGO/Virgo datasets & anomaly detection, gravitational waves, astrophysics, time-series & A benchmark for detecting anomalous transient gravitational-wave signals, including ''unknown-unknowns,'' using preprocessed LIGO time-series at 4096 Hz. Competitors submit inference models on Codabench for continuous 50 ms segments from dual interferometers. & Anomaly detection & Novel event detection in physical signals & ROC-AUC, Precision/Recall & Deep latent CNNs, Autoencoders & NSF HDR A3D3 sponsored; prize pool and starter kit provided on Codabench. & \cite{campolongo2025buildingmachinelearningchallenges} \href{https://arxiv.org/abs/2503.02112}{$\Rightarrow$ } & 8.0 & Novel approach treating forecasting as text generation is explained; framing is less conventional. & 9.0 & Compatible with standard forecasting datasets (e.g., M4, electricity). & 8.0 & RMSE and MAPE are included, but less emphasis on interpretability or time-series domain constraints. & 9.0 & Open-source with reprogramming layers, LLM interface scripts provided. & 8.0 & Model and architecture overview present, though usability guide is slightly lighter than others. \\ \hline
2025-03-03 & yes & HDR ML Anomaly Challenge- Butterfly & \href{https://www.codabench.org/competitions/3764/}{link} & Genomics; Image/CV & Detecting hybrid butterflies via image anomaly detection in genomic-informed dataset & anomaly detection, computer vision, genomics, butterfly hybrids & Image-based challenge for detecting butterfly hybrids in microscopy-driven species data. Participants evaluate  models on Codabench using image segmentation/classification.  & Anomaly detection & Hybrid detection in biological systems & Classification accuracy, F1 score & CNN-based detectors & Hybrid detection benchmarks hosted on Codabench. & \cite{campolongo2025buildingmachinelearningchallenges2} \href{https://arxiv.org/abs/2503.02112}{$\Rightarrow$ } & 8.0 & Task of detecting rare anomalies in butterfly physics is well-described with physics motivation. & 7.0 & Real detector data with injected anomalies is available, but requires NDA for full access. & 7.0 & Uses ROC, F1, and anomaly precision, standard in challenge evaluations. & 4.0 & Partial baselines described, but no codebase or reproducible runs. & 6.0 & Challenge site includes overview and metrics, but limited in walkthrough or examples. \\ \hline
2025-03-03 & yes & HDR ML Anomaly Challenge- Sea Level Rise & \href{https://www.codabench.org/competitions/3223/}{link} & Climate Science; Time-series, Image/CV & Detecting anomalous sea-level rise and flooding events via time-series and satellite imagery & anomaly detection, climate science, sea-level rise, time-series, remote sensing & A challenge combining North Atlantic sea-level time-series and satellite imagery to detect flooding anomalies. Models submitted via Codabench.  & Anomaly detection & Detection of environmental anomalies & ROC-AUC, Precision/Recall & CNNs, RNNs, Transformers & Sponsored by NSF HDR; integrates sensor and satellite data. & \cite{campolongo2025buildingmachinelearningchallenges3} \href{https://arxiv.org/abs/2503.02112}{$\Rightarrow$ } & 9.0 & Clear anomaly detection objective framed for physical signal discovery (LIGO/Virgo). & 10.0 & Preprocessed waveform data from dual interferometers, public and well-structured. & 9.0 & ROC-AUC, Precision/Recall, and confusion-based metrics are standardized. & 1.0 & No starter model or baseline code linked & 9.0 & Codabench page, GitHub starter kit, and related papers provide strong guidance. \\ \hline
2025-01-24 & yes & Single Qubit Readout on QICK System & \href{https://github.com/fastmachinelearning/ml-quantum-readout}{link} & Quantum Computing & Real-time single-qubit state classification using FPGA firmware & qubit readout, hls4ml, FPGA, QICK & Implements real-time ML models for single-qubit readout on the Quantum Instrumentation Control Kit (QICK), using hls4ml to deploy quantized neural networks on RFSoC FPGAs. Offers high-fidelity, low-latency quantum state discrimination.  & Classification & Single-shot fidelity, inference latency & Accuracy, Latency & hls4ml quantized NN & Achieves {\textasciitilde}96\% fidelity with {\textasciitilde}32 ns latency and low FPGA resource utilization. & \cite{diguglielmo2025endtoendworkflowmachinelearningbased} \href{https://arxiv.org/abs/2501.14663}{$\Rightarrow$ } & 8.0 & Task clearly framed around detecting hybrid species via images, but exact labeling methods and hybrid definitions may need elaboration. & 8.0 & Dataset hosted on Codabench; appears structured but details on image sourcing and labeling pipeline are limited. & 9.0 & Classification accuracy and F1 are standard and appropriate. & 1.0 & No starter model or baseline code linked & 7.5 & Codabench task page describes dataset and evaluation method but lacks full API/docs. \\ \hline
2023-11-20 & yes & GPQA A Graduate Level Google Proof Question and Answer Benchmark & \href{https://arxiv.org/abs/2311.12022}{link} & Science (Biology, Physics, Chemistry) & Graduate-level, expert-validated multiple-choice questions hard even with web access & Google-proof, multiple-choice, expert reasoning, science QA & Contains 448 challenging questions written by domain experts, with expert accuracy at 65\% (74\% discounting clear errors) and non-experts reaching just 34\%. GPT-4 baseline scores {\textasciitilde}39\%{\textemdash}designed for scalable oversight evaluation.  & Multiple choice & Scientific reasoning, knowledge probing & Accuracy & GPT-4 baseline & {\textquotedblleft}Google-proof{\textquotedblright}; supports oversight research. & \cite{rein2023gpqagraduatelevelgoogleproofqa} \href{https://arxiv.org/abs/2311.12022}{$\Rightarrow$ } & 9.0 & Clear dual-modality task (image + time-series); environmental focus is well described. & 9.0 & Time-series and satellite imagery data provided; sensor info and collection intervals are explained. & 9.0 & ROC-AUC, Precision/Recall are appropriate and robust. & 1.0 & No starter model or baseline code linked & 6.5 & Moderate Codabench documentation with climate context; lacks pipeline-level walkthrough. \\ \hline
2024-12-13 & yes & SeafloorAI & \href{https://neurips.cc/virtual/2024/poster/97432}{link} & Marine Science; Vision-Language & Large-scale vision-language dataset for seafloor mapping and geological classification & sonar imagery, vision-language, seafloor mapping, segmentation, QA & A first-of-its-kind dataset covering 17,300 sq km of seafloor with 696K sonar images, 827K segmentation masks, and 696K natural-language descriptions plus {\textasciitilde}7M QA pairs{\textemdash}designed for both vision and language-based ML models in marine science  & Image segmentation, Vision-language QA & Geospatial understanding, multimodal reasoning & Segmentation pixel accuracy, QA accuracy & SegFormer, ViLT-style multimodal models & Data processing code publicly available, covering five geological layers; curated with marine scientists & \cite{nguyen2024seafloorailargescalevisionlanguagedataset} \href{https://arxiv.org/abs/2411.00172}{$\Rightarrow$ } & 9.0 & Real-time qubit classification task clearly defined in quantum instrumentation context. & 9.0 & Dataset available on Zenodo with signal traces; compact and reproducible. & 9.0 & Accuracy and latency are well defined and crucial in this setting. & 9.0 & GitHub repo has reproducible code and HLS firmware targeting FPGA. & 8.0 & Good setup instructions, but no interactive visualization or starter notebook. \\ \hline
2024-12-13 & yes & SuperCon3D & \href{https://neurips.cc/virtual/2024/poster/97553}{link} & Materials Science; Superconductivity & Dataset and models for predicting and generating high-Tc superconductors using 3D crystal structures & superconductivity, crystal structures, equivariant GNN, generative models & SuperCon3D introduces 3D crystal structures with associated critical temperatures (Tc) and two deep-learning models: SODNet (equivariant graph model) and DiffCSP-SC (diffusion generator) designed to screen and synthesize high-Tc candidates.  & Regression (Tc prediction), Generative modeling & Structure-to-property prediction, structure generation & MAE (Tc), Validity of generated structures & SODNet, DiffCSP-SC & Demonstrates advantage of combining ordered and disordered structural data in model design & \cite{zhuang2024supercon3d} & 10.0 & Multimodal task (segmentation + natural language QA pairs);. & 10.0 & sonar imagery + masks + descriptions, georeferenced and labeled with QA & 9.0 & Pixel accuracy and QA metrics clearly defined; tasks split by modality. & 8.0 & Baseline models (SegFormer, ViLT) are cited, partial configs likely available. & 8.5 & Paper + GitHub metadata and processing details are comprehensive, though full dataset is not yet available. \\ \hline
2024-12-13 & yes & GeSS & \href{https://neurips.cc/virtual/2024/poster/97816}{link} & Scientific ML; Geometric Deep Learning & Benchmark suite evaluating geometric deep learning models under real-world distribution shifts & geometric deep learning, distribution shift, OOD robustness, scientific applications & GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access  & Classification, Regression & OOD performance in scientific settings & Accuracy, RMSE, OOD robustness delta & GCN, EGNN, DimeNet++ & Includes no-OOD, unlabeled-OOD, and few-label scenarios & \cite{zou2024gess} & 9.0 & Well-defined problem (Tc prediction, generation) with strong scientific motivation (high-Tc materials), but no formal hardware constraints. & 9.0 & Includes curated 3D crystal structures and Tc data; readily downloadable and used in paper models. & 9.0 & MAE and structural validity used, well-established in materials modeling. & 8.0 & Provides two reference models (SODNet, DiffCSP-SC) with results. Code likely available post-conference. & 8.0 & Paper and poster explain design choices well; software availability confirms reproducibility but limited external documentation. \\ \hline
2024-12-13 & yes & Vocal Call Locator & \href{https://neurips.cc/virtual/2024/poster/97470}{link} & Neuroscience; Bioacoustics & Benchmarking sound-source localization of rodent vocalizations from multi-channel audio & source localization, bioacoustics, time-series, SSL & The first large-scale benchmark (767K sounds across 9 conditions) for localizing rodent vocal calls using synchronized audio and video in standard lab environments, enabling systematic evaluation of sound-source localization algorithms in bioacoustics  & Sound source localization & Source localization accuracy in bioacoustic settings & Localization error (cm), Recall/Precision & CNN-based SSL models & Dataset spans real, simulated, and mixed audio; supports benchmarking across data types & \cite{peterson2024vcl} \href{https://neurips.cc/virtual/2024/poster/97470}{$\Rightarrow$ } & 9.0 & Clear benchmark scenarios across GDL tasks under multiple real-world shift settings; OOD settings precisely categorized. & 8.0 & Scientific graph datasets provided in multiple shift regimes; standardized splits across domains. Exact format of data not specified. & 9.0 & Includes base metrics (accuracy, RMSE) plus OOD delta robustness for evaluation under shifts. & 9.0 & Multiple baselines (11 algorithms x 3 backbones) evaluated; setup supports reproducible comparison. & 2.0 & Paper, poster, and source code provide thorough access to methodology and implementation. Setup instructions and accompanying code not present. \\ \hline
2024-12-13 & yes & MassSpecGym & \href{https://neurips.cc/virtual/2024/poster/97823}{link} & Cheminformatics; Molecular Discovery & Benchmark suite for discovery and identification of molecules via MS/MS & mass spectrometry, molecular structure, de novo generation, retrieval, dataset & MassSpecGym curates the largest public MS/MS dataset with three standardized tasks{\textemdash}de novo structure generation, molecule retrieval, and spectrum simulation{\textemdash}using challenging generalization splits to propel ML-driven molecule discovery  & De novo generation, Retrieval, Simulation & Molecular identification and generation from spectral data & Structure accuracy, Retrieval precision, Simulation MSE & Graph-based generative models, Retrieval baselines & Dataset{\textasciitilde}\ensuremath{>}1M spectra; open-source GitHub repo; widely cited as a go-to benchmark for MS/MS tasks & \cite{bushuiev2024massspecgym} \href{https://neurips.cc/virtual/2024/poster/97823}{$\Rightarrow$ } & 9.0 & Focused on sound source localization for rodent vocalizations in lab settings; well-scoped. & 9.5 & 767000 annotated audio segments across diverse conditions. Minor deduction for no train/test/valid split. & 9.5 & Localization error, precision/recall used & 7.0 & CNN-based baselines referenced but unclear whether pretrained models or training code are available. & 2.0 & Poster and paper outline benchmark intent and setup; repo expected but not confirmed in dataset card. \\ \hline
2024-12-13 & yes & Urban Data Layer & \href{https://neurips.cc/virtual/2024/poster/97837}{link} & Urban Computing; Data Engineering & Unified data pipeline for multi-modal urban science research & data pipeline, urban science, multi-modal, benchmark & UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks.  & Prediction, Classification & Multi-modal urban inference, standardization & Task-specific accuracy or RMSE & Baseline regression/classification pipelines & Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models & \cite{wang2024urbandatalayer} \href{https://neurips.cc/virtual/2024/poster/97837}{$\Rightarrow$ } & 9.0 & Three tasks (de novo generation, retrieval, simulation) are clearly defined for MS/MS molecule discovery. & 10.0 & Over 1 million spectra with structure annotations; dataset is open-source and well-documented. & 9.0 & Task-appropriate metrics (structure accuracy, precision, MSE) are specified and used consistently. & 8.0 & Baseline models are available (graph-based and retrieval), though not exhaustive. & 9.0 & GitHub repo and poster provide code and reproducibility guidance. \\ \hline
2024-12-13 & yes & Delta Squared-DFT & \href{https://neurips.cc/virtual/2024/poster/97788}{link} & Computational Chemistry; Materials Science & Benchmarking machine-learning corrections to DFT using Delta Squared-trained models for reaction energies & density functional theory, Delta Squared-ML correction, reaction energetics, quantum chemistry & Introduces the Delta Squared-ML paradigm{\textemdash}using ML corrections to DFT to predict reaction energies with accuracy comparable to CCSD(T), while training on small CC datasets. Evaluated across 10 reaction datasets covering organic and organometallic transformations.  & Regression & High-accuracy energy prediction, DFT correction & Mean Absolute Error (eV), Energy ranking accuracy & Delta Squared-ML correction networks, Kernel ridge regression & Demonstrates CC-level accuracy with {\textasciitilde}1\% of high-level data. Benchmarks publicly included for reproducibility. & \cite{liu2024delta2dft} \href{https://neurips.cc/virtual/2024/poster/97788}{$\Rightarrow$ } & 8.0 & Clear goals around unifying urban data formats and tasks (e.g., air quality prediction), though some specifics could be more formal. & 9.0 & Multi-modal data is standardized and accessible; GitHub repo available. & 8.0 & Uses common task metrics like accuracy/RMSE, though varies by task. & 7.0 & Baseline regression/classification models included. & 8.0 & Source code supports pipeline reuse, but formal evaluation splits may vary. \\ \hline
2024-12-13 & yes & LLMs for Crop Science & \href{https://neurips.cc/virtual/2024/poster/97570}{link} & Agricultural Science; NLP & Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific prompts & crop science, prompt engineering, domain adaptation, question answering & Establishes a benchmark of 3,500 expert-annotated prompts and QA pairs covering crop traits, growth stages, and environmental interactions. Tests GPT-style LLMs on accuracy and domain reasoning using in-context, chain-of-thought, and retrieval-augmented prompts.  & Question Answering, Inference & Scientific knowledge, crop reasoning & Accuracy, F1 score & GPT-4, LLaMA-2-13B, T5-XXL & Includes examples with retrieval-augmented and chain-of-thought prompt templates; supports few-shot adaptation. & \cite{patel2024llmcropsci} \href{https://neurips.cc/virtual/2024/poster/97570}{$\Rightarrow$ } & 9.0 & The task of ML correction to DFT energy predictions is well-specified. & 9.0 & 10 public reaction datasets with DFT and CC references; well-documented. & 8.0 & Uses MAE and ranking accuracy, suitable for this task. & 8.0 & Includes both \ensuremath{\Delta}{\texttwosuperior}-ML and KRR baselines. & 9.0 & Public benchmarks and clear reproducibility via datasets and model code. \\ \hline
2024-12-13 & yes & SPIQA LLM & \href{https://neurips.cc/virtual/2024/poster/97575}{link} & Multimodal Scientific QA; Computer Vision & Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter performance) & multimodal QA, scientific figures, image+text, chain-of-thought prompting & A workshop version of SPIQA comparing 10 LLM adapter methods on the SPIQA benchmark with scientific diagram/questions. Highlights performance differences between chain-of-thought and end-to-end adapter models.  & Multimodal QA & Visual reasoning, scientific figure understanding & Accuracy, F1 score & LLaVA, MiniGPT-4, Owl-LLM adapter variants & Companion to SPIQA main benchmark; compares adapter strategies using same images and QA pairs. & \cite{zhong2024spiqa_llm} \href{https://neurips.cc/virtual/2024/poster/97575}{$\Rightarrow$ } & 6.0 & Task of QA over scientific figures is interesting but not fully formalized in input/output terms. & 6.0 & Uses SPIQA dataset with {\textasciitilde}10 adapters; figures and questions are included, but not fully open. & 7.0 & Reports accuracy and F1; fair but no visual reasoning-specific metric. & 6.0 & 10 LLM adapter baselines; results included. & 5.0 & Poster paper and limited documentation; no reproducibility instructions. \\ \hline
\end{longtable}
}
\end{landscape}
\printbibliography

\end{document}
