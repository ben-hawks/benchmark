# CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)

**Date**: 2024-04-02

**Expiration**: 

**Valid**: yes

**Name**: CURIE  Scientific Long-Context Understanding, Reasoning and Information Extraction 

**URL**: https://arxiv.org/abs/2404.02029

**Domain**: Multidomain Science

**Focus**: Long-context scientific reasoning

**Keywords**: long-context, information extraction, multimodal

**Description**: CURIE is a benchmark of 580 problems across six scientific disciplines—materials science, quantum computing, biology, chemistry, climate science, and astrophysics— designed to evaluate LLMs on long-context understanding, reasoning, and information  extraction in realistic scientific workflows. 

**Task Types**: Information extraction, Reasoning, Concept tracking, Aggregation, Algebraic manipulation, Multimodal comprehension

**AI Capability**: Long-context understanding and scientific reasoning

**Metrics**: Accuracy

**Models**: 

**Notes**: Good

**Citation**:

-
  - type: misc
  - id: curie2024
  - url: https://arxiv.org/abs/2404.02029
  - year: 2024
  - author: TODO: Add authors
  - title: Scientific Reasoning Benchmarks from the CURIE Dataset
  - bibtex: |
      @misc{curie2024,
        title={Scientific Reasoning Benchmarks from the CURIE Dataset},
        author={TODO: Add authors},
        year={2024},
        url={https://arxiv.org/abs/2404.02029}
      }

