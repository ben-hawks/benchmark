<h1
id="gpqa-a-graduate-level-google-proof-question-and-answer-benchmark">GPQA:
A Graduate-Level Google-Proof Question and Answer Benchmark</h1>
<p><strong>Edit:</strong> <a
href="https://github.com/mlcommons-science/benchmark/tree/main/source">edit
this entry</a></p>
<p><strong>Date</strong>: 2023-11-20</p>
<p><strong>Name</strong>: GPQA: A Graduate-Level Google-Proof Question
and Answer Benchmark</p>
<p><strong>Domain</strong>: Science Biology, Physics, Chemistry</p>
<p><strong>Focus</strong>: Graduate-level, expert-validated
multiple-choice questions hard even with web access</p>
<p><strong>Keywords</strong>: Google-proof, multiple-choice, expert
reasoning, science QA</p>
<p><strong>Task Types</strong>: Multiple choice</p>
<p><strong>Metrics</strong>: Accuracy</p>
<p><strong>Models</strong>: GPT-4 baseline</p>
<p><strong>Citation</strong>:</p>
<ul>
<li><p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty,
Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R.
Bowman. Gpqa: a graduate-level google-proof q and a benchmark. 2023.
URL: https://arxiv.org/abs/2311.12022, arXiv:2311.12022.</p>
<ul>
<li><p>bibtex: ``` <span class="citation"
data-cites="misc">@misc</span>{rein2023gpqagraduatelevelgoogleproofqa2,</p>
<pre><code>archiveprefix = {arXiv},

author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},

eprint        = {2311.12022},

primaryclass  = {cs.AI},

title         = {GPQA: A Graduate-Level Google-Proof Q and A Benchmark},

url           = {https://arxiv.org/abs/2311.12022},

year          = {2023}</code></pre>
<p>}</p>
<p>```</p></li>
</ul></li>
</ul>
<p><strong>Ratings:</strong></p>
<p>Software:</p>
<ul>
<li><p><strong>Rating:</strong> 3</p></li>
<li><p><strong>Reason:</strong> Dataset and benchmark materials are
publicly available via HuggingFace and GitHub, but no integrated
runnable code or software framework is provided.</p></li>
</ul>
<p>Specification:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Task is clearly defined as a
multiple-choice benchmark requiring expert-level scientific reasoning.
Input/output formats and evaluation criteria are well
described.</p></li>
</ul>
<p>Dataset:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> The GPQA dataset is publicly released,
well curated, with metadata and clearly documented splits.</p></li>
</ul>
<p>Metrics:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Accuracy is the primary metric and is
clearly defined and appropriate for multiple-choice QA.</p></li>
</ul>
<p>Reference Solution:</p>
<ul>
<li><p><strong>Rating:</strong> 1</p></li>
<li><p><strong>Reason:</strong> No baseline implementations or starter
code are linked or provided for reproduction.</p></li>
</ul>
<p>Documentation:</p>
<ul>
<li><p><strong>Rating:</strong> 3</p></li>
<li><p><strong>Reason:</strong> Documentation includes dataset
description and benchmark instructions, but lacks detailed usage
tutorials or pipelines.</p></li>
</ul>
<p><strong>Average Rating:</strong> 3.667</p>
<p><strong>Radar Plot:</strong> <img
src="../../tex/images/gpqa_a_graduate-level_google-proof_question_and_answer_benchmark_radar.png"
alt="Gpqa A Graduate-Level Google-Proof Question And Answer Benchmark radar plot" /></p>
