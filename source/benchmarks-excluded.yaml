# This file contains benchmarks that are excluded from the main benchmarks.yaml file.
# They may be excluded for various reasons, such as being a platform rather than a specific benchmark,
# or not meeting certain criteria. This file is maintained for record-keeping and potential future reference

# Exclusion Reason: Platform for hosting benchmarks, not a benchmark itself
- date: "2022-01-01"
  version: "v1.0"
  last_updated: "2025-03"
  expired: "unknown"
  valid: "yes"
  valid_date: "2022-01-01"
  name: "Codabench"
  url: "https://www.codabench.org/"
  doi: "https://doi.org/10.1016/j.patter.2022.100543"
  domain:
  - General ML
  - Multiple
  focus: "Open-source platform for organizing reproducible AI benchmarks and competitions"
  keywords:
  - "benchmark platform"
  - "code submission"
  - "competitions"
  - "meta-benchmark"
  summary: |
    Codabench (successor to CodaLab) is a flexible, easy-to-use, reproducible API platform for hosting AI benchmarks
    and code-submission challenges. It supports custom scoring, inverted benchmarks, and scalable public or private queues .
  licensing: "https://github.com/codalab/codalab-competitions/wiki/Privacy"
  task_types:
  - "Multiple"
  ai_capability_measured:
  - "Model reproducibility"
  - "performance across datasets"
  metrics:
  - "Submission count"
  - "Leaderboard ranking"
  - "Task-specific metrics"
  models:
  - "Arbitrary code submissions"
  ml_motif:
  - "Multiple"
  type: "Platform"
  ml_task:
  - "Multiple"
  solutions: "Several"
  notes: |
    Hosts 51 public competitions, ~26 k users, 177 k submissions
  contact:
    name: "Isabelle Guyon (Université Paris-Saclay)"
    email: "unknown"
  cite:
  - |
    @article{xu-2022,
      author    = {Xu, Zhen and Escalera, Sergio and Pavão, Adrien and Richard, Magali and Tu, Wei-Wei and Yao, Quanming and Zhao, Huan and Guyon, Isabelle},
      doi       = {10.1016/j.patter.2022.100543},
      issn      = {2666-3899},
      journal   = {Patterns},
      month     = jul,
      number    = {7},
      pages     = {100543},
      publisher = {Elsevier BV},
      title     = {Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform},
      url       = {http://dx.doi.org/10.1016/j.patter.2022.100543},
      volume    = {3},
      year      = {2022}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1sIwNDCs01s790DApVt5leiG8UaBDFVINA4bOixZ1CUw/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    specification:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    dataset:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    metrics:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    reference_solution:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.
    documentation:
      rating: 1
      reason: |
        This is a platform for posting benchmarks, not a benchmark in itself.


# Exclusion Reason: Dashboard & Benchmark for pure inference performance of vLLM Library, not a scientific benchmark
- date: "2022-06-22"
  version: "v1.0"
  last_updated: "2025-01"
  expired: "unknown"
  valid: "yes"
  valid_date: "2022-06-22"
  name: "vLLM Performance Dashboard"
  url: "https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/"
  doi: "unknown"
  domain:
  - LLM
  - HPC/inference
  focus: "Interactive dashboard showing inference performance of vLLM"
  keywords:
  - "Dashboard"
  - "Throughput visualization"
  - "Latency analysis"
  - "Metric tracking"
  summary: |
    A live visual dashboard for vLLM showcasing throughput, latency, and other inference metrics across models and hardware configurations.
  licensing: "unknown"
  task_types:
  - "Performance visualization"
  ai_capability_measured:
  - "Throughput"
  - "latency"
  - "hardware utilization"
  metrics:
  - "Tokens/sec"
  - "TTFT"
  - "Memory usage"
  models:
  - "LLaMA-2"
  - "Mistral"
  - "Qwen"
  ml_motif:
  - "HPC/inference"
  type: "Framework"
  ml_task:
  - "Visualization"
  solutions: "0"
  notes: |
    Built using ObservableHQ; integrates live data from vLLM benchmarks.
    The URL requires a login to access the content.
  contact:
    name: "Simon Mo"
    email: "unknown"
  cite:
  - |
    @misc{mo2024vllm_dashboard,
      title={vLLM Performance Dashboard},
      author={Mo, Simon},
      year={2024},
      url={https://simon-mo-workspace.observablehq.cloud/vllm-dashboard-v0/}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "(none)"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 4
      reason: |
        Interactive dashboard built with ObservableHQ and linked to vLLM benchmarks.
        Source code is not fully open, but backend integration with vLLM is well-maintained.
    specification:
      rating: 4
      reason: |
        While primarily a visualization tool, it includes benchmark configurations,
        metric definitions, and supports comparison across models and hardware.
    dataset:
      rating: 2
      reason: |
        No datasets are bundled; the dashboard visualizes metrics derived from model
        inference logs or external endpoints, not a formal dataset.
    metrics:
      rating: 4
      reason: |
        Tracks tokens/sec, TTFT, memory usage, and platform comparisons. Metrics are clear
        but focused on visualization rather than statistical robustness.
    reference_solution:
      rating: 3
      reason: |
        Dashboards include reproducible views of benchmarked models, but do not ship
        with runnable model code. Relies on external serving infrastructure.
    documentation:
      rating: 4
      reason: |
        Public dashboard with instructions and tooltips; documentation is clear, though access
        is restricted (login required) and backend setup is opaque to users.

# Exclusion Reason: Library for serving LLMs, Benchmark is for library's inference performance, not a scientific benchmark
- date: "2023-09-12"
  version: "v0.10.0"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-09-12"
  name: "vLLM Inference and Serving Engine"
  url: "https://github.com/vllm-project/vllm/tree/main/benchmarks"
  doi: "unknown"
  domain:
  - LLM
  - HPC/inference
  focus: "High-throughput, memory-efficient inference and serving engine for LLMs"
  keywords:
  - "LLM inference"
  - "PagedAttention"
  - "CUDA graph"
  - "streaming API"
  - "quantization"
  summary: |
    vLLM is a fast, high-throughput, memory-efficient inference and serving engine for large language models, 
    featuring PagedAttention, continuous batching, and support for quantized and pipelined model execution. 
    Benchmarks compare it to TensorRT-LLM, SGLang, and others.
  licensing: "Apache License 2.0"
  task_types:
  - "Inference Benchmarking"
  ai_capability_measured:
  - "Throughput"
  - "latency"
  - "memory efficiency"
  metrics:
  - "Tokens/sec"
  - "Time to First Token (TTFT)"
  - "Memory footprint"
  models:
  - "LLaMA"
  - "Mixtral"
  - "FlashAttention-based models"
  ml_motif:
  - "HPC/inference"
  type: "Framework"
  ml_task:
  - "Inference"
  solutions: "0"
  notes: |
    Incubated by LF AI and Data; achieves up to 24x throughput over HuggingFace Transformers
  contact:
    name: "Woosuk Kwon (vLLM Team)"
    email: "unknown"
  cite:
  - |
    @inproceedings{10.1145/3600006.3613165,
      author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
      title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
      year = {2023},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3600006.3613165},
      doi = {10.1145/3600006.3613165},
      abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
      booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
      pages = {611-626},
      numpages = {16},
      location = {Koblenz, Germany},
      series = {SOSP '23}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1nPZyRZEZHciGXYNJShR9FCJbre7ImJLKf2MG6g4-3gQ/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Actively maintained open-source project under Apache 2.0. GitHub repo includes
        full serving engine, benchmarking scripts, CUDA integration, and deployment examples.
    specification:
      rating: 5
      reason: |
        Inference benchmarks are well-defined with clear input/output formats and platform-specific constraints.
        Covers multiple models, hardware backends, and batching configurations.
    dataset:
      rating: 3
      reason: |
        No traditional dataset is included. Instead, it uses structured configs and logs suitable for inference benchmarking.
        FAIR principles are only partially applicable.
    metrics:
      rating: 5
      reason: |
        Comprehensive performance metrics like tokens/sec, time-to-first-token (TTFT), and memory footprint
        are consistently applied and benchmarked across frameworks.
    reference_solution:
      rating: 4
      reason: |
        Provides runnable scripts and configs for several models (LLaMA, Mixtral, etc.) across platforms.
        Baselines are reproducible, though not all models are fully wrapped or hosted.
    documentation:
      rating: 4
      reason: |
        Well-structured GitHub documentation with setup instructions, config examples, benchmarking comparisons,
        and performance tuning guides.
# Exclusion Reason: Library and framework for serving LLMs and VLMs, not a scientific benchmark
- date: "2023-12-12"
  version: "v0.4.9"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-12-12"
  name: "SGLang Framework"
  url: "https://github.com/sgl-project/sglang/tree/main/benchmark"
  doi: "10.48550/arXiv.2312.07104"
  domain:
  - LLM Vision
  focus: "Fast serving framework for LLMs and vision-language models"
  keywords:
  - "LLM serving"
  - "vision-language"
  - "RadixAttention"
  - "performance"
  - "JSON decoding"
  summary: |
    A high-performance open-source serving framework combining efficient backend runtime (RadixAttention, batching, quantization) and expressive frontend language, boosting LLM/VLM inference throughput up to ~3x over alternatives.
  licensing: "Apache License 2.0"
  task_types:
  - "Model serving framework"
  ai_capability_measured:
  - "Serving throughput"
  - "JSON/task-specific latency"
  metrics:
  - "Tokens/sec"
  - "Time-to-first-token"
  - "Throughput gain vs baseline"
  models:
  - "LLaVA"
  - "DeepSeek"
  - "Llama"
  ml_motif:
  - "LLM Vision"
  type: "Framework"
  ml_task:
  - "Model serving"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Deployed in production (xAI, NVIDIA, Google Cloud); v0.4.8 release June 2025.
  contact:
    name: "SGLang Team"
    email: "unknown"
  cite:
  - |
    @misc{zheng2024sglangefficientexecutionstructured,
      archiveprefix = {arXiv},
      author        = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
      eprint        = {2312.07104},
      primaryclass  = {cs.AI},
      title         = {SGLang: Efficient Execution of Structured Language Model Programs},
      url           = {https://arxiv.org/abs/2312.07104},
      year          = {2024}
    }
  datasets:
    links:
    - name: "Benchmark configs"
      url: ""
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Actively maintained and production-deployed (e.g., xAI, NVIDIA); source code available under
        Apache 2.0. Includes efficient backends (RadixAttention, quantization, batching) and full
        serving infrastructure.
    specification:
      rating: 4
      reason: |
        The framework clearly defines performance targets, serving logic, and model integration.
        Input/output expectations are consistent, but not all benchmarks are standardized.
    dataset:
      rating: 2
      reason: |
        Does not introduce new datasets; instead, it evaluates performance using existing model benchmarks.
        Only configuration files are included.
    metrics:
      rating: 5
      reason: |
        Serving-related metrics such as tokens/sec, time-to-first-token, and throughput gain vs. baselines
        are well-defined and consistently applied.
    reference_solution:
      rating: 3
      reason: |
        Provides benchmark configs and example integrations (e.g., with LLaVA, DeepSeek), but not all
        models or scripts are runnable out-of-the-box.
    documentation:
      rating: 4
      reason: |
        Strong GitHub documentation, install guides, and benchmarks. Some advanced topics (e.g.,
        scaling, hardware tuning) could use deeper walkthroughs.

# Exclusion Reason: Not a scientific benchmark specifically
- date: "2020-01-01"
  version: "v1.0"
  last_updated: "2020-01"
  expired: "unknown"
  valid: "yes"
  valid_date: "2020-01-01"
  name: "BenchCouncil AIBench"
  url: "https://www.benchcouncil.org/AIBench/"
  doi: "10.48550/arXiv.1908.08998"
  domain:
  - General
  focus: "End-to-end AI benchmarking across micro, component, and application levels"
  keywords:
  - "benchmarking"
  - "AI systems"
  - "application-level evaluation"
  summary: |
    AIBench is a comprehensive benchmark suite that evaluates AI workloads at different levels (micro, component, application) across hardware systems-covering image generation, object detection, translation, recommendation, video prediction, etc.
  licensing: "Apache License 2.0"
  task_types:
  - "Training"
  - "Inference"
  - "End-to-end AI workloads"
  ai_capability_measured:
  - "System-level AI workload performance"
  metrics:
  - "Throughput"
  - "Latency"
  - "Accuracy"
  models:
  - "ResNet"
  - "BERT"
  - "GANs"
  - "Recommendation systems"
  ml_motif:
  - "General"
  type: "Benchmark"
  ml_task:
  - "NA"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Covers scenario-distilling, micro, component, and end-to-end benchmarks.
  contact:
    name: "Wanling Gao (BenchCouncil)"
    email: "unknown"
  cite:
  - |
    @misc{gao2019aibenchindustrystandardinternet,
      archiveprefix = {arXiv},
      author        = {Wanling Gao and Fei Tang and Lei Wang and Jianfeng Zhan and Chunxin Lan and Chunjie Luo and Yunyou Huang and Chen Zheng and Jiahui Dai and Zheng Cao and Daoyi Zheng and Haoning Tang and Kunlin Zhan and Biao Wang and Defei Kong and Tong Wu and Minghe Yu and Chongkang Tan and Huan Li and Xinhui Tian and Yatao Li and Junchao Shao and Zhenyu Wang and Xiaoyu Wang and Hainan Ye},
      eprint        = {1908.08998},
      primaryclass  = {cs.CV},
      title         = {AIBench: An Industry Standard Internet Service AI Benchmark Suite},
      url           = {https://arxiv.org/abs/1908.08998},
      year          = {2019}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1scxhARd4vzEaWpVfwKPF_nTSxv4DirlQqcGlSG0yzJc/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        No containerized or automated implementation provided for full benchmark suite
    specification:
      rating: 4
      reason: |
        Task coverage is broad and well-scoped, but system constraints and expected outputs are not uniformly defined
    dataset:
      rating: 3
      reason: |
        Multiple datasets are mentioned, but not consistently FAIR-documented, versioned, or linked
    metrics:
      rating: 4
      reason: |
        Metrics are appropriate, but standardization and reproducibility across tasks vary
    reference_solution:
      rating: 3
      reason: |
        Reference models (e.g., ResNet, BERT) described; no turnkey implementation or results repository for all levels
    documentation:
      rating: 3
      reason: |
        Paper is comprehensive, but minimal user-facing documentation or structured reproduction guide
# Exclusion Reason: Not a scientific benchmark specifically
- date: "2020-01-01"
  version: "v1.0"
  last_updated: "2020-01"
  expired: "unknown"
  valid: "yes"
  valid_date: "2020-01-01"
  name: "BenchCouncil BigDataBench"
  url: "https://www.benchcouncil.org/BigDataBench/"
  doi: "10.48550/arXiv.1802.08254"
  domain:
  - General
  focus: "Big data and AI benchmarking across structured, semi-structured, and unstructured
    data workloads"
  keywords:
  - "big data"
  - "AI benchmarking"
  - "data analytics"
  summary: |
    BigDataBench provides benchmarks for evaluating big data and AI workloads with realistic datasets (13 sources) and pipelines across analytics, graph, warehouse, NoSQL, streaming, and AI.
  licensing: "Apache License 2.0"
  task_types:
  - "Data preprocessing"
  - "Inference"
  - "End-to-end data pipelines"
  ai_capability_measured:
  - "Data processing and AI model inference performance at scale"
  metrics:
  - "Data throughput"
  - "Latency"
  - "Accuracy"
  models:
  - "CNN"
  - "LSTM"
  - "SVM"
  - "XGBoost"
  ml_motif:
  - "General"
  type: "Benchmark"
  ml_task:
  - "NA"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Built on eight data motifs; provides Hadoop, Spark, Flink, MPI implementations.
  contact:
    name: "Jianfeng Zhan (BenchCouncil)"
    email: "unknown"
  cite:
  - |
    @misc{gao2018bigdatabenchscalableunifiedbig,
      archiveprefix = {arXiv},
      author        = {Wanling Gao and Jianfeng Zhan and Lei Wang and Chunjie Luo and Daoyi Zheng and Xu Wen and Rui Ren and Chen Zheng and Xiwen He and Hainan Ye and Haoning Tang and Zheng Cao and Shujie Zhang and Jiahui Dai},
      eprint        = {1802.08254},
      primaryclass  = {cs.DC},
      title         = {BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite},
      url           = {https://arxiv.org/abs/1802.08254},
      year          = {2018}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1FlvWeGm_J5QabOL7J0RWN3udzl0QFDs7wafptXx8sRU/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: "https://docs.google.com/document/d/1VFRxhR2G5A83S8PqKBrP99LLVgcCGvX2WW4vTtwxmQ4/edit?usp=sharing"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        No automated setup across all tasks; some components require manual integration.
    specification:
      rating: 4
      reason: |
        Specific I/O formats and hardware constraints are not uniformly detailed across all tasks.
    dataset:
      rating: 4
      reason: |
        Some datasets lack consistent versioning or rich metadata annotations.
    metrics:
      rating: 5
      reason: |
        None
    reference_solution:
      rating: 4
      reason: |
        Not all benchmark components have fully reproducible baselines; deployment across platforms is fragmented.
    documentation:
      rating: 4
      reason: |
        Setup requires manual steps; some task-specific instructions lack clarity.
# Exclusion Reason: Platform for hosting code for scientific papers, not a benchmark itself
- date: "ongoing"
  version: "v1.0"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "ongoing"
  name: "Papers With Code (SOTA Platform)"
  url: "https://paperswithcode.com/sota"
  doi: "unknown"
  domain:
  - General ML
  - All domains
  focus: "Open platform tracking state-of-the-art results, benchmarks, and implementations
    across ML tasks and papers"
  keywords:
  - "leaderboard"
  - "benchmarking"
  - "reproducibility"
  - "open-source"
  summary: |
    Papers With Code (PWC) aggregates benchmark suites, tasks, and code across ML research:
    12,423 benchmarks, 5,358 unique tasks, and 154,766 papers with code links. It tracks SOTA metrics and fosters reproducibility.
  licensing: "Apache License 2.0"
  task_types:
  - "Multiple (Classification, Detection, NLP, etc.)"
  ai_capability_measured:
  - "Model performance across tasks (accuracy"
  - "F1"
  - "BLEU"
  - "etc.)"
  metrics:
  - "Task-specific (Accuracy, F1, BLEU, etc.)"
  models:
  - "All published models with code"
  ml_motif:
  - "Multiple"
  type: "Platform"
  ml_task:
  - "Multiple"
  solutions: "0"
  notes: |
    Community-driven open platform; automatic data extraction and versioning.
  contact:
    name: "Papers With Code Team"
    email: "unknown"
  cite:
  - |
    @InProceedings{pmlr-v37-blum15,
      title =    {The Ladder: A Reliable Leaderboard for Machine Learning Competitions},
      author =   {Blum, Avrim and Hardt, Moritz},
      booktitle =        {Proceedings of the 32nd International Conference on Machine Learning},
      pages =    {1006--1014},
      year =     {2015},
      editor =   {Bach, Francis and Blei, David},
      volume =   {37},
      series =   {Proceedings of Machine Learning Research},
      address =          {Lille, France},
      month =    jul,
      publisher =    {PMLR},
      pdf =      {http://proceedings.mlr.press/v37/blum15.pdf},
      url =      {https://proceedings.mlr.press/v37/blum15.html},
      abstract =         {The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of re-submission. In this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from a Kaggle competition. Notably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1pbn_K20t6Kr0zxdUSAL68ChruOu6B_x5sZbp-dcrG_g/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Actively maintained open-source platform (https://paperswithcode.com) under Apache 2.0 license;
        includes automatic integration with GitHub, datasets, and models for reproducibility.
    specification:
      rating: 4
      reason: |
        Task and benchmark structures are well organized and standardized, but due to its broad coverage,
        input/output formats vary significantly between tasks and are not always tightly controlled.
    dataset:
      rating: 3
      reason: |
        Relies on external datasets submitted by the community. While links are available, FAIR compliance
        is not guaranteed or systematically enforced across all benchmarks.
    metrics:
      rating: 5
      reason: |
        Tracks state-of-the-art using task-specific metrics like Accuracy, F1, BLEU, etc., with consistent
        aggregation and historical SOTA tracking.
    reference_solution:
      rating: 3
      reason: |
        Provides links to implementations of many SOTA models, but no single unified reference baseline
        is required or maintained per benchmark.
    documentation:
      rating: 4
      reason: |
        Strong front-end documentation and metadata on benchmarks, tasks, and models; however, some benchmark-specific
        instructions are sparse or dependent on external paper links.
# Exclusion Reason: LLM Performance evaluation suite, not a scientific benchmark specifically
- date: "2024-10-31"
  version: "v1.0"
  last_updated: "2024-11"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-10-31"
  name: "LLM-Inference-Bench"
  url: "https://github.com/argonne-lcf/LLM-Inference-Bench"
  doi: "unknown"
  domain:
  - LLM
  - HPC/inference
  focus: "Hardware performance benchmarking of LLMs on AI accelerators"
  keywords:
  - "LLM"
  - "inference benchmarking"
  - "GPU"
  - "accelerator"
  - "throughput"
  summary: |
    A suite evaluating inference performance of LLMs (LLaMA, Mistral, Qwen) across diverse accelerators (NVIDIA, AMD, Intel, SambaNova) and frameworks (vLLM, DeepSpeed-MII, etc.), with an interactive dashboard and per-platform metrics.
  licensing: "BSD 3-Clause New or Revised License"
  task_types:
  - "Inference Benchmarking"
  ai_capability_measured:
  - "Inference throughput"
  - "latency"
  - "hardware utilization"
  metrics:
  - "Token throughput (tok/s)"
  - "Latency"
  - "Framework-hardware mix performance"
  models:
  - "LLaMA-2-7B"
  - "LLaMA-2-70B"
  - "Mistral-7B"
  - "Qwen-7B"
  ml_motif:
  - "HPC/inference"
  type: "Dataset"
  ml_task:
  - "Inference Benchmarking"
  solutions: "0"
  notes: |
    Licensed under BSD-3, maintained by Argonne; supports GPUs and accelerators.
  contact:
    name: "Krishna Teja Chitty-Venkata (Argonne LCF)"
    email: "unknown"
  cite:
  - |
    @INPROCEEDINGS{10820566,
      author={Chitty-Venkata, Krishna Teja and Raskar, Siddhisanket and Kale, Bharat and Ferdaus, Farah and Tanikanti, Aditya and Raffenetti, Ken and Taylor, Valerie and Emani, Murali and Vishwanath, Venkatram},
      booktitle={SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
      title={LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators}, 
      year={2024},
      volume={},
      number={},
      pages={1362-1379},
      doi={10.1109/SCW63240.2024.00178}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1I3UvByGn4KaruQC1pi6XcfoAOzt4iiA61S0nR9ovC94/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Public GitHub repository under BSD-3 license.
        Includes scripts, configurations, and dashboards for running and visualizing LLM inference benchmarks
        across multiple accelerator platforms.
    specification:
      rating: 5
      reason: |
        Benchmark scope, models, accelerator targets, and supported frameworks are clearly specified.
        Input configurations and output metrics are standardized across hardware types.
    dataset:
      rating: 2
      reason: |
        No novel dataset is introduced; benchmark relies on pre-trained LLMs and synthetic inference inputs.
        Dataset structure and FAIR considerations are minimal.
    metrics:
      rating: 5
      reason: |
        Hardware-specific metrics (token throughput, latency, utilization) are well-defined, consistently measured,
        and aggregated in dashboards.
    reference_solution:
      rating: 3
      reason: |
        Inference configurations and baseline performance results are provided, but there are no
        full reference training pipelines or model implementations.
    documentation:
      rating: 4
      reason: |
        GitHub repo provides clear usage instructions, setup guides, and interactive dashboard tooling.
        Some areas like benchmarking extensions or advanced tuning are less detailed.