# MLCommons Science Working Group AI Benchmark Collection

This repository holds a list of scientific AI benchmarks maintained by the MLCommons Science Working Group. Benchmark definitions live in YAML, and we provide scripts to validate, summarize, and publish the collection. Contributions from the community are welcome.

## Quick Links

- Benchmark website: <https://mlcommons-science.github.io/benchmark/>
- Ontology sources (current): `source/benchmarks.yaml`, `source/benchmarks-addon.yaml`
- YAML schema and example: `source/benchmarks-format.yaml`, `source/benchmarks-sample.yaml`
- Benchmark format guide: `docs/benchmark-format.md`
- Tooling reference: `docs/tooling.md`
- Rating system: `docs/ratings.md`
- Contribution guide: `CONTRIBUTING.md`

## What Lives in This Repository

- Versioned benchmark metadata expressed in YAML, including citations, FAIR scores, and ratings.
- A Python toolchain (`bin/generate.py` and supporting modules) that validates entries and produces Markdown, LaTeX, and MkDocs outputs.
- Automation for building the public benchmark site in `www/science-ai-benchmarks/`.

All generated artifacts are derived from the YAML sources. Do not edit rendered files manually.

## Quick Start

### Local Python Environment

```bash
git clone https://github.com/mlcommons-science/benchmark.git
cd benchmark
python3 -m venv .venv
source .venv/bin/activate
make install           # installs dependencies from requirements.txt
```

### Docker Workflow

A Docker image is provided with Python, LaTeX, and the repository tooling preconfigured:

```bash
docker build -t benchmark .
docker run --rm -it -v "$PWD":/workspace -e SERVE_HOST=0.0.0.0 -p 8000:8000 benchmark
```

You can execute the Makefile targets and scripts inside the container.

## Add or Update a Benchmark

1. Create a feature branch in your fork (see `CONTRIBUTING.md` for the full workflow).
2. Use `source/benchmarks-sample.yaml` together with the schema in `source/benchmarks-format.yaml` as a reference. Additional YAML source files can be introduced following the same structure.
3. Apply edits with two-space indentation and use the literal string `"unknown"` when information is unavailable.
4. Run the catalogue checks before committing:
   ```bash
   make check          # ensures required fields are present and flags non-ASCII characters
   make check_url      # optional; validates that referenced URLs resolve
   ```
   If a site blocks automated requests (so the URL works in your browser but still fails in the script), follow the “URL verification workflow” in `docs/tooling.md`.
5. Review your diff so the pull request contains only YAML and documentation updates. Avoid committing changes under `content/` or `www/`.
6. Open a pull request. Maintainers will regenerate Markdown, LaTeX, and MkDocs outputs during review before publishing.

Consult `docs/tooling.md` for the full list of Make targets and script options.

## Repository Layout

- `bin/` – validation and publishing scripts.
- `content/` – generated Markdown, TeX, and MkDocs assets that feed the published site.
- `source/` – benchmark catalogue, schema, and MkDocs homepage (`index.md`).
- `tests/` – unit tests for key scripts.
- `www/` – MkDocs output generated by `make mkdocs`; contains the static site published via GitHub Pages and is ignored by Git.

## How to Cite

If you use this repository, the benchmark ontology, or any derived artifacts, please cite all relevant works associated with this project.

### MLCommons Science Working Group AI Benchmarks Collection
Gregor von Laszewski, Ben Hawks, Marco Colombo, Reece Shiraishi, Anjay Krishnan, Nhan Tran, and Geoffrey C. Fox. 2025. *MLCommons Science Working Group AI Benchmarks Collection.* MLCommons Science Working Group. Available at: https://mlcommons-science.github.io/benchmark/benchmarks.pdf

**BibTeX:**
```bibtex
@misc{mlcommons-benchmarks-collection,
  author = {
    Gregor von Laszewski and
    Ben Hawks and
    Marco Colombo and
    Reece Shiraishi and
    Anjay Krishnan and
    Nhan Tran and
    Geoffrey C. Fox
  },
  title = {MLCommons Science Working Group AI Benchmarks Collection},
  url = {https://mlcommons-science.github.io/benchmark/benchmarks.pdf},
  note = {Online Collection: \url{https://mlcommons-science.github.io/benchmark/}},
  month = jun,
  year = 2025,
  howpublished = {GitHub}
}
```

### An MLCommons Scientific Benchmarks Ontology
Ben Hawks, Gregor von Laszewski, Matthew D. Sinclair, Marco Colombo, Shivaram Venkataraman, Rutwik Jain, Yiwei Jiang, Nhan Tran, and Geoffrey Fox. 2025. *An MLCommons Scientific Benchmarks Ontology.* arXiv:2511.05614.

**BibTeX:**
```bibtex
@misc{hawks2025mlcommons_ontology,
  title={An MLCommons Scientific Benchmarks Ontology},
  author={Ben Hawks and Gregor von Laszewski and Matthew D. Sinclair and Marco Colombo and Shivaram Venkataraman and Rutwik Jain and Yiwei Jiang and Nhan Tran and Geoffrey Fox},
  year={2025},
  eprint={2511.05614},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2511.05614}
}
```

### AI Benchmarks Carpentry and Democratization
Gregor von Laszewski, Wesley Brewer, Jeyan Thiyagalingam, Juri Papay, Armstrong Foundjem, Piotr Luszczek, Murali Emani, Shirley V. Moore, Vijay Janapa Reddi, Matthew D. Sinclair, Sebastian Lobentanzer, Sujata Goswami, Benjamin Hawks, Marco Colombo, Nhan Tran, Christine R. Kirkpatrick, Abdulkareem Alsudais, Gregg Barrett, Tianhao Li, Kirsten Morehouse, Shivaram Venkataraman, Rutwik Jain, Kartik Mathur, Victor Lu, Tejinder Singh, Khojasteh Z. Mirza, Kongtao Chen, Sasidhar Kunapuli, Gavin Farrell, Renato Umeton, and Geoffrey C. Fox. 2025. *AI Benchmark Democratization and Carpentry.* arXiv:2512.11588.

**BibTeX:**
```bibtex
@misc{vonlaszewski2025aibenchmarkdemocratizationcarpentry,
  title = {AI Benchmark Democratization and Carpentry},
  author = {Gregor von Laszewski and Wesley Brewer and Jeyan Thiyagalingam and Juri Papay and Armstrong Foundjem and Piotr Luszczek and Murali Emani and Shirley V. Moore and Vijay Janapa Reddi and Matthew D. Sinclair and Sebastian Lobentanzer and Sujata Goswami and Benjamin Hawks and Marco Colombo and Nhan Tran and Christine R. Kirkpatrick and Abdulkareem Alsudais and Gregg Barrett and Tianhao Li and Kirsten Morehouse and Shivaram Venkataraman and Rutwik Jain and Kartik Mathur and Victor Lu and Tejinder Singh and Khojasteh Z. Mirza and Kongtao Chen and Sasidhar Kunapuli and Gavin Farrell and Renato Umeton and Geoffrey C. Fox},
  year = {2025},
  eprint = {2512.11588},
  archivePrefix = {arXiv},
  primaryClass = {cs.AI},
  url = {https://arxiv.org/abs/2512.11588}
}
```

## License

This project is released under the [Apache 2.0 License](LICENSE).

For questions or suggestions, please open an issue or start a discussion on the MLCommons Science Working Group forums.
