# CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)

**Date**: 2024-04-02

**Expiration**: None

**Valid**: yes

**Name**: CURIE  Scientific Long-Context Understanding, Reasoning and Information Extraction 

**URL**: https://arxiv.org/abs/2404.02029

**Domain**: Multidomain Science

**Focus**: Long-context scientific reasoning

**Keyword**: long-context, information extraction, multimodal

**Description**: CURIE is a benchmark of 580 problems across six scientific disciplines—materials science, quantum computing, biology, chemistry, climate science, and astrophysics— designed to evaluate LLMs on long-context understanding, reasoning, and information  extraction in realistic scientific workflows. 

**Task Types**: Information extraction, Reasoning, Concept tracking, Aggregation, Algebraic manipulation, Multimodal comprehension

**AI Capability**: Long-context understanding and scientific reasoning

**Metrics**: Accuracy

**Models**: 

**Notes**: Good

**Citation**: @misc{curie2024, title={Scientific Reasoning Benchmarks from the CURIE Dataset}, author={TODO: Add authors}, year={2024}, url={https://arxiv.org/abs/2404.02029} }

