<h1
id="curie-scientific-long-context-understanding-reasoning-and-information-extraction">CURIE
(Scientific Long-Context Understanding, Reasoning and Information
Extraction)</h1>
<p><strong>Edit:</strong> <a
href="https://github.com/mlcommons-science/benchmark/tree/main/source">edit
this entry</a></p>
<p><strong>Date</strong>: 2024-04-02</p>
<p><strong>Name</strong>: CURIE Scientific Long-Context Understanding,
Reasoning and Information Extraction</p>
<p><strong>Domain</strong>: Multidomain Science</p>
<p><strong>Focus</strong>: Long-context scientific reasoning</p>
<p><strong>Keywords</strong>: long-context, information extraction,
multimodal</p>
<p><strong>Task Types</strong>: Information extraction, Reasoning,
Concept tracking, Aggregation, Algebraic manipulation, Multimodal
comprehension</p>
<p><strong>Metrics</strong>: Accuracy</p>
<p><strong>Models</strong>: unkown</p>
<p><strong>Citation</strong>:</p>
<ul>
<li><p>Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li,
Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka,
Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan,
Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan
Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise
Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing
VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael P Brenner,
Viren Jain, Sameera Ponda, and Subhashini Venugopalan. Curie: evaluating
llms on multitask scientific long context understanding and reasoning.
2025. URL: https://arxiv.org/abs/2503.13517, arXiv:2503.13517.</p>
<ul>
<li><p>bibtex: ``` <span class="citation"
data-cites="misc">@misc</span>{cui2025curieevaluatingllmsmultitask,</p>
<pre><code>title={CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning}, 

author={Hao Cui and Zahra Shamsi and Gowoon Cheon and Xuejian Ma and Shutong Li and Maria Tikhanovskaya and Peter Norgaard and Nayantara Mudur and Martyna Plomecka and Paul Raccuglia and Yasaman Bahri and Victor V. Albert and Pranesh Srinivasan and Haining Pan and Philippe Faist and Brian Rohr and Ekin Dogus Cubuk and Muratahan Aykol and Amil Merchant and Michael J. Statt and Dan Morris and Drew Purves and Elise Kleeman and Ruth Alcantara and Matthew Abraham and Muqthar Mohammad and Ean Phing VanLee and Chenfei Jiang and Elizabeth Dorfman and Eun-Ah Kim and Michael P Brenner and Viren Jain and Sameera Ponda and Subhashini Venugopalan},

year={2025},

eprint={2503.13517},

archivePrefix={arXiv},

primaryClass={cs.CL},

url={https://arxiv.org/abs/2503.13517}, </code></pre>
<p>}</p>
<p>```</p></li>
</ul></li>
</ul>
<p><strong>Ratings:</strong></p>
<p>Software:</p>
<ul>
<li><p><strong>Rating:</strong> 4</p></li>
<li><p><strong>Reason:</strong> Code is available, but not well
documented</p></li>
</ul>
<p>Specification:</p>
<ul>
<li><p><strong>Rating:</strong> 1</p></li>
<li><p><strong>Reason:</strong> Explains types of problems in detail,
but does not state exactly how to administer them.</p></li>
</ul>
<p>Dataset:</p>
<ul>
<li><p><strong>Rating:</strong> 4</p></li>
<li><p><strong>Reason:</strong> Dataset is available via Github, but
hard to find</p></li>
</ul>
<p>Metrics:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Quantitiative metrics such as ROUGE-L
and F1 used. Metrics are tailored to the specific problem.</p></li>
</ul>
<p>Reference Solution:</p>
<ul>
<li><p><strong>Rating:</strong> 1</p></li>
<li><p><strong>Reason:</strong> Exists, but is not open</p></li>
</ul>
<p>Documentation:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Associated paper explains all
criteria</p></li>
</ul>
<p><strong>Average Rating:</strong> 3.333</p>
<p><strong>Radar Plot:</strong> <img
src="../../tex/images/curie_scientific_long-context_understanding_reasoning_and_information_extraction_radar.png"
alt="Curie Scientific Long-Context Understanding Reasoning And Information Extraction radar plot" /></p>
