# GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark


**Edit:** [edit this entry](https://github.com/mlcommons-science/benchmark/tree/main/source)


**Date**: 2023-11-20


**Name**: GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark


**Domain**: Science  Biology, Physics, Chemistry 


**Focus**: Graduate-level, expert-validated multiple-choice questions hard even with web access


**Keywords**: Google-proof, multiple-choice, expert reasoning, science QA


**Task Types**: Multiple choice


**Metrics**: Accuracy


**Models**: GPT-4 baseline


**Citation**:


- David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: a graduate-level google-proof q and a benchmark. 2023. URL: https://arxiv.org/abs/2311.12022, arXiv:2311.12022.

  - bibtex:
      ```
      @misc{rein2023gpqagraduatelevelgoogleproofqa2,

        archiveprefix = {arXiv},

        author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},

        eprint        = {2311.12022},

        primaryclass  = {cs.AI},

        title         = {GPQA: A Graduate-Level Google-Proof Q and A Benchmark},

        url           = {https://arxiv.org/abs/2311.12022},

        year          = {2023}

      }

      ```

**Ratings:**


Software:


  - **Rating:** 3


  - **Reason:** Dataset and benchmark materials are publicly available via HuggingFace and GitHub, but no integrated runnable code or software framework is provided. 


Specification:


  - **Rating:** 5


  - **Reason:** Task is clearly defined as a multiple-choice benchmark requiring expert-level scientific reasoning. Input/output formats and evaluation criteria are well described. 


Dataset:


  - **Rating:** 5


  - **Reason:** The GPQA dataset is publicly released, well curated, with metadata and clearly documented splits. 


Metrics:


  - **Rating:** 5


  - **Reason:** Accuracy is the primary metric and is clearly defined and appropriate for multiple-choice QA. 


Reference Solution:


  - **Rating:** 1


  - **Reason:** No baseline implementations or starter code are linked or provided for reproduction. 


Documentation:


  - **Rating:** 3


  - **Reason:** Documentation includes dataset description and benchmark instructions, but lacks detailed usage tutorials or pipelines. 


**Average Rating:** 3.667


**Radar Plot:**
 ![Gpqa A Graduate-Level Google-Proof Question And Answer Benchmark radar plot](../../tex/images/gpqa_a_graduate-level_google-proof_question_and_answer_benchmark_radar.png)