- date: "2020-09-07"
  expired: null
  valid: 'yes'
  name: MMLU (Massive Multitask Language Understanding)
  url: https://paperswithcode.com/dataset/mmlu
  domain: Multidomain
  focus: Academic knowledge and reasoning across 57 subjects
  keyword:
    - multitask
    - multiple-choice
    - zero-shot
    - few-shot
    - knowledge probing
  description: |
    Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 
    multiple-choice tasks covering elementary mathematics, US history, computer science, 
    law, and more, designed to evaluate a model's breadth and depth of knowledge in 
    zero-shot and few-shot settings.
  task_types:
    - Multiple choice
  ai_capability_measured: General reasoning, subject-matter understanding
  metrics:
    - Accuracy
  models:
    - GPT-4o
    - Gemini 1.5 Pro
    - o1
    - DeepSeek-R1
  notes: Good
  cite:
    - |
      @article{hendrycks2021measuring,
        title={Measuring Massive Multitask Language Understanding},
        author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and others},
        journal={arXiv preprint arXiv:2009.03300},
        year={2021},
        url={https://arxiv.org/abs/2009.03300}
      }

- date: "2023-11-20"
  expired: null
  valid: 'yes'
  name: GPQA Diamond
  url: https://arxiv.org/abs/2311.12022
  domain: Science
  focus: Graduate-level scientific reasoning
  keyword:
    - Google-proof
    - graduate-level
    - science QA
    - chemistry
    - physics
  description: |
    GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,
    and chemistry, written by domain experts. It is “Google-proof”—experts score 65% 
    (74% after error correction) while skilled non-experts with web access score only 34%. 
    State-of-the-art LLMs like GPT-4 reach around 39% accuracy.
  task_types:
    - Multiple choice
    - Multi-step QA
  ai_capability_measured: Scientific reasoning, deep knowledge
  metrics:
    - Accuracy
  models:
    - o1
    - DeepSeek-R1
  notes: Good
  cite:
    - |
      @misc{rein2023gpqagraduatelevelgoogleproofqa,
        title={GPQA: A Graduate-Level Google-Proof Q and A Benchmark},
        author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and others},
        year={2023},
        url={https://arxiv.org/abs/2311.12022}
      }

- date: "2018-03-14"
  expired: null
  valid: 'yes'
  name: ARC-Challenge (Advanced Reasoning Challenge)
  url: https://allenai.org/data/arc
  domain: Science
  focus: Grade-school science with reasoning emphasis
  keyword:
    - grade-school
    - science QA
    - challenge set
    - reasoning
  description: |
    The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school
    science questions that retrieval-based and word co-occurrence algorithms both fail, 
    requiring advanced reasoning over a 14-million-sentence corpus.
  task_types:
    - Multiple choice
  ai_capability_measured: Commonsense and scientific reasoning
  metrics:
    - Accuracy
  models:
    - GPT-4
    - Claude
  notes: Good
  cite:
    - |
      @inproceedings{clark2018think,
        title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},
        author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and others},
        booktitle={EMNLP 2018},
        pages={237–248},
        year={2018},
        url={https://allenai.org/data/arc}
      }

- date: "2025-01-24"
  expired: null
  valid: 'yes'
  name: Humanity's Last Exam
  url: https://arxiv.org/abs/2501.14249
  domain: Multidomain
  focus: Broad cross-domain academic reasoning
  keyword:
    - cross-domain
    - academic exam
    - multiple-choice
    - multidisciplinary
  description: |
    Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000
    questions across diverse academic disciplines, designed to evaluate LLMs' ability to
    reason across domains without external resources.
  task_types:
    - Multiple choice
  ai_capability_measured: Cross-domain academic reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - |
      @misc{phan2025humanitys,
        title={Humanity's Last Exam},
        author={Phan, Long and Gatti, Alice and Han, Ziwen and others},
        year={2025},
        url={https://arxiv.org/abs/2501.14249}
      }

- date: "2024-11-07"
  expired: null
  valid: 'yes'
  name: FrontierMath
  url: https://arxiv.org/abs/2411.04872
  domain: Mathematics
  focus: Challenging advanced mathematical reasoning
  keyword:
    - symbolic reasoning
    - number theory
    - algebraic geometry
    - category theory
  description: |
    FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning
    number theory, real analysis, algebraic geometry, and category theory, measuring LLMs’ 
    ability to solve problems requiring deep abstract reasoning.
  task_types:
    - Problem solving
  ai_capability_measured: Symbolic and abstract mathematical reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - |
      @misc{glazer2024frontiermath,
        title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},
        author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and others},
        year={2024},
        url={https://arxiv.org/abs/2411.04872}
      }

- date: "2024-07-18"
  expired: null
  valid: 'yes'
  name: SciCode
  url: https://arxiv.org/abs/2407.13168
  domain: Scientific Programming
  focus: Scientific code generation and problem solving
  keyword:
    - code synthesis
    - scientific computing
    - programming benchmark
  description: |
    SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80
    real research tasks across 16 scientific subfields, evaluating models on knowledge recall, 
    reasoning, and code synthesis for scientific computing tasks.
  task_types:
    - Coding
  ai_capability_measured: Program synthesis, scientific computing
  metrics:
    - Solve rate (%)
  models:
    - Claude3.5-Sonnet
  notes: Good
  cite:
    - |
      @misc{tian2024scicode,
       title={SciCode: A Research Coding Benchmark Curated by Scientists},
       author={Tian, Minyang and Gao, Luyu and Zhang, Shizhuo and others},
       year={2024},
       url={https://arxiv.org/abs/2407.13168}
      }

- date: "2025-03-13"
  expired: null
  valid: 'yes'
  name: AIME (American Invitational Mathematics Examination)
  url: https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions
  domain: Mathematics
  focus: Pre-college advanced problem solving
  keyword:
    - algebra
    - combinatorics
    - number theory
    - geometry
  description: |
    The AIME is a 15-question, 3-hour exam for high-school students featuring challenging
    short-answer math problems in algebra, number theory, geometry, and combinatorics, 
    assessing depth of problem-solving ability.
  task_types:
    - Problem solving
  ai_capability_measured: Mathematical problem-solving and reasoning
  metrics:
    - Accuracy
  models: []
  notes:
  cite:
    - |
      @misc{www-aime,
        author = {TBD},
        title = {AIME},
        url = {https://www.vals.ai/benchmarks/aime-2025-03-13},
        month = mar,
        year = 2025,
        note = {[Online accessed 2025-06-24]}
      }

- date: "2025-02-15"
  expired: null
  valid: 'yes'
  name: MATH-500
  url: https://huggingface.co/datasets/HuggingFaceH4/MATH-500
  domain: Mathematics
  focus: Math reasoning generalization
  keyword:
    - calculus
    - algebra
    - number theory
    - geometry
  description: |
    MATH-500 is a curated subset of 500 problems from the OpenAI MATH dataset, spanning
    high-school to advanced levels, designed to evaluate LLMs’ mathematical reasoning and 
    generalization.
  task_types:
    - Problem solving
  ai_capability_measured: Math reasoning and generalization
  metrics:
    - Accuracy
  models: []
  notes: Dataset hosted on Hugging Face
  cite:
    - |
      @misc{huggingface2025math500,
        title={MATH-500},
        author={HuggingFaceH4},
        year={2025},
        url={https://huggingface.co/datasets/HuggingFaceH4/MATH-500}
      }

- date: "2024-04-02"
  expired: null
  valid: 'yes'
  name: CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)
  url: https://arxiv.org/abs/2404.02029
  domain: Multidomain Science
  focus: Long-context scientific reasoning
  keyword:
    - long-context
    - information extraction
    - multimodal
  description: |
    CURIE is a benchmark of 580 problems across six scientific disciplines—materials
    science, quantum computing, biology, chemistry, climate science, and astrophysics—
    designed to evaluate LLMs on long-context understanding, reasoning, and information 
    extraction in realistic scientific workflows.
  task_types:
    - Information extraction
    - Reasoning
    - Concept tracking
    - Aggregation
    - Algebraic manipulation
    - Multimodal comprehension
  ai_capability_measured: Long-context understanding and scientific reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - |
      @misc{curie2024,
        title={Scientific Reasoning Benchmarks from the CURIE Dataset},
        author={TODO: Add authors},
        year={2024},
        url={https://arxiv.org/abs/2404.02029}
      }

- date: "2023-01-26"
  expired: null
  valid: 'yes'
  name: FEABench (Finite Element Analysis Benchmark)
  url: https://github.com/alleninstitute/feabench
  domain: Computational Engineering
  focus: FEA simulation accuracy and performance
  keyword:
    - finite element
    - simulation
    - PDE
  description: |
    FEABench is a suite evaluating finite element analysis tools on standardized 
    PDE-based simulation tasks with complex geometries and boundary conditions, 
    measuring both accuracy and runtime performance.
  task_types:
    - Simulation
    - Performance evaluation
  ai_capability_measured: Numerical simulation accuracy and efficiency
  metrics:
    - Solve time
    - Error norm
  models:
    - FEniCS
    - deal.II
  notes: Good
  cite:
    - |
      @misc{allen2023feabench,
        title={FEABench: A Finite Element Analysis Benchmark},
        author={Allen Institute},
        year={2023},
        url={https://github.com/alleninstitute/feabench}
      }

- date: "2024-07-12"
  expired: null
  valid: 'yes'
  name: SPIQA (Scientific Paper Image Question Answering)
  url: https://arxiv.org/abs/2407.09413
  domain: Computer Science
  focus: Multimodal QA on scientific figures
  keyword:
    - multimodal QA
    - figure understanding
    - table comprehension
    - chain-of-thought
  description: |
    SPIQA assesses AI models' ability to interpret and answer questions about figures
    and tables in scientific papers by integrating visual and textual modalities 
    with chain-of-thought reasoning.
  task_types:
    - Question answering
    - Multimodal QA
    - Chain-of-Thought evaluation
  ai_capability_measured: Visual-textual reasoning in scientific contexts
  metrics:
    - Accuracy
    - F1 score
  models:
    - Chain-of-Thought models
    - Multimodal QA systems
  notes: Good
  cite:
    - |
      @article{zhong2024spiqa,
        title={SPIQA: Scientific Paper Image Question Answering},
        author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin},
        year={2024},
        url={https://arxiv.org/abs/2407.09413}
      }

- date: "2020-09-28"
  expired: null
  valid: 'yes'
  name: MedQA
  url: https://arxiv.org/abs/2009.13081
  domain: Medical Question Answering
  focus: Medical board exam QA
  keyword:
    - USMLE
    - diagnostic QA
    - medical knowledge
    - multilingual
  description: |
    MedQA is a large-scale multiple-choice dataset drawn from professional medical
    board exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge 
    questions in English and Chinese.
  task_types:
    - Multiple choice
  ai_capability_measured: Medical diagnosis and knowledge retrieval
  metrics:
    - Accuracy
  models:
    - Neural reader
    - Retrieval-based QA systems
  notes: Multilingual (English, Simplified and Traditional Chinese)
  cite:
    - |
      @article{jin2020what,
        title={What Disease Does This Patient Have? A Large-scale Open-domain Question Answering Dataset from Medical Exams},
        author={Jin, Di and Li, Ying and Zhang, Yichong and others},
        year={2020},
        url={https://arxiv.org/abs/2009.13081}
      }

- date: "2025-05-13"
  expired: null
  valid: 'yes'
  name: BaisBench (Biological AI Scientist Benchmark)
  url: https://arxiv.org/abs/2505.08341
  domain: Computational Biology
  focus: Omics-driven AI research tasks
  keyword:
    - single-cell annotation
    - biological QA
    - autonomous discovery
  description: |
    BaisBench evaluates AI scientists' ability to perform data-driven biological research
    by annotating cell types in single-cell datasets and answering MCQs derived from 
    biological study insights, measuring autonomous scientific discovery.
  task_types:
    - Cell type annotation
    - Multiple choice
  ai_capability_measured: Autonomous biological research capabilities
  metrics:
    - Annotation accuracy
    - QA accuracy
  models:
    - LLM-based AI scientist agents
  notes: Underperforms human experts; aims to advance AI-driven discovery
  cite:
    - |
      @misc{luo2025benchmarkingaiscientistsomics,
        title={Benchmarking AI scientists in omics data-driven biological research},
        author={Luo, Erpai and Jia, Jinmeng and Xiong, Yifan and others},
        year={2025},
        url={https://arxiv.org/abs/2505.08341}
      }

- date: "2023-01-26"
  expired: null
  valid: 'yes'
  name: MOLGEN
  url: https://github.com/zjunlp/MolGen
  domain: Computational Chemistry
  focus: Molecular generation and optimization
  keyword:
    - SELFIES
    - GAN
    - property optimization
  description: |
    MolGen is a pre-trained molecular language model that generates chemically valid
    molecules using SELFIES and reinforcement learning, guided by chemical feedback 
    to optimize properties such as logP, QED, and docking score.
  task_types:
    - Distribution learning
    - Goal-oriented generation
  ai_capability_measured: Generation of valid and optimized molecular structures
  metrics:
    - Validity%
    - Novelty%
    - QED
    - Docking score
  models:
    - MolGen
  notes: This is a model, not a benchmark
  cite:
    - |
      @article{fang2023domain,
        title={Domain-Agnostic Molecular Generation with Chemical Feedback},
        author={Fang, Yin and Zhang, Ningyu and Chen, Zhuo and others},
        year={2023},
        url={https://arxiv.org/abs/2301.11259}
      }

- date: "2020-05-02"
  expired: null
  valid: 'yes'
  name: Open Graph Benchmark (OGB) - Biology
  url: https://ogb.stanford.edu/docs/home/
  domain: Graph ML
  focus: Biological graph property prediction
  keyword:
    - node prediction
    - link prediction
    - graph classification
  description: |
    OGB-Biology is a suite of large-scale biological network datasets (protein-protein
    interaction, drug-target, etc.) with standardized splits and evaluation protocols 
    for node, link, and graph property prediction tasks.
  task_types:
    - Node property prediction
    - Link property prediction
    - Graph property prediction
  ai_capability_measured: Scalability and generalization in graph ML for biology
  metrics:
    - Accuracy
    - ROC-AUC
  models:
    - GCN
    - GraphSAGE
    - GAT
  notes: Community-driven updates
  cite:
    - |
      @misc{hu2020ogb,
        title={Open Graph Benchmark: Datasets for Machine Learning on Graphs},
        author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and others},
        year={2020},
        url={https://arxiv.org/abs/2005.00687}
      }

- date: "2011-10-01"
  expired: null
  valid: 'yes'
  name: Materials Project
  url: https://materialsproject.org/
  domain: Materials Science
  focus: DFT-based property prediction
  keyword:
    - DFT
    - materials genome
    - high-throughput
  description: |
    The Materials Project provides an open-access database of computed properties for
    inorganic materials via high-throughput density functional theory (DFT), accelerating 
    materials discovery.
  task_types:
    - Property prediction
  ai_capability_measured: Prediction of inorganic material properties
  metrics:
    - MAE
    - R²
  models:
    - Automatminer
    - Crystal Graph Neural Networks
  notes: Core component of the Materials Genome Initiative
  cite:
    - |
      @article{jain2013materials,
        title={The Materials Project: A materials genome approach},
        author={Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin A.},
        journal={APL Materials},
        volume    = {1},
        number    = {1},
        year={2013},
        doi       = {10.1063/1.4812323},
        url={https://materialsproject.org/}
      }

- date: "2020-10-20"
  expired: null
  valid: 'yes'
  name: OCP (Open Catalyst Project)
  url: https://opencatalystproject.org/
  domain: Chemistry; Materials Science
  focus: Catalyst adsorption energy prediction
  keyword:
    - DFT relaxations
    - adsorption energy
    - graph neural networks
  description: |
    The Open Catalyst Project (OC20 and OC22) provides DFT-calculated catalyst-adsorbate 
    relaxation datasets, challenging ML models to predict energies and forces for 
    renewable energy applications.
  task_types:
    - Energy prediction
    - Force prediction
  ai_capability_measured: Prediction of adsorption energies and forces
  metrics:
    - MAE (energy)
    - MAE (force)
  models:
    - CGCNN
    - SchNet
    - DimeNet++
    - GemNet-OC
  notes: Public leaderboards; active community development
  cite:
    - |
      @article{chanussot2021oc20,
        title     = {The Open Catalyst 2020 (OC20) Dataset and Community Challenges},
        author    = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
        journal   = {ACS Catalysis},
        volume    = {11},
        number    = {10},
        pages     = {6059--6072},
        year      = {2021},
        doi       = {10.1021/acscatal.0c04525},
        url       = {https://pubs.acs.org/doi/10.1021/acscatal.0c04525}
      }
    - |
      @article{tran2023oc22,
        title     = {The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
        author    = {Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Félix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
        journal   = {ACS Catalysis},
        volume    = {13},
        number    = {5},
        pages     = {3066--3084},
        year      = {2023},
        doi       = {10.1021/acscatal.2c05426},
        url       = {https://pubs.acs.org/doi/10.1021/acscatal.2c05426}
      }
    - |
      @article{doi:10.1021/acscatal.0c04525,
        author = {Chanussot, Lowik and Das, Abhishek and Goyal, Siddharth and Lavril, Thibaut and Shuaibi, Muhammed and Riviere, Morgane and Tran, Kevin and Heras-Domingo, Javier and Ho, Caleb and Hu, Weihua and Palizhati, Aini and Sriram, Anuroop and Wood, Brandon and Yoon, Junwoong and Parikh, Devi and Zitnick, C. Lawrence and Ulissi, Zachary},
      title = {Open Catalyst 2020 (OC20) Dataset and Community Challenges},
        journal = {ACS Catalysis},
        volume = {11},
        number = {10},
        pages = {6059-6072},
        year = {2021},
        doi = {10.1021/acscatal.0c04525},
        URL = {https://doi.org/10.1021/acscatal.0c04525},eprint = {https://doi.org/10.1021/acscatal.0c04525}}"
    - |
      @article{tran2023b,
        title={The Open Catalyst 2022 (OC22) Dataset and Challenges for Oxide Electrocatalysts},
        volume={13},
        ISSN={2155-5435},
        url={http://dx.doi.org/10.1021/acscatal.2c05426},
        DOI={10.1021/acscatal.2c05426},
        number={5},
        journal={ACS Catalysis},
        publisher={American Chemical Society (ACS)},
        author={Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M. and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and Sriram, Anuroop and Therrien, Félix and Abed, Jehad and Voznyy, Oleksandr and Sargent, Edward H. and Ulissi, Zachary and Zitnick, C. Lawrence},
        year={2023},
        month=feb, pages={3066-3084} 
      }

- date: "2023-06-20"
  expired: null
  valid: 'yes'
  name: JARVIS-Leaderboard
  url: https://arxiv.org/abs/2306.11688
  domain: Materials Science; Benchmarking
  focus: Comparative evaluation of materials design methods
  keyword:
    - leaderboards
    - materials methods
    - simulation
  description: |
    JARVIS-Leaderboard is a community-driven platform benchmarking AI, electronic
    structure, force-fields, quantum computing, and experimental methods across hundreds
    of materials science tasks.
  task_types:
    - Method benchmarking
    - Leaderboard ranking
  ai_capability_measured: Performance comparison across diverse materials design methods
  metrics:
    - MAE
    - RMSE
    - Accuracy
  models: []
  notes: 1,281 contributions across 274 benchmarks
  cite:
    - |
      @article{choudhary2024jarvis,
        title = {{JARVIS-Leaderboard}: a large scale benchmark of materials design methods},
      author = {Choudhary, Kamal and Wines, Daniel and Li, Kangming and Garrity, Kevin F. and Gupta, Vishu and Romero, Aldo H. and Krogel, Jaron T. and Saritas, Kayahan and Fuhr, Addis and Ganesh, Panchapakesan and Kent, Paul R. C. and Yan, Keqiang and Lin, Yuchao and Ji, Shuiwang and Blaiszik, Ben and Reiser, Patrick and Friederich, Pascal and Agrawal, Ankit and Tiwary, Pratyush and Beyerle, Eric and Minch, Peter and Rhone, Trevor D. and Takeuchi, Ichiro and Wexler, Robert B. and Mannodi-Kanakkithodi, Arun and Ertekin, Elif and Mishra, Avanish and Mathew, Nithin and Wood, Mitchell and Rohskopf, Andrew D. and Hattrick-Simpers, Jason and Wang, Shih-Han and Achenie, Luke E. K. and Xin, Hongliang and Williams, Maureen and Biacchi, Adam J. and Tavazza, Francesca},
        journal = {npj Computational Materials},
        volume = {10},
        number = {1},
        pages = {93},
        year = {2024},
        doi = {10.1038/s41524-024-01259-w},
        url = {https://doi.org/10.1038/s41524-024-01259-w}
      }

- date: "2022-02-22"
  expired: null
  valid: 'yes'
  name: Quantum Computing Benchmarks (QML)
  url:
    - https://github.com/XanaduAI/qml-benchmarks
    - https://pennylane.ai/datasets/collection/qml-benchmarks
  domain: Quantum Computing
  focus: Quantum algorithm performance evaluation
  keyword:
    - quantum circuits
    - state preparation
    - error correction
  description: |
    A suite of benchmarks evaluating quantum hardware and algorithms on tasks such as state 
    preparation, circuit optimization, and error correction across multiple platforms.
  task_types:
    - Circuit benchmarking
    - State classification
  ai_capability_measured: Quantum algorithm performance and fidelity
  metrics:
    - Fidelity
    - Success probability
  models:
    - IBM Q
    - IonQ
    - AQT@LBNL
  notes: Hardware-agnostic, application-level metrics. The citation may not be correct.
  cite:
    - |
      @inproceedings{kiwit2023,
        title={Application-Oriented Benchmarking of Quantum Generative Learning Using QUARK},
        url={http://dx.doi.org/10.1109/QCE57702.2023.00061},
        DOI={10.1109/qce57702.2023.00061},
        booktitle={2023 IEEE International Conference on Quantum Computing and Engineering (QCE)},
        publisher={IEEE},
        author={Kiwit, Florian J. and Marso, Marwa and Ross, Philipp and Riofrío, Carlos A. and Klepsch, Johannes and Luckow, Andre},
        year={2023},
        month=sep, pages={475-484}
      }

- date: "2024-10-01"
  expired: null
  valid: 'yes'
  name: CFDBench (Fluid Dynamics)
  url: https://arxiv.org/abs/2310.05963
  domain: Fluid Dynamics; Scientific ML
  focus: Neural operator surrogate modeling
  keyword:
    - neural operators
    - CFD
    - FNO
    - DeepONet
  description: |
    CFDBench provides large-scale CFD data for four canonical fluid flow problems, 
    assessing neural operators' ability to generalize to unseen PDE parameters and domains.
  task_types:
    - Surrogate modeling
  ai_capability_measured: Generalization of neural operators for PDEs
  metrics:
    - L2 error
    - MAE
  models:
    - FNO
    - DeepONet
    - U-Net
  notes: 302K frames across 739 cases
  cite:
    - |
      @misc{luo2024cfdbenchlargescalebenchmarkmachine,
        title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics},
        author={Luo, Yining and Chen, Yingfa and Zhang, Zhen},
        year={2024},
        url={https://arxiv.org/abs/2310.05963}
      }

- date: null
  expired: null
  valid: 'yes'
  name: SatImgNet
  url: null
  domain: Remote Sensing
  focus: Satellite imagery classification
  keyword:
    - land-use
    - zero-shot
    - multi-task
  description: |
    SATIN (sometimes referred to as SatImgNet) is a multi-task metadataset of 27 satellite
    imagery classification datasets evaluating zero-shot transfer of vision-language models
    across diverse remote sensing tasks.
  task_types:
    - Image classification
  ai_capability_measured: Zero-shot land-use classification
  metrics:
    - Accuracy
  models: []
  notes: Public leaderboard available
  cite:
    - |
      @misc{roberts2023satinmultitaskmetadatasetclassifying,
        title={SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models}, 
        author={Jonathan Roberts and Kai Han and Samuel Albanie},
        year={2023},
        eprint={2304.11619},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2304.11619}, 
      }


- date: "2023-07-19"
  expired: null
  valid: 'yes'
  name: ClimateLearn
  url: https://arxiv.org/abs/2307.01909
  domain: Climate Science; Forecasting
  focus: ML for weather and climate modeling
  keyword:
    - medium-range forecasting
    - ERA5
    - data-driven
  description: |
    ClimateLearn provides standardized datasets and evaluation protocols for machine 
    learning models in medium-range weather and climate forecasting using ERA5 reanalysis.
  task_types:
    - Forecasting
  ai_capability_measured: Global weather prediction (3-5 days)
  metrics:
    - RMSE
    - Anomaly correlation
  models:
    - CNN baselines
    - ResNet variants
  notes: Includes physical and ML baselines. Appears to be the same as the SatImgNet entry
  cite:
    - |
      @misc{nguyen2023climatelearnbenchmarkingmachinelearning, 
        title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 
        author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},
        year={2023}, eprint={2307.01909}, 
        archivePrefix={arXiv}, 
        primaryClass={cs.LG},
        url={https://arxiv.org/abs/2307.01909}
      }

- date: "2022-06-09"
  expired: null
  valid: 'yes'
  name: BIG-Bench (Beyond the Imitation Game Benchmark)
  url: https://github.com/google/BIG-bench
  domain: NLP; AI Evaluation
  focus: Diverse reasoning and generalization tasks
  keyword:
    - few-shot
    - multi-task
    - bias analysis
  description: |
    BIG-Bench is a collaborative suite of 204 tasks designed to probe LLMs' reasoning, 
    knowledge, and bias across diverse domains and difficulty levels beyond simple imitation.
  task_types:
    - Few-shot evaluation
    - Multi-task evaluation
  ai_capability_measured: Reasoning and generalization across diverse tasks
  metrics:
    - Accuracy
    - Task-specific metrics
  models:
    - GPT-3
    - Dense Transformers
    - Sparse Transformers
  notes: Human baselines included
  cite:
    - |
      @misc{srivastava2023imitationgamequantifyingextrapolating,
        title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
        author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
        year={2023},
        eprint={2206.04615},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/2206.04615}, 
      }

- date: "2019-11-20"
  expired: null
  valid: 'yes'
  name: CommonSenseQA
  url: https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge
  domain: NLP; Commonsense
  focus: Commonsense question answering
  keyword:
    - ConceptNet
    - multiple-choice
    - adversarial
  description: |
    CommonsenseQA is a challenging multiple-choice QA dataset built from ConceptNet,
    requiring models to apply commonsense knowledge to select the correct answer 
    among five choices.
  task_types:
    - Multiple choice
  ai_capability_measured: Commonsense reasoning and knowledge integration
  metrics:
    - Accuracy
  models:
    - BERT-large
    - RoBERTa
    - GPT-3
  notes: Baseline 56%, human 89%
  cite:
    - |
      @misc{talmor2019commonsenseqaquestionansweringchallenge,
        title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
        author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
        year={2019},
        eprint={1811.00937},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/1811.00937}, 
      }

- date: "2019-07-24" 
  expired: null
  valid: 'yes'
  name: Winogrande
  url: https://leaderboard.allenai.org/winogrande/submissions/public
  domain: NLP; Commonsense
  focus: Winograd Schema-style pronoun resolution
  keyword:
    - adversarial
    - pronoun resolution
  description: |
    WinoGrande is a large-scale adversarial dataset of 44,000 Winograd Schema-style 
    questions with reduced bias using AFLite, serving as both a benchmark and transfer 
    learning resource.
  task_types:
    - Pronoun resolution
  ai_capability_measured: Robust commonsense reasoning
  metrics:
    - Accuracy
    - AUC
  models:
    - RoBERTa
    - BERT
    - GPT-2
  notes: Human ~94%
  cite:
    - |
      @misc{sakaguchi2019winograndeadversarialwinogradschema,
        title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
        author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
        year={2019},
        eprint={1907.10641},
        archivePrefix={arXiv},
        primaryClass={cs.CL},
        url={https://arxiv.org/abs/1907.10641}, 
      }
