# GPQA: A Graduate-Level Google-Proof Q&A Benchmark

**Date**: 2023-11-20

**Expiration**: 

**Valid**: yes

**Name**: GPQA: A Graduate-Level Google-Proof Q&A Benchmark

**URL**: https://arxiv.org/abs/2311.12022

**Domain**: Science  Biology, Physics, Chemistry 

**Focus**: Graduate-level, expert-validated multiple-choice questions hard even with web access

**Keywords**: Google-proof, multiple-choice, expert reasoning, science QA

**Description**: Contains 448 challenging questions written by domain experts, with expert accuracy at 65%  74% discounting clear errors  and non-experts reaching just 34%. GPT‑4 baseline scores ~39%—designed for scalable oversight evaluation. :contentReference oaicite:2 {index=2}   

**Task Types**: Multiple choice

**AI Capability**: Scientific reasoning, knowledge probing

**Metrics**: Accuracy

**Models**: GPT‑4 baseline

**Notes**: “Google-proof”; supports oversight research.

**Citation**:

-
  - type: article
  - id: rein2023gpqa
  - url: https://arxiv.org/abs/2311.12022
  - year: 2023
  - author: Rein, David, Hou, Betty Li, Stickland, Asa Cooper, others
  - title: GPQA: A Graduate-Level Google-Proof Q\&A Benchmark
  - bibtex: |
      @article{rein2023gpqa,
        title={GPQA: A Graduate-Level Google-Proof Q\&A Benchmark},
        author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and others},
        year={2023},
        url={https://arxiv.org/abs/2311.12022}
      }

