#
# THIS ENTRY IS FICTIONAL
#

- date: '2024-05-01'
    decription: |
      The date of availability of the benchmark. If an official realease date is 
      not available data of adding the entry.
    condition: required
  
  version: v1.0
    description: |
      The version number of the benchmark 
    condition: optional

  last_updated: '2024-05-01' #could also be null
    description: |
      x 
    condition: optional
  
  expired: null
    description: |
      x 
    condition: optional
    
  valid: 'yes' #CHECK DEFINITION
    date: 2025-01-01
    description: |
      Identifies if the benchmark is valid at the time of review. Invalidity may be caused by the code or data 
      no longer available
    condition: required
    
  name: Jet Classification
    description: |
      The name of the benchmark
    condition: required
  
  url: https://github.com/fastmachinelearning/fastml-science/tree/main/jet-classify
    description: |
      The main URL for this benchmark. We also recommend to add it to the citation. 
    condition: required

  doi: xyz...
    description: |
      A doi number that may be associated with the benchmark. THis could bo one obtained by Zendoo. 
      We also recommend to add this to the citation.
    condition: optional
  
  domain: Particle Physics
    description: |
      The scientific domain this benchmark belongs to
    condition: required
  
  focus: Real-time classification of particle jets using HL-LHC simulation features #Perhaps put under "focus"
    description: |
      This field is unclear
    condition: required
  
  keywords:
  - classification
  - real-time ML
  - jet tagging
  - QKeras
    description: |
      x 
    condition: ">=1"
  
  description: |
    This benchmark evaluates ML models for real-time classification of
    particle jets using high-level features derived from simulated LHC data. It
    includes both full-precision and quantized models optimized for FPGA deployment.
    description: |
      An easy to understand description of the benchmark so that members of the 
      community know what the benchmark is about.
    condition: required

  licensing: Apache 2.0
    decription: The license for thebenchmark
    condition: required
  
  task_types:
  - Classification
    description: |
      The task type the is defined by this benchmark. Gregor finds that this requires a bit more explanation as
      unclear for most.
    condition: required
  
  ai_capability_measured: #Must be a YAML list
  - Real-time inference 
  - Model compression performance
    description: |
      The AI capability that is measured. Gregor finds this requires definition of what an AI capability is.
    condition: ">=1"
  
  metrics:
  - Accuracy
  - AUC
    description: |
      A list of metrics used by the benchmark 
    condition: ">=1"
  
  models:
  - Keras DNN
  - QKeras quantized DNN
    description: |
      A list of models that are used by the benchmark.
    condition: ">=1"
  
  ml_motif: #What type of ML is tested
  - Real-time
    description: |
      x 
    condition: ">=1"
  
  type: Benchmark 
    description: |
      The type of this benchmark. This could be indication that the effore represents a designed benchmark, or
      an application comparision that contains implicit benchmarks. We wil be analysing candidates to identify 
      types and make them available here.
    condition: required
  
  ml_task: # note this was a single attribute, Gregor likes to change to list
    - Supervised Learning
    description: |
      A list of machine learning task that is used in this benchmaks
    condition: ">=1"
  
  solutions: '2' 
    description: |
      obsolete as results should cature this. results is a list thus we can get the number of list elements.
      This is easier to manage.
    condition: deprecated
  
  notes: Includes both float and quantized models using QKeras
    description: |
      An added note describing the benchmark in more detail or adding some related information.
    condition: optional
  
  contact: 
    name: Jules Muhizi
    email: null
    description: |
      A contact person or organization for this benchmark
    condition: optional
  
  cite: 
    - |
      @article{hawks2022fastml,
        title={Fast Machine Learning for Science: Benchmarks and Dataset},
        author={Hawks, Ben and Tran, Nhan and others},
        year={2022},
        url={https://arxiv.org/abs/2207.07958}
      }
    description: |
      A list of bibtex citations. It is important that the citation is in bibtex. This may include 
      scholarly articles, web pages, githib pages. At least one must be provided.
    condition: ">=1"
  
  dataset: 
    - name: OpenML
      url: hls4ml_lhc_jets_hlf (https://www.openml.org/d/42468)
    - name: JetClass
      url: https://zenodo.org/record/6619768
    description: |
      A list of datasets used with the benchmark. 
    condition: ">=1"
  
  results:
    - name: Gemini LLM Deep Research
      url: https://docs.google.com/document/d/1Mr7J4F8PDAIBXJ2vrfVssxLekEVW7ahJ4wpSe6FN5yw
    - name: ChatGPT LLM
      url: https://docs.google.com/document/d/1runrcij-eoH3_lgGZ8wm2z1YbL1Qf5cSNbVbHyWFDs4
    description: |
      A list of results ideally published on the web. It could also point to a paper.
    condition: optional
  
  fair: 
    reproducible: true
    benchmark_ready: true # This field is depricated as true implies runnable
    description: |
      attributes important with fair princible. It is true if fair applies.
    condition: optional
  
  ratings:
    description: |
      A set of rationgs asssociated with the benchmark. The goal is to rate all benchmarks, but this could
      be ime consuming and prevent listing here. To allow addition in the list without 
      ratings we make this section optional. A value of -1 in any of the categories means 
      it has not been evaluated yet.
    condition: optional

    software: #Can the end user set up and run the benchark
      rating: 0
      reason: laksdjflkajsdlfkajsdlf
      description: |
        Evaluates maturaty and software engeniering aspects use to define this benchmark.
      condition: optional

    specification:
      rating: 9.0
      reason: |
        Task and format (multiple-choice QA with 5 options) are clearly
        defined; grounded in ConceptNet with consistent structure, though no hardware/system
        constraints are specified.
      description: |
        Evaluates the maturity of the specification and explenations for the existence of this benchmak.
      condition: optional

    dataset:
      rating: 9.0
      reason: |
        Public, versioned, and FAIR-compliant; includes metadata, splits,
        and licensing; well-integrated with HuggingFace and other ML libraries.
      description: |
        Evaluates the dataset, inlcuding attributes such as 
          public availability
          versioning
          metadata
          licensing
      condition: optional
    
    metrics:
      rating: 9.0
      reason: | 
        Accuracy is a simple, reproducible metric aligned
        with task goals; no ambiguity in evaluation.
      description: |
        The exitance and maturity of an accuracy value relevant for the domain community
      condition: optional
    
    reference_solution:
      rating: 8.0
      reason: |
        Several baseline models (e.g., BERT, RoBERTa) are reported
        with scores; implementations exist in public repos, but not bundled as an official
        starter kit.
      description: |
        The availability of a refernce solution.
      condition: optional
    
    documentation:
      rating: 7.0
      reason: |
        Clear paper, GitHub repo, and integration with HuggingFace
        Datasets; full reproducibility requires manually connecting models to dataset.
      description: |
        The maturity of the documentation including how to replicate the benchmark
      condition: optional

