# LLM-Inference-Bench

**Date**: 2024-10-31

**Expiration**: 

**Valid**: yes

**Name**: LLM-Inference-Bench

**URL**: https://github.com/argonne-lcf/LLM-Inference-Bench

**Domain**: LLM; HPC/inference

**Focus**: Hardware performance benchmarking of LLMs on AI accelerators

**Keywords**: LLM, inference benchmarking, GPU, accelerator, throughput

**Description**: A suite evaluating inference performance of LLMs  LLaMA, Mistral, Qwen  across diverse accelerators  NVIDIA, AMD, Intel, SambaNova  and frameworks  vLLM, DeepSpeed‑MII, etc. , with an interactive dashboard and per-platform metrics. :contentReference oaicite:3 {index=3} 

**Task Types**: Inference Benchmarking

**AI Capability**: Inference throughput, latency, hardware utilization

**Metrics**: Token throughput  tok/s , Latency, Framework-hardware mix performance

**Models**: LLaMA-2‑7B, LLaMA-2‑70B, Mistral‑7B, Qwen‑7B

**Notes**: Licensed under BSD‑3, maintained by Argonne; supports GPUs and accelerators. :contentReference oaicite:4 {index=4}

**Citation**:

-
  - type: article
  - id: chitty2024llm
  - year: 2024
  - journal: arXiv preprint arXiv:2411.00136
  - author: Chitty-Venkata, Krishna Teja, Raskar, Siddhisanket, others
  - title: LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators
  - bibtex: |
      @article{chitty2024llm,
        title={LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators},
        author={Chitty-Venkata, Krishna Teja and Raskar, Siddhisanket and others},
        journal={arXiv preprint arXiv:2411.00136},
        year={2024}
      }

