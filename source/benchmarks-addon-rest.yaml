

- date: "2025-01-24"
  version: "v1.0"
  last_updated: "2025-02"
  expired: "unknown"
  valid: "yes"
  valid_date: "2025-01-24"
  name: "Single Qubit Readout on QICK System"
  url: "https://github.com/fastmachinelearning/ml-quantum-readout"
  doi: "10.48550/arXiv.2501.14663"
  domain: "Quantum Computing"
  focus: "Real-time single-qubit state classification using FPGA firmware"
  keywords:
  - "qubit readout"
  - "hls4ml"
  - "FPGA"
  - "QICK"
  summary: |
    Implements real-time ML models for single-qubit readout on the Quantum Instrumentation Control Kit (QICK), using hls4ml to deploy quantized neural networks on RFSoC FPGAs. Offers high-fidelity, low-latency quantum state discrimination. :contentReference[oaicite:0]{index=0}
  licensing: "NA"
  task_types:
  - "Classification"
  ai_capability_measured:
  - "Single-shot fidelity"
  - "inference latency"
  metrics:
  - "Accuracy"
  - "Latency"
  models:
  - "hls4ml quantized NN"
  ml_motif:
  - "Real-time"
  type: "Benchmark"
  ml_task:
  - "Supervised Learning"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Achieves ~96% fidelity with ~32 ns latency and low FPGA resource utilization. 
  contact:
    name: "Javier Campos, Giuseppe Di Guglielmo"
    email: "unknown"
  cite:
  - |
    @misc{diguglielmo2025endtoendworkflowmachinelearningbased,
      archiveprefix = {arXiv},
      author        = {Giuseppe Di Guglielmo and Botao Du and Javier Campos and Alexandra Boltasseva and Akash V. Dixit and Farah Fahim and Zhaxylyk Kudyshev and Santiago Lopez and Ruichao Ma and Gabriel N. Perdue and Nhan Tran and Omer Yesilyurt and Daniel Bowring},
      eprint        = {2501.14663},
      primaryclass  = {quant-ph},
      title         = {End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml},
      url           = {https://arxiv.org/abs/2501.14663},
      year          = {2025}
    }
  datasets:
    links:
    - name: "Zenodo: ml-quantum-readout dataset"
      url: "zenodo.org/records/14427490"
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "(none)"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Code and FPGA firmware available on GitHub; integration with hls4ml demonstrated.
        Some deployment details and examples are provided but overall software maturity is moderate.
    specification:
      rating: 4
      reason: |
        Task clearly defined: real-time single-qubit state classification with latency and
        fidelity constraints. Labeling and ground truth definitions could be more explicit.
    dataset:
      rating: 4
      reason: |
        Dataset hosted on Zenodo with structured data; however, detailed documentation on
        image acquisition and labeling pipeline is limited.
    metrics:
      rating: 5
      reason: |
        Standard classification metrics (accuracy, latency) are used and directly relevant
        to the quantum readout task.
    reference_solution:
      rating: 1
      reason: |
        No baseline or starter models with runnable code are linked publicly.
    documentation:
      rating: 4
      reason: |
        Codabench task page and GitHub repo provide descriptions and usage instructions,
        but detailed API or deployment tutorials are limited.
- date: "2023-11-20"
  version: "v1.0"
  last_updated: "2023-11"
  expired: "unknown"
  valid: "yes"
  valid_date: "2023-11-20"
  name: "GPQA: A Graduate-Level Google-Proof Question and Answer Benchmark"
  url: "https://arxiv.org/abs/2311.12022"
  doi: "10.48550/arXiv.2311.12022"
  domain: "Science (Biology, Physics, Chemistry)"
  focus: "Graduate-level, expert-validated multiple-choice questions hard even with
    web access"
  keywords:
  - "Google-proof"
  - "multiple-choice"
  - "expert reasoning"
  - "science QA"
  summary: |
    Contains 448 challenging questions written by domain experts, with expert accuracy at 65% (74% discounting clear errors) and non-experts reaching just 34%. GPT-4 baseline scores ~39%-designed for scalable oversight evaluation. 
  licensing: "NA"
  task_types:
  - "Multiple choice"
  ai_capability_measured:
  - "Scientific reasoning"
  - "knowledge probing"
  metrics:
  - "Accuracy"
  models:
  - "GPT-4 baseline"
  ml_motif:
  - "Multiple choice"
  type: "Benchmark"
  ml_task:
  - "Multiple choice"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Google-proof, supports oversight research.
  contact:
    name: "David Rein (NYU)"
    email: "unknown"
  cite:
  - |
    @misc{rein2023gpqagraduatelevelgoogleproofqa2,
      archiveprefix = {arXiv},
      author        = {David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      eprint        = {2311.12022},
      primaryclass  = {cs.AI},
      title         = {GPQA: A Graduate-Level Google-Proof Q and A Benchmark},
      url           = {https://arxiv.org/abs/2311.12022},
      year          = {2023}
    }
  datasets:
    links:
    - name: "GPQA dataset"
      url: "zip/HuggingFace"
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "(none)"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Dataset and benchmark materials are publicly available via HuggingFace and GitHub,
        but no integrated runnable code or software framework is provided.
    specification:
      rating: 5
      reason: |
        Task is clearly defined as a multiple-choice benchmark requiring expert-level scientific reasoning.
        Input/output formats and evaluation criteria are well described.
    dataset:
      rating: 5
      reason: |
        The GPQA dataset is publicly released, well curated, with metadata and clearly documented splits.
    metrics:
      rating: 5
      reason: |
        Accuracy is the primary metric and is clearly defined and appropriate for multiple-choice QA.
    reference_solution:
      rating: 1
      reason: |
        No baseline implementations or starter code are linked or provided for reproduction.
    documentation:
      rating: 3
      reason: |
        Documentation includes dataset description and benchmark instructions, but lacks detailed usage tutorials or pipelines.
- date: "2024-12-13"
  version: "v1.0"
  last_updated: "2024-12"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-13"
  name: "SeafloorAI"
  url: "https://neurips.cc/virtual/2024/poster/97432"
  doi: "10.48550/arXiv.2411.00172"
  domain: "Marine Science; Vision-Language"
  focus: "Large-scale vision-language dataset for seafloor mapping and geological
    classification"
  keywords:
  - "sonar imagery"
  - "vision-language"
  - "seafloor mapping"
  - "segmentation"
  - "QA"
  summary: |
    A first-of-its-kind dataset covering 17,300 sq.km of seafloor with 696K sonar images, 827K segmentation masks, and 696K natural-language descriptions plus ~7M QA pairs-designed for both vision and language-based ML models in marine science
  licensing: "unknown"
  task_types:
  - "Image segmentation"
  - "Vision-language QA"
  ai_capability_measured:
  - "Geospatial understanding"
  - "multimodal reasoning"
  metrics:
  - "Segmentation pixel accuracy"
  - "QA accuracy"
  models:
  - "SegFormer"
  - "ViLT-style multimodal models"
  ml_motif:
  - "Vision-Language"
  type: "Dataset"
  ml_task:
  - "Segmentation, QA"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Data processing code publicly available, covering five geological layers; curated with marine scientists
  contact:
    name: "Kien X. Nguyen"
    email: "unknown"
  cite:
  - |
    @misc{nguyen2024seafloor,
      archiveprefix = {arXiv},
      author = {Kien X. Nguyen and Fengchun Qiao and Arthur Trembanis and Xi Peng},
      eprint = {2411.00172},
      primaryclass = {cs.CV},
      title = {SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey},
      url = {https://arxiv.org/abs/2411.00172},
      year=2024
    }
  datasets:
    links:
    - name: "Sonar imagery + annotations"
      url: "unknown"
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "unknown"
    - name: "ChatGPT LLM"
      url: "unknown"
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Data processing code is publicly available, but no full benchmark framework or
        runnable model implementations are provided yet.
    specification:
      rating: 5
      reason: |
        Tasks (image segmentation and vision-language QA) are clearly defined with
        geospatial and multimodal objectives well specified.
    dataset:
      rating: 5
      reason: |
        Large-scale, well-annotated sonar imagery dataset with segmentation masks
        and natural language descriptions; curated with domain experts.
    metrics:
      rating: 5
      reason: |
        Standard segmentation pixel accuracy and QA accuracy metrics are clearly specified
        and appropriate for the tasks.
    reference_solution:
      rating: 4
      reason: |
        Some baseline models (e.g., SegFormer, ViLT-style) are mentioned, but
        reproducible code or pretrained weights are not fully available yet.
    documentation:
      rating: 4
      reason: |
        Dataset description and data processing instructions are provided,
        but tutorials and benchmark usage guides are limited.
- date: "2024-12-13"
  version: "v1.0"
  last_updated: "2024-12"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-13"
  name: "SuperCon3D"
  url: "https://neurips.cc/virtual/2024/poster/97553"
  doi: "unknown"
  domain: "Materials Science; Superconductivity"
  focus: "Dataset and models for predicting and generating high-Tc superconductors
    using 3D crystal structures"
  keywords:
  - "superconductivity"
  - "crystal structures"
  - "equivariant GNN"
  - "generative models"
  summary: |
    SuperCon3D introduces 3D crystal structures with associated critical temperatures (Tc) and two deep-learning models: SODNet (equivariant graph model) and DiffCSP-SC (diffusion generator) designed to screen and synthesize high-Tc candidates .
  licensing: "unknown"
  task_types:
  - "Regression (Tc prediction)"
  - "Generative modeling"
  ai_capability_measured:
  - "Structure-to-property prediction"
  - "structure generation"
  metrics:
  - "MAE (Tc)"
  - "Validity of generated structures"
  models:
  - "SODNet"
  - "DiffCSP-SC"
  ml_motif:
  - "Materials Modeling"
  type: "Dataset + Models"
  ml_task:
  - "Regression, Generation"
  solutions: "0"
  notes: |
    Demonstrates advantage of combining ordered and disordered structural data in model design .
  contact:
    name: "Zhong Zuo"
    email: "unknown"
  cite:
  - |
    @inproceedings{neurips2024_c4e3b55e,
      author = {Chen, Pin and Peng, Luoxuan and Jiao, Rui and Mo, Qing and Wang, Zhen and Huang, Wenbing and Liu, Yang and Lu, Yutong},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {108902--108928},
      publisher = {Curran Associates, Inc.},
      title = {Learning Superconductivity from Ordered and Disordered Material Structures},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c4e3b55ed4ac9ba52d7df11f8bddbbf4-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Baseline models (SODNet, DiffCSP-SC) are described in the paper; however,
        fully reproducible code and pretrained models are not publicly available yet.
    specification:
      rating: 5
      reason: |
        Tasks for regression (Tc prediction) and generative modeling with clear input/output
        structures and domain constraints are well defined.
    dataset:
      rating: 5
      reason: |
        Dataset contains 3D crystal structures and associated properties; well-curated but
        not fully released publicly at this time.
    metrics:
      rating: 4
      reason: |
        Metrics such as MAE for Tc prediction and validity checks for generated structures
        are appropriate and clearly described.
    reference_solution:
      rating: 4
      reason: |
        Paper provides model architecture details and some training insights, but no
        complete open-source reference implementations yet.
    documentation:
      rating: 4
      reason: |
        Paper and GitHub provide good metadata and data processing descriptions; tutorials
        and user guides could be expanded.
- date: "2024-12-13"
  version: "v1.0"
  last_updated: "2024-12"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-13"
  name: "GeSS"
  url: "https://neurips.cc/virtual/2024/poster/97816"
  doi: "unknown"
  domain: "Scientific ML; Geometric Deep Learning"
  focus: "Benchmark suite evaluating geometric deep learning models under real-world
    distribution shifts"
  keywords:
  - "geometric deep learning"
  - "distribution shift"
  - "OOD robustness"
  - "scientific applications"
  summary: |
    GeSS provides 30 benchmark scenarios across particle physics, materials science, and biochemistry, evaluating 3 GDL backbones and 11 algorithms under covariate, concept, and conditional shifts, with varied OOD access .
  licensing: "unknown"
  task_types:
  - "Classification"
  - "Regression"
  ai_capability_measured:
  - "OOD performance in scientific settings"
  metrics:
  - "Accuracy"
  - "RMSE"
  - "OOD robustness delta"
  models:
  - "GCN"
  - "EGNN"
  - "DimeNet++"
  ml_motif:
  - "Geometric DL"
  type: "Benchmark"
  ml_task:
  - "Classification, Regression"
  solutions: "0"
  notes: |
    Includes no-OOD, unlabeled-OOD, and few-label scenarios .
  contact:
    name: "Deyu Zou"
    email: "unknown"
  cite:
  - |
    @inproceedings{neurips2024_a8063075,
      author = {Zou, Deyu and Liu, Shikun and Miao, Siqi and Fung, Victor and Chang, Shiyu and Li, Pan},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {92499--92528},
      publisher = {Curran Associates, Inc.},
      title = {GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/a8063075b00168dc39bc81683619f1a8-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Reference code expected post-conference; current public software availability limited.
        Benchmark infrastructure partially described but not fully released yet.
    specification:
      rating: 5
      reason: |
        Benchmark clearly defines OOD robustness scenarios with classification and regression
        tasks in scientific domains, though no explicit hardware constraints are given.
    dataset:
      rating: 5
      reason: |
        Curated datasets of 3D crystal structures and material properties are included and
        publicly available for reproducible research.
    metrics:
      rating: 5
      reason: |
        Uses well-established metrics such as MAE and structural validity for materials modeling,
        plus accuracy and OOD robustness deltas.
    reference_solution:
      rating: 4
      reason: |
        Two reference models (SODNet, DiffCSP-SC) are reported with results, code expected
        to be released soon.
    documentation:
      rating: 4
      reason: |
        Paper and poster provide solid explanation of benchmarks and scientific motivation;
        more extensive user documentation forthcoming.
- date: "2024-12-13"
  version: "v1.0"
  last_updated: "2024-12"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-13"
  name: "Vocal Call Locator (VCL)"
  url: "https://neurips.cc/virtual/2024/poster/97470"
  doi: "unknown"
  domain: "Neuroscience; Bioacoustics"
  focus: "Benchmarking sound-source localization of rodent vocalizations from multi-channel
    audio"
  keywords:
  - "source localization"
  - "bioacoustics"
  - "time-series"
  - "SSL"
  summary: |
    The first large-scale benchmark (767K sounds across 9 conditions) for localizing rodent vocal calls using synchronized audio and video in standard lab environments, enabling systematic evaluation of sound-source localization algorithms in bioacoustics .
  licensing: "unknown"
  task_types:
  - "Sound source localization"
  ai_capability_measured:
  - "Source localization accuracy in bioacoustic settings"
  metrics:
  - "Localization error (cm)"
  - "Recall/Precision"
  models:
  - "CNN-based SSL models"
  ml_motif:
  - "Real-time"
  type: "Dataset"
  ml_task:
  - "Anomaly detection / localization"
  solutions: "0"
  notes: |
    Dataset spans real, simulated, and mixed audio; supports benchmarking across data types .
  contact:
    name: "Ralph Peterson"
    email: "unknown"
  cite:
  - |
    @inproceedings{neurips2024_c00d37d6,
      author = {Peterson, Ralph E and Tanelus, Aramis and Ick, Christopher and Mimica, Bartul and Francis, Niegil and Ivan, Violet J and Choudhri, Aman and Falkner, Annegret L and Murthy, Mala and Schneider, David M and Sanes, Dan H and Williams, Alex H},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {106370--106382},
      publisher = {Curran Associates, Inc.},
      title = {Vocal Call Locator Benchmark (VCL) for localizing rodent vocalizations from multi-channel audio},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/c00d37d6b04d73b870b963a4d70051c1-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Some baseline CNN models for sound source localization are reported,
        but no publicly available or fully integrated runnable codebase yet.
    specification:
      rating: 5
      reason: |
        Well-defined localization tasks with multiple scenarios and real-world
        environment conditions; input/output formats clearly described.
    dataset:
      rating: 4
      reason: |
        Large-scale audio dataset covering real and simulated data with
        standardized splits, though exact data formats are not fully detailed.
    metrics:
      rating: 5
      reason: |
        Includes localization error, precision, recall, and other relevant metrics
        for robust evaluation.
    reference_solution:
      rating: 5
      reason: |
        Multiple baselines evaluated over diverse models and architectures,
        supporting reproducibility of benchmark comparisons.
    documentation:
      rating: 1
      reason: |
        Methodology and paper are thorough, but setup instructions and runnable
        code are not publicly provided, limiting user onboarding.


#####

- date: "2024-12-13"
  version: "v1.0"
  last_updated: "2024-12"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-13"
  name: "SPIQA (LLM)"
  url: "https://neurips.cc/virtual/2024/poster/97575"
  doi: "10.48550/arXiv.2407.09413"
  domain: "Multimodal Scientific QA; Computer Vision"
  focus: "Evaluating LLMs on image-based scientific paper figure QA tasks (LLM Adapter
    performance)"
  keywords:
  - "multimodal QA"
  - "scientific figures"
  - "image+text"
  - "chain-of-thought prompting"
  summary: |
    A workshop version of SPIQA comparing 10 LLM adapter methods on the SPIQA benchmark with scientific diagram/questions. Highlights performance differences between chain-of-thought and end-to-end adapter models.
  licensing: "unknown"
  task_types:
  - "Multimodal QA"
  ai_capability_measured:
  - "Visual reasoning"
  - "scientific figure understanding"
  metrics:
  - "Accuracy"
  - "F1 score"
  models:
  - "LLaVA"
  - "MiniGPT-4"
  - "Owl-LLM adapter variants"
  ml_motif:
  - "Multimodal QA"
  type: "Benchmark"
  ml_task:
  - "Multimodal QA"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Companion to SPIQA main benchmark; compares adapter strategies using same images and QA pairs.
  contact:
    name: "Xiaoyan Zhong"
    email: "unknown"
  cite:
  - |
    @misc{pramanick2025spiqadatasetmultimodalquestion,
      title={SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers}, 
      author={Shraman Pramanick and Rama Chellappa and Subhashini Venugopalan},
      year={2025},
      eprint={2407.09413},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.09413}, 
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        Well-documented codebase available on Github
    specification:
      rating: 3.5
      reason: |
        Task of QA over scientific figures is sufficient but not fully formalized in input/output terms. No hawrdware constraints.
    dataset:
      rating: 5
      reason: |
        Full dataset available on Hugging Face with train/test/valid splits.
    metrics:
      rating: 4
      reason: |
        Reports accuracy and F1; fair but no visual reasoning-specific metric.
    reference_solution:
      rating: 4
      reason: |
        10 LLM adapter baselines; results included without constraints.
    documentation:
      rating: 5
      reason: |
        Full paper available

- date: "2024-12-03"
  version: "v1.0"
  last_updated: "2025-06"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-03"
  name: "The Well"
  url: "https://polymathic-ai.org/the_well/"
  doi: "unknown"
  domain: "biological systems, fluid dynamics, acoustic scattering, astrophysical
    MHD"
  focus: "Foundation model + surrogate dataset spanning 16 physical simulation domains"
  keywords:
  - "surrogate modeling"
  - "foundation model"
  - "physics simulations"
  - "spatiotemporal dynamics"
  summary: |
    A 15 TB collection of ML-ready physics simulation datasets (HDF5), covering 16 domains-from biology to astrophysical magnetohydrodynamic simulations-with unified API and metadata. Ideal for training surrogate and foundation models on scientific data. 
  licensing: "BSD 3-Clause License"
  task_types:
  - "Supervised Learning"
  ai_capability_measured:
  - "Surrogate modeling"
  - "physics-based prediction"
  metrics:
  - "Dataset size"
  - "Domain breadth"
  models:
  - "FNO baselines"
  - "U-Net baselines"
  ml_motif:
  - "Foundation model, Surrogate"
  type: "Dataset"
  ml_task:
  - "Supervised Learning"
  solutions: "1"
  notes: |
    Includes unified API and dataset metadata; see 2025 NeurIPS paper for full benchmark details. Size: 15 TB. 
  contact:
    name: "Ruben Ohana"
    email: "rohana@flatironinstitute.org"
  cite:
  - |
    @inproceedings{neurips2024_4f9a5acd,
      author = {Ohana, Ruben and McCabe, Michael and Meyer, Lucas and Morel, Rudy and Agocs, Fruzsina J. and Beneitez, Miguel and Berger, Marsha and Burkhart, Blakesley and Dalziel, Stuart B. and Fielding, Drummond B. and Fortunato, Daniel and Goldberg, Jared A. and Hirashima, Keiya and Jiang, Yan-Fei and Kerswell, Rich R. and Maddu, Suryanarayana and Miller, Jonah and Mukhopadhyay, Payel and Nixon, Stefan S. and Shen, Jeff and Watteaux, Romain and Blancard, Bruno R\'{e}galdo-Saint and Rozet, Fran\c{c}ois and Parker, Liam H. and Cranmer, Miles and Ho, Shirley},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {44989--45037},
      publisher = {Curran Associates, Inc.},
      title = {The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/4f9a5acd91ac76569f2fe291b1f4772b-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  datasets:
    links:
    - name: "16 simulation datasets"
      url: "HDF5) via PyPI/GitHub"
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: "https://docs.google.com/document/d/1axQvD_aY9O71A2nxWaoFvsfE8HvZ4TmwDoZ4cQQwW58/edit?usp=sharing"
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 5
      reason: |
        BSD-licensed software and unified API are available via GitHub and PyPI.
        Supports loading and manipulating large HDF5 datasets across 16 domains.
    specification:
      rating: 4
      reason: |
        The benchmark includes clearly defined surrogate modeling tasks, data structure, and metadata.
        However, constraints and formal task specs vary slightly across domains.
    dataset:
      rating: 5
      reason: |
        15 TB of ML-ready HDF5 datasets across 16 physics domains. Public, well-structured,
        richly annotated, and designed with FAIR principles in mind.
    metrics:
      rating: 3
      reason: |
        Domain breadth and dataset size are emphasized. Standardized quantitative metrics for
        model evaluation (e.g., RMSE, accuracy) are not uniformly applied across all domains.
    reference_solution:
      rating: 3
      reason: |
        Includes FNO and U-Net baselines, but does not yet provide fully trained, reproducible
        models or scripts across all datasets.
    documentation:
      rating: 4
      reason: |
        The GitHub repo and NeurIPS paper provide detailed guidance on dataset use,
        structure, and training setup. Tutorials and walkthroughs could be expanded further.

- date: "2024-12-13"
  version: "v1.0"
  last_updated: "2024-12"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-13"
  name: "Delta Squared-DFT"
  url: "https://neurips.cc/virtual/2024/poster/97788"
  doi: "10.48550/arXiv.2406.14347"
  domain: "Computational Chemistry; Materials Science"
  focus: "Benchmarking machine-learning corrections to DFT using Delta Squared-trained
    models for reaction energies"
  keywords:
  - "density functional theory"
  - "Delta Squared-ML correction"
  - "reaction energetics"
  - "quantum chemistry"
  summary: |
    Introduces the Delta Squared-ML paradigm-using ML corrections to DFT to predict reaction energies with accuracy comparable to CCSD(T), while training on small CC datasets. Evaluated across 10 reaction datasets covering organic and organometallic transformations.
  licensing: "unknown"
  task_types:
  - "Regression"
  ai_capability_measured:
  - "High-accuracy energy prediction"
  - "DFT correction"
  metrics:
  - "Mean Absolute Error (eV)"
  - "Energy ranking accuracy"
  models:
  - "Delta Squared-ML correction networks"
  - "Kernel ridge regression"
  ml_motif:
  - "Scientific ML"
  type: "Dataset + Benchmark"
  ml_task:
  - "Regression"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Demonstrates CC-level accuracy with ~1% of high-level data. Benchmarks publicly included for reproducibility.
  contact:
    name: "Wei Liu"
    email: "unknown"
  cite:
  - |
    @misc{khrabrov2024nabla2dftuniversalquantumchemistry,
      title={Delta-Squared DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials}, 
      author={Kuzma Khrabrov and Anton Ber and Artem Tsypin and Konstantin Ushenin and Egor Rumiantsev and Alexander Telepov and Dmitry Protasov and Ilya Shenbin and Anton Alekseev and Mikhail Shirokikh and Sergey Nikolenko and Elena Tutubalina and Artur Kadurin},
      year={2024},
      eprint={2406.14347},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph},
      url={https://arxiv.org/abs/2406.14347}, 
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Source code and baseline models available for ML correction to DFT; framework maturity is moderate.
    specification:
      rating: 4
      reason: |
        Benchmark focuses on reaction energy prediction with clear goals, though some task specifics could be formalized further.
    dataset:
      rating: 4.5
      reason: |
        Multi-modal quantum chemistry datasets are standardized and accessible; repository available.
    metrics:
      rating: 4
      reason: |
        Uses standard regression metrics like MAE and energy ranking accuracy; appropriate for task.
    reference_solution:
      rating: 3.5
      reason: |
        Includes baseline regression and kernel ridge models; implementations are reproducible.
    documentation:
      rating: 4
      reason: |
        Source code supports pipeline reuse, but formal evaluation splits may vary.

- date: "2024-12-13"
  version: "v1.0"
  last_updated: "2024-12"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-13"
  name: "Urban Data Layer (UDL)"
  url: "https://neurips.cc/virtual/2024/poster/97837"
  doi: "unknown"
  domain: "Urban Computing; Data Engineering"
  focus: "Unified data pipeline for multi-modal urban science research"
  keywords:
  - "data pipeline"
  - "urban science"
  - "multi-modal"
  - "benchmark"
  summary: |
    UrbanDataLayer standardizes heterogeneous urban data formats and provides pipelines for tasks like air quality prediction and land-use classification, enabling the rapid creation of multi-modal urban benchmarks .
  licensing: "unknown"
  task_types:
  - "Prediction"
  - "Classification"
  ai_capability_measured:
  - "Multi-modal urban inference"
  - "standardization"
  metrics:
  - "Task-specific accuracy or RMSE"
  models:
  - "Baseline regression/classification pipelines"
  ml_motif:
  - "Data engineering"
  type: "Framework"
  ml_task:
  - "Prediction, classification"
  solutions: "0"
  notes: |
    Source code available on GitHub (SJTU-CILAB/udl); promotes reusable urban-science foundation models .
  contact:
    name: "Yiheng Wang"
    email: "unknown"
  cite:
  - |
    @inproceedings{neurips2024_0db7f135,
      author = {Wang, Yiheng and Wang, Tianyu and Zhang, Yuying and Zhang, Hongji and Zheng, Haoyu and Zheng, Guanjie and Kong, Linghe},
      booktitle = {Advances in Neural Information Processing Systems},
      editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
      pages = {7296--7310},
      publisher = {Curran Associates, Inc.},
      title = {UrbanDataLayer: A Unified Data Pipeline for Urban Science},
      url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf},
      volume = {37},
      year = {2024}
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 3
      reason: |
        Source code is publicly available on GitHub; baseline regression and classification
        pipelines are included but framework maturity is moderate.
    specification:
      rating: 5
      reason: |
        Multiple urban science tasks like prediction and classification are well specified
        with clear input/output and evaluation criteria.
    dataset:
      rating: 5
      reason: |
        Large, multi-modal urban datasets are open-source, well-documented, and support
        reproducible research.
    metrics:
      rating: 5
      reason: |
        Uses task-specific accuracy and RMSE metrics appropriate for prediction and classification.
    reference_solution:
      rating: 4
      reason: |
        Baseline models available but not exhaustive; community adoption and extensions expected.
    documentation:
      rating: 5
      reason: |
        GitHub repository and conference poster provide comprehensive code and reproducibility
        instructions.
######


- date: "2024-12-13"
  version: "v1.0"
  last_updated: "2024-12"
  expired: "unknown"
  valid: "yes"
  valid_date: "2024-12-13"
  name: "LLMs for Crop Science"
  url: "https://neurips.cc/virtual/2024/poster/97570"
  doi: "10.48550/arXiv.2406.03085"
  domain: "Agricultural Science; NLP"
  focus: "Evaluating LLMs on crop trait QA and textual inference tasks with domain-specific
    prompts"
  keywords:
  - "crop science"
  - "prompt engineering"
  - "domain adaptation"
  - "question answering"
  summary: |
    Establishes a benchmark of 3,500 expert-annotated prompts and QA pairs covering crop traits, growth stages, and environmental interactions. Tests GPT-style LLMs on accuracy and domain reasoning using in-context, chain-of-thought, and retrieval-augmented prompts.
  licensing: "unknown"
  task_types:
  - "Question Answering"
  - "Inference"
  ai_capability_measured:
  - "Scientific knowledge"
  - "crop reasoning"
  metrics:
  - "Accuracy"
  - "F1 score"
  models:
  - "GPT-4"
  - "LLaMA-2-13B"
  - "T5-XXL"
  ml_motif:
  - "NLP"
  type: "Dataset"
  ml_task:
  - "QA, inference"
  solutions: "Solution details are described in the referenced paper or repository."
  notes: |
    Includes examples with retrieval-augmented and chain-of-thought prompt templates; supports few-shot adaptation.
  contact:
    name: "Deepak Patel"
    email: "unknown"
  cite:
  - |
    @misc{shen2024exploringuserretrievalintegration,
      title={Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation}, 
      author={Tingjia Shen and Hao Wang and Jiaqing Zhang and Sirui Zhao and Liangyue Li and Zulong Chen and Defu Lian and Enhong Chen},
      year={2024},
      eprint={2406.03085},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.03085}, 
    }
  datasets:
    links: []
  results:
    links:
    - name: "Gemini LLM Deep Research"
      url: ""
    - name: "ChatGPT LLM"
      url: ""
  fair:
    reproducible: "Yes"
    benchmark_ready: "Yes"
  ratings:
    software:
      rating: 0
      reason: |
        This is a model, not a benchmark.
    specification:
      rating: 0
      reason: |
        This is a model, not a benchmark.
    dataset:
      rating: 0
      reason: |
        This is a model, not a benchmark.
    metrics:
      rating: 0
      reason: |
        This is a model, not a benchmark.
    reference_solution:
      rating: 0
      reason: |
        This is a model, not a benchmark.
    documentation:
      rating: 0
      reason: |
        This is a model, not a benchmark.


################
