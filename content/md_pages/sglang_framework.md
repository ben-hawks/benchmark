# SGLang Framework

**Date**: 2023-12-12

**Expiration**: 

**Valid**: yes

**Name**: SGLang Framework

**URL**: https://github.com/sgl-project/sglang/tree/main/benchmark

**Domain**: LLM Vision

**Focus**: Fast serving framework for LLMs and vision-language models

**Keywords**: LLM serving, vision-language, RadixAttention, performance, JSON decoding

**Description**: A high-performance open-source serving framework combining efficient backend runtime  RadixAttention, batching, quantization  and expressive frontend language, boosting LLM/VLM inference throughput up to ~3x over alternatives. :contentReference oaicite:5 {index=5} 

**Task Types**: Model serving framework

**AI Capability**: Serving throughput, JSON/task-specific latency

**Metrics**: Tokens/sec, Time-to-first-token, Throughput gain vs baseline

**Models**: LLaVA, DeepSeek, Llama

**Notes**: Deployed in production  xAI, NVIDIA, Google Cloud ; v0.4.8 release June 2025. :contentReference oaicite:6 {index=6}

**Citation**:

-
  - type: article
  - id: zheng2023sglang
  - url: https://arxiv.org/abs/2312.07104
  - year: 2023
  - author: Zheng, Lianmin, Yin, Liangsheng, others
  - title: SGLang: Efficient Execution of Structured Language Model Programs
  - bibtex: |
      @article{zheng2023sglang,
        title={SGLang: Efficient Execution of Structured Language Model Programs},
        author={Zheng, Lianmin and Yin, Liangsheng and others},
        year={2023},
        url={https://arxiv.org/abs/2312.07104}
      }

