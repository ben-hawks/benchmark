<h1 id="commonsenseqa">CommonSenseQA</h1>
<p><strong>Edit:</strong> <a
href="https://github.com/mlcommons-science/benchmark/tree/main/source">edit
this entry</a></p>
<p><strong>Date</strong>: 2019-11-20</p>
<p><strong>Name</strong>: CommonSenseQA</p>
<p><strong>Domain</strong>: NLP; Commonsense</p>
<p><strong>Focus</strong>: Commonsense question answering</p>
<p><strong>Keywords</strong>: ConceptNet, multiple-choice,
adversarial</p>
<p><strong>Task Types</strong>: Multiple choice</p>
<p><strong>Metrics</strong>: Accuracy</p>
<p><strong>Models</strong>: BERT-large, RoBERTa, GPT-3</p>
<p><strong>Citation</strong>:</p>
<ul>
<li><p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan
Berant. Commonsenseqa: a question answering challenge targeting
commonsense knowledge. 2019. URL: https://arxiv.org/abs/1811.00937,
arXiv:1811.00937.</p>
<ul>
<li><p>bibtex: ``` <span class="citation"
data-cites="misc">@misc</span>{talmor2019commonsenseqaquestionansweringchallenge,</p>
<pre><code>title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 

author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},

year={2019},

eprint={1811.00937},

archivePrefix={arXiv},

primaryClass={cs.CL},

url={https://arxiv.org/abs/1811.00937}, </code></pre>
<p>}</p>
<p>```</p></li>
</ul></li>
</ul>
<p><strong>Ratings:</strong></p>
<p>Software:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> All code given on Github site</p></li>
</ul>
<p>Specification:</p>
<ul>
<li><p><strong>Rating:</strong> 4</p></li>
<li><p><strong>Reason:</strong> Task and format multiple-choice QA with
5 options are clearly defined; grounded in ConceptNet with consistent
structure, though no hardware/system constraints are specified.</p></li>
</ul>
<p>Dataset:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Public, versioned, and FAIR-compliant;
includes metadata, splits, and licensing; well-integrated with
HuggingFace and other ML libraries.</p></li>
</ul>
<p>Metrics:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Accuracy is a simple, reproducible
metric aligned with task goals; no ambiguity in evaluation.</p></li>
</ul>
<p>Reference Solution:</p>
<ul>
<li><p><strong>Rating:</strong> 4</p></li>
<li><p><strong>Reason:</strong> Several baseline models e.g., BERT,
RoBERTa are reported with scores; implementations exist in public repos,
but not run with hardware constraints</p></li>
</ul>
<p>Documentation:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Given in paper.</p></li>
</ul>
<p><strong>Average Rating:</strong> 4.667</p>
<p><strong>Radar Plot:</strong> <img
src="../../tex/images/commonsenseqa_radar.png"
alt="Commonsenseqa radar plot" /></p>
