- date: "2020-09-07"
  expired: null
  valid: 'yes'
  name: MMLU (Massive Multitask Language Understanding)
  url: https://paperswithcode.com/dataset/mmlu
  domain: Multidomain
  focus: Academic knowledge and reasoning across 57 subjects
  keyword:
    - multitask
    - multiple-choice
    - zero-shot
    - few-shot
    - knowledge probing
  description: |
    Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 
    multiple-choice tasks covering elementary mathematics, US history, computer science, 
    law, and more, designed to evaluate a model's breadth and depth of knowledge in 
    zero-shot and few-shot settings.
  task_types:
    - Multiple choice
  ai_capability_measured: General reasoning, subject-matter understanding
  metrics:
    - Accuracy
  models:
    - GPT-4o
    - Gemini 1.5 Pro
    - o1
    - DeepSeek-R1
  notes: Good
  cite:
    - "@article{hendrycks2021measuring,
       title={Measuring Massive Multitask Language Understanding},
       author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and others},
       journal={arXiv preprint arXiv:2009.03300},
       year={2021},
       url={https://arxiv.org/abs/2009.03300}
     }"

- date: "2023-11-20"
  expired: null
  valid: 'yes'
  name: GPQA Diamond
  url: https://arxiv.org/abs/2311.12022
  domain: Science
  focus: Graduate-level scientific reasoning
  keyword:
    - Google-proof
    - graduate-level
    - science QA
    - chemistry
    - physics
  description: |
    GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,
    and chemistry, written by domain experts. It is “Google-proof”—experts score 65% 
    (74% after error correction) while skilled non-experts with web access score only 34%. 
    State-of-the-art LLMs like GPT-4 reach around 39% accuracy.
  task_types:
    - Multiple choice
    - Multi-step QA
  ai_capability_measured: Scientific reasoning, deep knowledge
  metrics:
    - Accuracy
  models:
    - o1
    - DeepSeek-R1
  notes: Good
  cite:
    - "@misc{rein2023gpqagraduatelevelgoogleproofqa,
       title={GPQA: A Graduate-Level Google-Proof Q&A Benchmark},
       author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and others},
       year={2023},
       url={https://arxiv.org/abs/2311.12022}
     }"

- date: "2018-03-14"
  expired: null
  valid: 'yes'
  name: ARC-Challenge (Advanced Reasoning Challenge)
  url: https://allenai.org/data/arc
  domain: Science
  focus: Grade-school science with reasoning emphasis
  keyword:
    - grade-school
    - science QA
    - challenge set
    - reasoning
  description: |
    The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school
    science questions that retrieval-based and word co-occurrence algorithms both fail, 
    requiring advanced reasoning over a 14-million-sentence corpus.
  task_types:
    - Multiple choice
  ai_capability_measured: Commonsense and scientific reasoning
  metrics:
    - Accuracy
  models:
    - GPT-4
    - Claude
  notes: Good
  cite:
    - "@inproceedings{clark2018think,
       title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},
       author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and others},
       booktitle={EMNLP 2018},
       pages={237–248},
       year={2018},
       url={https://allenai.org/data/arc}
     }"

- date: "2025-01-24"
  expired: null
  valid: 'yes'
  name: Humanity's Last Exam
  url: https://arxiv.org/abs/2501.14249
  domain: Multidomain
  focus: Broad cross-domain academic reasoning
  keyword:
    - cross-domain
    - academic exam
    - multiple-choice
    - multidisciplinary
  description: |
    Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000
    questions across diverse academic disciplines, designed to evaluate LLMs' ability to
    reason across domains without external resources.
  task_types:
    - Multiple choice
  ai_capability_measured: Cross-domain academic reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - "@misc{phan2025humanitys,
       title={Humanity's Last Exam},
       author={Phan, Long and Gatti, Alice and Han, Ziwen and others},
       year={2025},
       url={https://arxiv.org/abs/2501.14249}
     }"

- date: "2024-11-07"
  expired: null
  valid: 'yes'
  name: FrontierMath
  url: https://arxiv.org/abs/2411.04872
  domain: Mathematics
  focus: Challenging advanced mathematical reasoning
  keyword:
    - symbolic reasoning
    - number theory
    - algebraic geometry
    - category theory
  description: |
    FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning
    number theory, real analysis, algebraic geometry, and category theory, measuring LLMs’ 
    ability to solve problems requiring deep abstract reasoning.
  task_types:
    - Problem solving
  ai_capability_measured: Symbolic and abstract mathematical reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - "@misc{glazer2024frontiermath,
       title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},
       author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and others},
       year={2024},
       url={https://arxiv.org/abs/2411.04872}
     }"

- date: "2024-07-18"
  expired: null
  valid: 'yes'
  name: SciCode
  url: https://arxiv.org/abs/2407.13168
  domain: Scientific Programming
  focus: Scientific code generation & problem solving
  keyword:
    - code synthesis
    - scientific computing
    - programming benchmark
  description: |
    SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80
    real research tasks across 16 scientific subfields, evaluating models on knowledge recall, 
    reasoning, and code synthesis for scientific computing tasks.
  task_types:
    - Coding
  ai_capability_measured: Program synthesis, scientific computing
  metrics:
    - Solve rate (%)
  models:
    - Claude3.5-Sonnet
  notes: Good
  cite:
    - "@misc{tian2024scicode,
       title={SciCode: A Research Coding Benchmark Curated by Scientists},
       author={Tian, Minyang and Gao, Luyu and Zhang, Shizhuo and others},
       year={2024},
       url={https://arxiv.org/abs/2407.13168}
     }"

- date: "2025-03-13"
  expired: null
  valid: 'yes'
  name: AIME (American Invitational Mathematics Examination)
  url: https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions
  domain: Mathematics
  focus: Pre-college advanced problem solving
  keyword:
    - algebra
    - combinatorics
    - number theory
    - geometry
  description: |
    The AIME is a 15-question, 3-hour exam for high-school students featuring challenging
    short-answer math problems in algebra, number theory, geometry, and combinatorics, 
    assessing depth of problem-solving ability.
  task_types:
    - Problem solving
  ai_capability_measured: Mathematical problem-solving and reasoning
  metrics:
    - Accuracy
  models: []
  notes: No formal paper; summary at https://www.vals.ai/benchmarks/aime-2025-03-13
  cite:
    - "aime_website"

- date: "2025-02-15"
  expired: null
  valid: 'yes'
  name: MATH-500
  url: https://huggingface.co/datasets/HuggingFaceH4/MATH-500
  domain: Mathematics
  focus: Math reasoning generalization
  keyword:
    - calculus
    - algebra
    - number theory
    - geometry
  description: |
    MATH-500 is a curated subset of 500 problems from the OpenAI MATH dataset, spanning
    high-school to advanced levels, designed to evaluate LLMs’ mathematical reasoning and 
    generalization.
  task_types:
    - Problem solving
  ai_capability_measured: Math reasoning and generalization
  metrics:
    - Accuracy
  models: []
  notes: Dataset hosted on Hugging Face
  cite:
    - "@misc{huggingface2025math500,
       title={MATH-500},
       author={HuggingFaceH4},
       year={2025},
       url={https://huggingface.co/datasets/HuggingFaceH4/MATH-500}
     }"

- date: "2024-04-02"
  expired: null
  valid: 'yes'
  name: CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)
  url: https://arxiv.org/abs/2404.02029
  domain: Multidomain Science
  focus: Long-context scientific reasoning
  keyword:
    - long-context
    - information extraction
    - multimodal
  description: |
    CURIE is a benchmark of 580 problems across six scientific disciplines—materials
    science, quantum computing, biology, chemistry, climate science, and astrophysics—
    designed to evaluate LLMs on long-context understanding, reasoning, and information 
    extraction in realistic scientific workflows.
  task_types:
    - Information extraction
    - Reasoning
    - Concept tracking
    - Aggregation
    - Algebraic manipulation
    - Multimodal comprehension
  ai_capability_measured: Long-context understanding and scientific reasoning
  metrics:
    - Accuracy
  models: []
  notes: Good
  cite:
    - "@misc{curie2024,
       title={Scientific Reasoning Benchmarks from the CURIE Dataset},
       author={TODO: Add authors},
       year={2024},
       url={https://arxiv.org/abs/2404.02029}
     }"

- date: "2023-01-26"
  expired: null
  valid: 'yes'
  name: FEABench (Finite Element Analysis Benchmark)
  url: https://github.com/alleninstitute/feabench
  domain: Computational Engineering
  focus: FEA simulation accuracy & performance
  keyword:
    - finite element
    - simulation
    - PDE
  description: |
    FEABench is a suite evaluating finite element analysis tools on standardized 
    PDE-based simulation tasks with complex geometries and boundary conditions, 
    measuring both accuracy and runtime performance.
  task_types:
    - Simulation
    - Performance evaluation
  ai_capability_measured: Numerical simulation accuracy and efficiency
  metrics:
    - Solve time
    - Error norm
  models:
    - FEniCS
    - deal.II
  notes: Good
  cite:
    - "@misc{allen2023feabench,
       title={FEABench: A Finite Element Analysis Benchmark},
       author={Allen Institute},
       year={2023},
       url={https://github.com/alleninstitute/feabench}
     }"

- date: "2024-07-12"
  expired: null
  valid: 'yes'
  name: SPIQA (Scientific Paper Image Question Answering)
  url: https://arxiv.org/abs/2407.09413
  domain: Computer Science
  focus: Multimodal QA on scientific figures
  keyword:
    - multimodal QA
    - figure understanding
    - table comprehension
    - chain-of-thought
  description: |
    SPIQA assesses AI models' ability to interpret and answer questions about figures
    and tables in scientific papers by integrating visual and textual modalities 
    with chain-of-thought reasoning.
  task_types:
    - Question answering
    - Multimodal QA
    - Chain-of-Thought evaluation
  ai_capability_measured: Visual-textual reasoning in scientific contexts
  metrics:
    - Accuracy
    - F1 score
  models:
    - Chain-of-Thought models
    - Multimodal QA systems
  notes: Good
  cite:
    - "@article{zhong2024spiqa,
       title={SPIQA: Scientific Paper Image Question Answering},
       author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin},
       year={2024},
       url={https://arxiv.org/abs/2407.09413}
     }"

- date: "2020-09-28"
  expired: null
  valid: 'yes'
  name: MedQA
  url: https://arxiv.org/abs/2009.13081
  domain: Medical Question Answering
  focus: Medical board exam QA
  keyword:
    - USMLE
    - diagnostic QA
    - medical knowledge
    - multilingual
  description: |
    MedQA is a large-scale multiple-choice dataset drawn from professional medical
    board exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge 
    questions in English and Chinese.
  task_types:
    - Multiple choice
  ai_capability_measured: Medical diagnosis and knowledge retrieval
  metrics:
    - Accuracy
  models:
    - Neural reader
    - Retrieval-based QA systems
  notes: Multilingual (English, Simplified & Traditional Chinese)
  cite:
    - "@article{jin2020what,
       title={What Disease Does This Patient Have? A Large-scale Open-domain Question Answering Dataset from Medical Exams},
       author={Jin, Di and Li, Ying and Zhang, Yichong and others},
       year={2020},
       url={https://arxiv.org/abs/2009.13081}
     }"

- date: "2025-05-13"
  expired: null
  valid: 'yes'
  name: BaisBench (Biological AI Scientist Benchmark)
  url: https://arxiv.org/abs/2505.083...