<h1 id="nixtla-neural-forecast-timellm">Nixtla Neural Forecast
TimeLLM</h1>
<p><strong>Date</strong>: 2023-10-03</p>
<p><strong>Name</strong>: Nixtla Neural Forecast TimeLLM</p>
<p><strong>Domain</strong>: Time-series; General ML</p>
<p><strong>Focus</strong>: Reprogramming LLMs for time series
forecasting</p>
<p><strong>Keywords</strong>: Time-LLM, language model, time-series,
reprogramming</p>
<p><strong>Task Types</strong>: Time-series forecasting</p>
<p><strong>Metrics</strong>: RMSE, MAPE</p>
<p><strong>Models</strong>: Time-LLM</p>
<p><strong>Citation</strong>:</p>
<ul>
<li><p>Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang,
Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and
Qingsong Wen. Time-llm: time series forecasting by reprogramming large
language models. 2024. URL: https://arxiv.org/abs/2310.01728,
arXiv:2310.01728.</p>
<ul>
<li><p>bibtex: ``` <span class="citation"
data-cites="misc">@misc</span>{jin2024timellmtimeseriesforecasting,</p>
<pre><code>title={Time-LLM: Time Series Forecasting by Reprogramming Large Language Models}, 

author={Ming Jin and Shiyu Wang and Lintao Ma and Zhixuan Chu and James Y. Zhang and Xiaoming Shi and Pin-Yu Chen and Yuxuan Liang and Yuan-Fang Li and Shirui Pan and Qingsong Wen},

year={2024},

eprint={2310.01728},

archivePrefix={arXiv},

primaryClass={cs.LG},

url={https://arxiv.org/abs/2310.01728}, </code></pre>
<p>}</p>
<p>```</p></li>
</ul></li>
</ul>
<p><strong>Ratings:</strong></p>
<p>Software:</p>
<ul>
<li><p><strong>Rating:</strong> 4</p></li>
<li><p><strong>Reason:</strong> Fully open-source under Apache 2.0,
integrated into the NeuralForecast library. Includes Time-LLM
implementation with example usage and training scripts.</p></li>
</ul>
<p>Specification:</p>
<ul>
<li><p><strong>Rating:</strong> 3</p></li>
<li><p><strong>Reason:</strong> High-level framing of forecasting as
language modeling is clear, but detailed input/output specifications,
constraints, and task formalization are minimal.</p></li>
</ul>
<p>Dataset:</p>
<ul>
<li><p><strong>Rating:</strong> 3</p></li>
<li><p><strong>Reason:</strong> Evaluated on standard datasets like M4
and ETT, but dataset splits and versioning are not bundled or explicitly
FAIR-compliant.</p></li>
</ul>
<p>Metrics:</p>
<ul>
<li><p><strong>Rating:</strong> 4</p></li>
<li><p><strong>Reason:</strong> Standard forecasting metrics such as
RMSE, MAPE, and SMAPE are reported. Evaluation is consistent, though
deeper metric justification is limited.</p></li>
</ul>
<p>Reference Solution:</p>
<ul>
<li><p><strong>Rating:</strong> 3</p></li>
<li><p><strong>Reason:</strong> Time-LLM implementation is open and
reproducible, but limited baselines or comparative implementations are
included directly.</p></li>
</ul>
<p>Documentation:</p>
<ul>
<li><p><strong>Rating:</strong> 3</p></li>
<li><p><strong>Reason:</strong> GitHub README provides installation and
quick usage examples, but lacks detailed API docs, training
walkthroughs, or extended tutorials.</p></li>
</ul>
<p><strong>Average Rating:</strong> 3.333</p>
<p><strong>Radar Plot:</strong> <img
src="../../tex/images/nixtla_neural_forecast_timellm_radar.png"
alt="Nixtla Neural Forecast Timellm radar plot" /></p>
