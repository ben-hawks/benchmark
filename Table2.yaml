- name: BaisBench (Biological AI Scientist Benchmark)
  description: Designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge in omics data-driven research.
  tasks:
    - Cell type annotation on single-cell datasets
    - Scientific discovery through multiple-choice questions derived from biological insights of recent single-cell studies.
  metrics:
    - Accuracy
  notable_models: Current models substantially underperform human experts.
  resources: "https://arxiv.org/abs/2505.08341"
  notes: null
  cite: \cite{@misc{luo2025benchmarkingaiscientistsomics,
          title={Benchmarking AI scientists in omics data-driven biological research}, 
          author={Erpai Luo and Jinmeng Jia and Yifan Xiong and Xiangyu Li and Xiaobo Guo and Baoqi Yu and Lei Wei and Xuegong Zhang},
          year={2025},
          eprint={2505.08341},
          archivePrefix={arXiv},
          primaryClass={cs.AI},
          url={https://arxiv.org/abs/2505.08341}, 
    }}
  
- name: MOLGEN
  description: A benchmark for molecular generation tasks in chemistry and drug discovery.
  tasks:
    - Generating novel molecules with desired properties.
  metrics:
    - Diversity
    - Validity
    - Novelty
    - Property scores
  notable_models: (Various generative AI models are benchmarked)
  resources: (Often part of broader computational chemistry AI platforms)
  notes: No paper, only a github at https://github.com/zjunlp/MolGen
  cite: null
  
- name: Open Graph Benchmark (OGB) - Biology
  description: Graph machine learning benchmarks for biological networks (e.g., protein-protein interaction networks, drug-target interactions).
  tasks:
    - Node classification
    - Link prediction
    - Graph classification on biological graphs
  metrics:
    - Accuracy
    - AUC
  notable_models: (Various graph neural networks and other graph ML models)
  resources: "https://ogb.stanford.edu/docs/home/"
  notes: Focuses on graph NNs
  cite: \cite{@misc{hu2021opengraphbenchmarkdatasets,
      title={Open Graph Benchmark: Datasets for Machine Learning on Graphs}, 
      author={Weihua Hu and Matthias Fey and Marinka Zitnik and Yuxiao Dong and Hongyu Ren and Bowen Liu and Michele Catasta and Jure Leskovec},
      year={2021},
      eprint={2005.00687},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.00687}, 
  }}

- name: Materials Project
  description: A vast database of material properties, often used as a benchmark for AI models predicting new materials.
  tasks:
    - Predicting material properties (e.g., bandgaps, formation energies, stability) for inorganic compounds.
  metrics:
    - Mean Absolute Error (MAE)
    - R-squared
  notable_models: MatGL, ALIGNN, MEGNet, etc.
  resources: "https://materialsproject.org/"
  notes: More like a database than a purpose-built AI benchmark
  cite: null

- name: OCP (Open Catalyst Project)
  description: Benchmarks for discovering new catalysts using AI, focusing on predicting adsorption energies and forces.
  tasks:
    - Predicting catalyst properties and reaction outcomes.
  metrics:
    - MAE on energies and forces
  notable_models: (Various graph neural networks and physics-informed models)
  resources: "https://opencatalystproject.org/"
  notes: Benchmark is from the Open Catalyst Challenge under the 'Datasets' subpage.
  cite: null
  
- name: JARVIS-Leaderboard (Joint Automated Repository for Various Integrated Simulations)
  description: NIST-maintained leaderboards for AI models in materials science, covering various properties and simulations.
  tasks:
    - Material property prediction (e.g., superconducting transition temperature)
    - Image classification in STEM
    - Force field prediction.
  metrics:
    - Task-specific metrics (e.g., MAE, accuracy)
  notable_models: (Various models for materials design)
  resources: "https://pages.nist.gov/jarvis_leaderboard/"
  notes: Collection of 322 benchmarks. One of the categories is AI.
  cite: null
  
- name: Quantum Computing Benchmarks (e.g., QML Benchmarks)
  description: Evaluates AI models for tasks in quantum computing, such as quantum state preparation, quantum control, and error correction.
  tasks:
    - Optimizing quantum circuits
    - Classifying quantum states.
  metrics:
    - Fidelity
    - Success probability
  notable_models: (Emerging field with specialized benchmarks)
  resources: ["https://github.com/XanaduAI/qml-benchmarks", "https://pennylane.ai/datasets/collection/qml-benchmarks"]
  notes: The github link has a script for evaluation, Pennylane has datasets
  cite: null
  
- name: Fluid Dynamics Benchmarks (e.g., based on CFD data)
  description: Benchmarks for AI models in computational fluid dynamics (CFD), such as predicting flow patterns or turbulence.
  tasks:
    - Solving Navier-Stokes equations
    - Predicting aerodynamic forces.
  metrics:
    - Error metrics (e.g., L2 error)
  notable_models: (Physics-informed neural networks, traditional ML models)
  resources: "https://arxiv.org/abs/2310.05963"
  notes: The link leads to a paper called CFDBench. Nowhere in the CFDBench paper does the word 'aerodynamic' appear.
  cite: \cite{@misc{luo2024cfdbenchlargescalebenchmarkmachine,
      title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics}, 
      author={Yining Luo and Yingfa Chen and Zhen Zhang},
      year={2024},
      eprint={2310.05963},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.05963}, 
  }}
  
- name: SatImgNet
  description: Benchmark for analyzing satellite imagery, relevant for climate science, disaster monitoring, and urban planning.
  tasks:
    - Object detection
    - Semantic segmentation
    - Change detection in satellite images.
  metrics:
    - mAP
    - IoU
    - Accuracy
  notable_models: (Various computer vision models)
  resources: null
  notes: Google search for 'satimgnet' and 'satimgnet benchmark' returned no ML-related results. 
  cite: null
  
- name: Climate Model Benchmarks (e.g., forecasting)
  description: Evaluating AI models for climate forecasting, predicting weather patterns, and understanding climate change.
  tasks:
    - Predicting temperature
    - Precipitation
    - Extreme weather events.
  metrics:
    - RMSE
    - Bias
  notable_models: (Neural weather models, climate models)
  resources: "https://arxiv.org/abs/2307.01909"
  notes: Closest result on Google is ClimateLearn, a framework for simplifying training/evaluation for ML models
  cite: \cite{@misc{nguyen2023climatelearnbenchmarkingmachinelearning,
      title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 
      author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},
      year={2023},
      eprint={2307.01909},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.01909}, 
  }}
  
- name: BIG-Bench (Beyond the Imitation Game Benchmark)
  description: A very large and diverse benchmark that includes many tasks requiring scientific reasoning and knowledge, often pushing the limits of language models.
  tasks:
    - Problem-solving
    - Knowledge recall
    - Common-sense reasoning across a wide array of topics, including scientific ones.
  metrics:
    - Accuracy
    - Various task-specific metrics.
  notable_models: (Many large language models are evaluated on BIG-Bench)
  resources: "https://github.com/google/BIG-bench"
  notes: The citation provided already appears in the Benchmark Carpentry paper's bibliography, so the provided citation is copied from there.
  cite: \cite{@article{srivastava2023beyond,
    title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
    author={BIG-bench authors},
    journal={Transactions on Machine Learning Research},
    issn={2835-8856},
    year={2023},
    url={https://openreview.net/forum?id=uyTL5Bvosj},
  }}

- name: CommonSenseQA
  description: Tests common sense reasoning, which is crucial for scientific understanding and problem-solving.
  tasks:
    - Answering multiple-choice questions that require context utilization and human-like language understanding.
  metrics:
    - Accuracy
  notable_models: (Evaluated across various LLMs)
  resources: "https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge"
  notes: This new link leads to the paper. No provided BibTex, so the BibTex citation wa manually written.
  cite: \cite{@misc{alontalmor2019commonsenseqa,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      url={https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge}, 
  }}
  
- name: Winogrande
  description: Assesses commonsense reasoning by resolving ambiguities in sentences that require an understanding of context.
  tasks:
    - Disambiguating sentences based on contextual understanding.
  metrics:
    - AUC
  notable_models: (Evaluated across various LLMs)
  resources: "https://leaderboard.allenai.org/winogrande/submissions/public"
  notes: The submission with an AUC of 1 is from a human. AUC is also listed as an evaluation metric.
  cite: null
