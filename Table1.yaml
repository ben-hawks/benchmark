#Yaml format 
---
- name: MMLU (Massive Multitask Language Understanding)
  cite: hendrycks2021measuring
  url: https://paperswithcode.com/dataset/mmlu
  domain: Multidomain
  focus: Academic knowledge and reasoning across 57 subjects
  task_types:
    - Multiple choice
  ai_capability_measured: General reasoning, subject-matter understanding
  notable_models:
    - GPT-4o
    - Gemini 1.5 Pro
    - o1
    - DeepSeek-R1
  Notes: Good
  Citations: |
    @article{hendrycks2021measuring,
      title={Measuring Massive Multitask Language Understanding},
      author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Zou, Eric and Lee, Dawn and Hall, Mantas and Ganguli, Deep and Tang, Danny and Song, Dawn and Steinhardt, Jacob and others},
      journal={arXiv preprint arXiv:2009.03300},
      year={2021},
      url={https://arxiv.org/abs/2009.03300}
    }

- name: GPQA Diamond
  cite: add citation
  url: https://arxiv.org/abs/2311.12022
  domain: Science
  focus: Graduate-level scientific reasoning
  task_types:
    - Multiple choice
    - Multi-step QA
  ai_capability_measured: Scientific reasoning, deep knowledge
  notable_models:
    - o1
    - DeepSeek-R1
  Notes: Good
  Citations: |
    @misc{rein2023gpqagraduatelevelgoogleproofqa,
      title={GPQA: A Graduate-Level Google-Proof Q&A Benchmark},
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      year={2023},
      eprint={2311.12022},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.12022}
    }

- name: ARC-Challenge (Advanced Reasoning Challenge)
  cite: clark2018think
  url: https://allenai.org/data/arc
  domain: Science
  focus: Grade-school science with an emphasis on reasoning
  task_types:
    - Multiple choice
  ai_capability_measured: Commonsense and scientific reasoning
  notable_models:
    - GPT-4
    - Claude
  Notes: Good
  Citations: |
    @inproceedings{clark2018think,
      title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge},
      author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
      booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
      pages={237--248},
      year={2018},
      organization={Association for Computational Linguistics},
      url={https://allenai.org/data/arc}
    }

- name: Humanity's Last Exam
  cite: add citation
  url: https://arxiv.org/abs/2501.14249
  domain: Multidomain
  focus: Broad academic evaluation to challenge top AI models
  task_types:
    - Multiple choice
  ai_capability_measured: Cross-domain academic reasoning
  notable_models: []
  Notes: Good
  Citations: |
    @misc{phan2025humanitys,
      title={Humanity’s Last Exam},
      author={Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Choi, Michael and Agrawal, Anish and Chopra, Arnav and Khoja, Adam and Kim, Ryan and Ren, Richard and Hausenloy, Jason and Zhang, Oliver and Mazeika, Mantas and Yue, Summer and Wang, Alexandr and Hendrycks, Dan and others},
      howpublished={arXiv preprint arXiv:2501.14249},
      month={jan},
      year={2025},
      url={https://arxiv.org/abs/2501.14249}
    }

- name: FrontierMath
  cite: add citation
  url: https://arxiv.org/abs/2411.04872
  domain: Mathematics
  focus: Challenging math problems for advanced reasoning
  task_types:
    - Problem solving
  ai_capability_measured: Symbolic and abstract mathematical reasoning
  notable_models: []
  Notes: Good
  Citations: |
    @misc{glazer2024frontiermath,
      title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI},
      author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and Chicharro, Diego and Chen, Evan and Gunning, Alex and Falkman Olsson, Caroline and Denain, Jean-Stanislas and Ho, Anson and de Oliveira Santos, Emily and Järviniemi, Olli and Barnett, Matthew and Sandler, Robert and Vrzala, Matej and Sevilla, Jaime and Ren, Qiuyu and Pratt, Elizabeth and Levine, Lionel and Barkley, Grant and Stewart, Natalie and Grechuk, Bogdan and Grechuk, Tetiana and Enugandla, Shreepranav Varma and Wildon, Mark},
      year={2024},
      howpublished={arXiv preprint arXiv:2411.04872},
      month={nov},
      note={Published 20 Dec 2024},
      url={https://arxiv.org/abs/2411.04872}
    }

- name: SciCode
  cite: add citation
  url: https://arxiv.org/abs/2407.13168
  domain: Scientific Programming
  focus: Scientific programming and algorithmic problem-solving
  task_types:
    - Coding
  ai_capability_measured: Program synthesis, scientific computing
  notable_models: []
  Notes: Good
  Citations: |
    @misc{tian2024scicode,
      title={SciCode: A Research Coding Benchmark Curated by Scientists},
      author={Tian, Minyang and Gao, Luyu and Zhang, Shizhuo Dylan and Chen, Xinan and Fan, Cunwei and Guo, Xuefei and Haas, Roland and Ji, Pan and Krongchon, Kittithat and Li, Yao and Liu, Shengyan and Luo, Di and Ma, Yutao and Tong, Hao and Zhang, Chenyu and Wang, Zihan and Wu, Bohao and Xiong, Yanyu and Yin, Shengzhu and Zhu, Minhui and Lieret, Kilian and Lu, Yanxin and Liu, Genglin and Du, Yufeng and Tao, Tianhua and Press, Ofir and Callan, Jamie and Huerta, Eliu A. and Peng, Hao},
      year={2024},
      howpublished={arXiv preprint arXiv:2407.13168},
      month={jul},
      note={Submitted 18 July 2024},
      url={https://arxiv.org/abs/2407.13168}
    }

- name: AIME (American Invitational Mathematics Examination)
  cite: aime_website
  url: https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions
  domain: Mathematics
  focus: Advanced problem-solving for pre-college students
  task_types:
    - Problem solving
  ai_capability_measured: Mathematical problem solving and reasoning
  notable_models: []
  Notes: No paper available; summary at https://www.vals.ai/benchmarks/aime-2025-03-13
  Citations: []

- name: MATH-500
  cite: add citation
  url: https://huggingface.co/datasets/HuggingFaceH4/MATH-500
  domain: Mathematics
  focus: Diverse math problems from high school to advanced levels
  task_types:
    - Problem solving
  ai_capability_measured: Math reasoning and generalization
  notable_models: []
  Notes: Dataset on Hugging Face
  Citations: []

- name: CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction)
  cite: curie2024
  url: https://arxiv.org/abs/2404.02029
  domain: Multidomain Science
  focus: Scientific problem-solving across six disciplines (e.g., materials science, quantum computing)
  task_types:
    - Information extraction
    - Reasoning
    - Concept tracking
    - Aggregation
    - Algebraic manipulation
    - Multimodal understanding
  ai_capability_measured: Long-context understanding, scientific reasoning, cross-domain knowledge
  notable_models: []
  Notes: Good
  Citations: |
    @misc{curie2024,
      title={Scientific Reasoning Benchmarks from the CURIE Dataset},
      author={TODO: Add authors from arXiv:2404.02029},
      year={2024},
      eprint={2404.02029},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.02029}
    }

- name: FEABench (Finite Element Analysis Benchmark)
  cite: zhu2024enhancingportfoliooptimizationtransformergan
  url: https://arxiv.org/abs/2404.02029
  domain: Engineering and Applied Physics
  focus: FEA-based physics, mathematics, and engineering simulation and reasoning
  task_types:
    - Finite Element Analysis
    - Simulation
    - Reasoning
  ai_capability_measured: Physics-informed simulation, mathematical modeling
  notable_models: []
  Notes: Good
  Citations: |
    @misc{zhu2024enhancingportfoliooptimizationtransformergan,
      title={Enhancing Portfolio Optimization via Transformer-GAN for Scientific Applications},
      author={Zhu, TODO: Add co-authors},
      year={2024},
      eprint={2404.02029},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02029}
    }

- name: SPIQA (Scientific Paper Image Question Answering)
  cite: spiqa2024
  url: https://arxiv.org/abs/2404.02029
  domain: Scientific Multimodal Understanding
  focus: Visual reasoning and question answering from scientific figures
  task_types:
    - Image-based QA
    - Figure reasoning
    - Long-context QA
  ai_capability_measured: Multimodal reasoning, scientific comprehension
  notable_models: []
  Notes: Good
  Citations: |
    @misc{spiqa2024,
      title={SPIQA: Scientific Paper Image Question Answering},
      author={TODO: Add authors from arXiv:2404.02029},
      year={2024},
      eprint={2404.02029},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.02029}
    }

- name: MedQA
  cite: add citation
  url: https://github.com/pubmedqa/MedQA
  domain: Biomedical and Clinical Science
  focus: Clinical knowledge and reasoning (based on USMLE-style questions)
