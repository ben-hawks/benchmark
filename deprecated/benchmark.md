| date | expiration | valid | name | url | domain | focus | keyword | description | task_types | ai_capability_measured | metrics | models | notes | cite |
|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|
| 2020-09-07 | None | yes | MMLU (Massive Multitask Language Understanding) | https://paperswithcode.com/dataset/mmlu | Multidomain | Academic knowledge and reasoning across 57 subjects | multitask, multiple-choice, zero-shot, few-shot, knowledge probing | Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57  multiple-choice tasks covering elementary mathematics, US history, computer science,  law, and more, designed to evaluate a model's breadth and depth of knowledge in  zero-shot and few-shot settings.  | Multiple choice | General reasoning, subject-matter understanding | Accuracy | GPT-4o, Gemini 1.5 Pro, o1, DeepSeek-R1 | Good | @article{hendrycks2021measuring, title={Measuring Massive Multitask Language Understanding}, author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and others}, journal={arXiv preprint arXiv:2009.03300}, year={2021}, url={https://arxiv.org/abs/2009.03300} } |
| 2023-11-20 | None | yes | GPQA Diamond | https://arxiv.org/abs/2311.12022 | Science | Graduate-level scientific reasoning | Google-proof, graduate-level, science QA, chemistry, physics | GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics, and chemistry, written by domain experts. It is “Google-proof”—experts score 65%  (74% after error correction) while skilled non-experts with web access score only 34%.  State-of-the-art LLMs like GPT-4 reach around 39% accuracy.  | Multiple choice, Multi-step QA | Scientific reasoning, deep knowledge | Accuracy | o1, DeepSeek-R1 | Good | @misc{rein2023gpqagraduatelevelgoogleproofqa, title={GPQA: A Graduate-Level Google-Proof Q&A Benchmark}, author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and others}, year={2023}, url={https://arxiv.org/abs/2311.12022} } |
| 2018-03-14 | None | yes | ARC-Challenge (Advanced Reasoning Challenge) | https://allenai.org/data/arc | Science | Grade-school science with reasoning emphasis | grade-school, science QA, challenge set, reasoning | The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school science questions that retrieval-based and word co-occurrence algorithms both fail,  requiring advanced reasoning over a 14-million-sentence corpus.  | Multiple choice | Commonsense and scientific reasoning | Accuracy | GPT-4, Claude | Good | @inproceedings{clark2018think, title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge}, author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and others}, booktitle={EMNLP 2018}, pages={237–248}, year={2018}, url={https://allenai.org/data/arc} } |
| 2025-01-24 | None | yes | Humanity's Last Exam | https://arxiv.org/abs/2501.14249 | Multidomain | Broad cross-domain academic reasoning | cross-domain, academic exam, multiple-choice, multidisciplinary | Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000 questions across diverse academic disciplines, designed to evaluate LLMs' ability to reason across domains without external resources.  | Multiple choice | Cross-domain academic reasoning | Accuracy |  | Good | ["@misc{phan2025humanitys, title={Humanity's Last Exam}, author={Phan, Long and Gatti, Alice and Han, Ziwen and others}, year={2025}, url={https://arxiv.org/abs/2501.14249} }"] |
| 2024-11-07 | None | yes | FrontierMath | https://arxiv.org/abs/2411.04872 | Mathematics | Challenging advanced mathematical reasoning | symbolic reasoning, number theory, algebraic geometry, category theory | FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning number theory, real analysis, algebraic geometry, and category theory, measuring LLMs’  ability to solve problems requiring deep abstract reasoning.  | Problem solving | Symbolic and abstract mathematical reasoning | Accuracy |  | Good | @misc{glazer2024frontiermath, title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI}, author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and others}, year={2024}, url={https://arxiv.org/abs/2411.04872} } |
| 2024-07-18 | None | yes | SciCode | https://arxiv.org/abs/2407.13168 | Scientific Programming | Scientific code generation & problem solving | code synthesis, scientific computing, programming benchmark | SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80 real research tasks across 16 scientific subfields, evaluating models on knowledge recall,  reasoning, and code synthesis for scientific computing tasks.  | Coding | Program synthesis, scientific computing | Solve rate (%) | Claude3.5-Sonnet | Good | @misc{tian2024scicode, title={SciCode: A Research Coding Benchmark Curated by Scientists}, author={Tian, Minyang and Gao, Luyu and Zhang, Shizhuo and others}, year={2024}, url={https://arxiv.org/abs/2407.13168} } |
| 2025-03-13 | None | yes | AIME (American Invitational Mathematics Examination) | https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions | Mathematics | Pre-college advanced problem solving | algebra, combinatorics, number theory, geometry | The AIME is a 15-question, 3-hour exam for high-school students featuring challenging short-answer math problems in algebra, number theory, geometry, and combinatorics,  assessing depth of problem-solving ability.  | Problem solving | Mathematical problem-solving and reasoning | Accuracy |  | No formal paper; summary at https://www.vals.ai/benchmarks/aime-2025-03-13 | aime_website |
| 2025-02-15 | None | yes | MATH-500 | https://huggingface.co/datasets/HuggingFaceH4/MATH-500 | Mathematics | Math reasoning generalization | calculus, algebra, number theory, geometry | MATH-500 is a curated subset of 500 problems from the OpenAI MATH dataset, spanning high-school to advanced levels, designed to evaluate LLMs’ mathematical reasoning and  generalization.  | Problem solving | Math reasoning and generalization | Accuracy |  | Dataset hosted on Hugging Face | @misc{huggingface2025math500, title={MATH-500}, author={HuggingFaceH4}, year={2025}, url={https://huggingface.co/datasets/HuggingFaceH4/MATH-500} } |
| 2024-04-02 | None | yes | CURIE (Scientific Long-Context Understanding, Reasoning and Information Extraction) | https://arxiv.org/abs/2404.02029 | Multidomain Science | Long-context scientific reasoning | long-context, information extraction, multimodal | CURIE is a benchmark of 580 problems across six scientific disciplines—materials science, quantum computing, biology, chemistry, climate science, and astrophysics— designed to evaluate LLMs on long-context understanding, reasoning, and information  extraction in realistic scientific workflows.  | Information extraction, Reasoning, Concept tracking, Aggregation, Algebraic manipulation, Multimodal comprehension | Long-context understanding and scientific reasoning | Accuracy |  | Good | @misc{curie2024, title={Scientific Reasoning Benchmarks from the CURIE Dataset}, author={TODO: Add authors}, year={2024}, url={https://arxiv.org/abs/2404.02029} } |
| 2023-01-26 | None | yes | FEABench (Finite Element Analysis Benchmark) | https://github.com/alleninstitute/feabench | Computational Engineering | FEA simulation accuracy & performance | finite element, simulation, PDE | FEABench is a suite evaluating finite element analysis tools on standardized  PDE-based simulation tasks with complex geometries and boundary conditions,  measuring both accuracy and runtime performance.  | Simulation, Performance evaluation | Numerical simulation accuracy and efficiency | Solve time, Error norm | FEniCS, deal.II | Good | @misc{allen2023feabench, title={FEABench: A Finite Element Analysis Benchmark}, author={Allen Institute}, year={2023}, url={https://github.com/alleninstitute/feabench} } |
| 2024-07-12 | None | yes | SPIQA (Scientific Paper Image Question Answering) | https://arxiv.org/abs/2407.09413 | Computer Science | Multimodal QA on scientific figures | multimodal QA, figure understanding, table comprehension, chain-of-thought | SPIQA assesses AI models' ability to interpret and answer questions about figures and tables in scientific papers by integrating visual and textual modalities  with chain-of-thought reasoning.  | Question answering, Multimodal QA, Chain-of-Thought evaluation | Visual-textual reasoning in scientific contexts | Accuracy, F1 score | Chain-of-Thought models, Multimodal QA systems | Good | @article{zhong2024spiqa, title={SPIQA: Scientific Paper Image Question Answering}, author={Zhong, Xiaoyan and Gao, Yijian and Gururangan, Suchin}, year={2024}, url={https://arxiv.org/abs/2407.09413} } |
| 2020-09-28 | None | yes | MedQA | https://arxiv.org/abs/2009.13081 | Medical Question Answering | Medical board exam QA | USMLE, diagnostic QA, medical knowledge, multilingual | MedQA is a large-scale multiple-choice dataset drawn from professional medical board exams (e.g., USMLE), testing AI systems on diagnostic and medical knowledge  questions in English and Chinese.  | Multiple choice | Medical diagnosis and knowledge retrieval | Accuracy | Neural reader, Retrieval-based QA systems | Multilingual (English, Simplified & Traditional Chinese) | @article{jin2020what, title={What Disease Does This Patient Have? A Large-scale Open-domain Question Answering Dataset from Medical Exams}, author={Jin, Di and Li, Ying and Zhang, Yichong and others}, year={2020}, url={https://arxiv.org/abs/2009.13081} } |
| 2025-05-13 | None | yes | BaisBench (Biological AI Scientist Benchmark) | https://arxiv.org/abs/2505.08341 | Computational Biology | Omics-driven AI research tasks | single-cell annotation, biological QA, autonomous discovery | BaisBench evaluates AI scientists' ability to perform data-driven biological research by annotating cell types in single-cell datasets and answering MCQs derived from  biological study insights, measuring autonomous scientific discovery.  | Cell type annotation, Multiple choice | Autonomous biological research capabilities | Annotation accuracy, QA accuracy | LLM-based AI scientist agents | Underperforms human experts; aims to advance AI-driven discovery | @misc{luo2025benchmarkingaiscientistsomics, title={Benchmarking AI scientists in omics data-driven biological research}, author={Luo, Erpai and Jia, Jinmeng and Xiong, Yifan and others}, year={2025}, url={https://arxiv.org/abs/2505.08341} } |
| 2023-01-26 | None | yes | MOLGEN | https://github.com/zjunlp/MolGen | Computational Chemistry | Molecular generation & optimization | SELFIES, GAN, property optimization | MolGen is a pre-trained molecular language model that generates chemically valid molecules using SELFIES and reinforcement learning, guided by chemical feedback  to optimize properties such as logP, QED, and docking score.  | Distribution learning, Goal-oriented generation | Generation of valid and optimized molecular structures | Validity%, Novelty%, QED, Docking score | MolGen | This is a model, not a benchmark | @article{fang2023domain, title={Domain-Agnostic Molecular Generation with Chemical Feedback}, author={Fang, Yin and Zhang, Ningyu and Chen, Zhuo and others}, year={2023}, url={https://arxiv.org/abs/2301.11259} } |
| 2020-05-02 | None | yes | Open Graph Benchmark (OGB) - Biology | https://ogb.stanford.edu/docs/home/ | Graph ML | Biological graph property prediction | node prediction, link prediction, graph classification | OGB-Biology is a suite of large-scale biological network datasets (protein-protein interaction, drug-target, etc.) with standardized splits and evaluation protocols  for node, link, and graph property prediction tasks.  | Node property prediction, Link property prediction, Graph property prediction | Scalability & generalization in graph ML for biology | Accuracy, ROC-AUC | GCN, GraphSAGE, GAT | Community-driven updates | @misc{hu2020ogb, title={Open Graph Benchmark: Datasets for Machine Learning on Graphs}, author={Hu, Weihua and Fey, Matthias and Zitnik, Marinka and others}, year={2020}, url={https://arxiv.org/abs/2005.00687} } |
| 2011-10-01 | None | yes | Materials Project | https://materialsproject.org/ | Materials Science | DFT-based property prediction | DFT, materials genome, high-throughput | The Materials Project provides an open-access database of computed properties for inorganic materials via high-throughput density functional theory (DFT), accelerating  materials discovery.  | Property prediction | Prediction of inorganic material properties | MAE, R² | Automatminer, Crystal Graph Neural Networks | Core component of the Materials Genome Initiative | @article{jain2013materials, title={The Materials Project: A materials genome approach...}, author={Jain, Anubhav and Ong, Shyue Ping and others}, journal={APL Materials}, year={2013}, url={https://materialsproject.org/} } |
| 2020-10-20 | None | yes | OCP (Open Catalyst Project) | https://opencatalystproject.org/ | Chemistry; Materials Science | Catalyst adsorption energy prediction | DFT relaxations, adsorption energy, graph neural networks | The Open Catalyst Project (OC20 & OC22) provides DFT-calculated catalyst-adsorbate  relaxation datasets, challenging ML models to predict energies and forces for  renewable energy applications.  | Energy prediction, Force prediction | Prediction of adsorption energies and forces | MAE (energy), MAE (force) | CGCNN, SchNet, DimeNet++, GemNet-OC | Public leaderboards; active community development | @article{chanussot2020open, title={The Open Catalyst 2020 (OC20) Dataset...}, author={Chanussot, Loïk and Das, Abhishek and others}, year={2020}, url={https://arxiv.org/abs/2010.09990} }, @article{tran2022open, title={The Open Catalyst 2022 (OC22) Dataset...}, author={Tran, Richard and Lan, Janice and others}, year={2022}, url={https://arxiv.org/abs/2206.08917} } |
| 2023-06-20 | None | yes | JARVIS-Leaderboard | https://arxiv.org/abs/2306.11688 | Materials Science; Benchmarking | Comparative evaluation of materials design methods | leaderboards, materials methods, simulation | JARVIS-Leaderboard is a community-driven platform benchmarking AI, electronic structure, force-fields, quantum computing, and experimental methods across hundreds of materials science tasks.  | Method benchmarking, Leaderboard ranking | Performance comparison across diverse materials design methods | MAE, RMSE, Accuracy |  | 1,281 contributions across 274 benchmarks | @article{choudhary2023jarvis, title={JARVIS-Leaderboard: A Large Scale Benchmark...}, author={Choudhary, Kamal and Wines, Daniel and others}, year={2023}, url={https://arxiv.org/abs/2306.11688} } |
| 2022-02-22 | None | yes | Quantum Computing Benchmarks (QML) | https://github.com/XanaduAI/qml-benchmarks, https://pennylane.ai/datasets/collection/qml-benchmarks | Quantum Computing | Quantum algorithm performance evaluation | quantum circuits, state preparation, error correction | A suite of benchmarks evaluating quantum hardware and algorithms on tasks such as state  preparation, circuit optimization, and error correction across multiple platforms.  | Circuit benchmarking, State classification | Quantum algorithm performance and fidelity | Fidelity, Success probability | IBM Q, IonQ, AQT@LBNL | Hardware-agnostic, application-level metrics | @misc{tomesh2022supermarq,...} |
| 2024-10-01 | None | yes | CFDBench (Fluid Dynamics) | https://arxiv.org/abs/2310.05963 | Fluid Dynamics; Scientific ML | Neural operator surrogate modeling | neural operators, CFD, FNO, DeepONet | CFDBench provides large-scale CFD data for four canonical fluid flow problems,  assessing neural operators' ability to generalize to unseen PDE parameters and domains.  | Surrogate modeling | Generalization of neural operators for PDEs | L2 error, MAE | FNO, DeepONet, U-Net | 302K frames across 739 cases | @misc{luo2024cfdbenchlargescalebenchmarkmachine, title={CFDBench: A Large-Scale Benchmark for Machine Learning Methods in Fluid Dynamics}, author={Luo, Yining and Chen, Yingfa and Zhang, Zhen}, year={2024}, url={https://arxiv.org/abs/2310.05963} } |
| None | None | yes | SatImgNet | None | Remote Sensing | Satellite imagery classification | land-use, zero-shot, multi-task | SATIN (sometimes referred to as SatImgNet) is a multi-task metadataset of 27 satellite imagery classification datasets evaluating zero-shot transfer of vision-language models across diverse remote sensing tasks.  | Image classification | Zero-shot land-use classification | Accuracy |  | Public leaderboard available | @article{roberts2023satin,...}, @misc{nguyen2023climatelearnbenchmarkingmachinelearning, title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover}, year={2023}, eprint={2307.01909}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2307.01909}} |
| 2023-07-19 | None | yes | ClimateLearn | https://arxiv.org/abs/2307.01909 | Climate Science; Forecasting | ML for weather & climate modeling | medium-range forecasting, ERA5, data-driven | ClimateLearn provides standardized datasets and evaluation protocols for machine  learning models in medium-range weather and climate forecasting using ERA5 reanalysis.  | Forecasting | Global weather prediction (3-5 days) | RMSE, Anomaly correlation | CNN baselines, ResNet variants | Includes physical and ML baselines. Appears to be the same as the SatImgNet entry | @misc{nguyen2023climatelearnbenchmarkingmachinelearning, title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, author={TungNguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover}, year={2023}, eprint={2307.01909}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2307.01909}} |
| 2022-06-09 | None | yes | BIG-Bench (Beyond the Imitation Game Benchmark) | https://github.com/google/BIG-bench | NLP; AI Evaluation | Diverse reasoning & generalization tasks | few-shot, multi-task, bias analysis | BIG-Bench is a collaborative suite of 204 tasks designed to probe LLMs' reasoning,  knowledge, and bias across diverse domains and difficulty levels beyond simple imitation.  | Few-shot evaluation, Multi-task evaluation | Reasoning and generalization across diverse tasks | Accuracy, Task-specific metrics | GPT-3, Dense Transformers, Sparse Transformers | Human baselines included | @misc{srivastava2023imitationgamequantifyingextrapolating, title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmüller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakaş and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bartłomiej Bojanowski and Batuhan Özyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and César Ferri Ramírez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Moseguí González and Danielle Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Martínez-Plumed and Francesca Happé and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-López and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Schütze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fernández Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Kocoń and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jörg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Colón and Luke Metz and Lütfi Kerem Şenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ramírez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and Mátyás Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Michał Swędrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Miłkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphaël Millière and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Théo Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu}, year={2023}, eprint={2206.04615}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2206.04615}, } |
| 2019-11-20 | None | yes | CommonSenseQA | https://paperswithcode.com/paper/commonsenseqa-a-question-answering-challenge | NLP; Commonsense | Commonsense question answering | ConceptNet, multiple-choice, adversarial | CommonsenseQA is a challenging multiple-choice QA dataset built from ConceptNet, requiring models to apply commonsense knowledge to select the correct answer  among five choices.  | Multiple choice | Commonsense reasoning and knowledge integration | Accuracy | BERT-large, RoBERTa, GPT-3 | Baseline 56%, human 89% | @inproceedings{talmor2019commonsenseqa,...} |
| 2019-07-24 | None | yes | Winogrande | https://leaderboard.allenai.org/winogrande/submissions/public | NLP; Commonsense | Winograd Schema-style pronoun resolution | adversarial, pronoun resolution | WinoGrande is a large-scale adversarial dataset of 44,000 Winograd Schema-style  questions with reduced bias using AFLite, serving as both a benchmark and transfer  learning resource.  | Pronoun resolution | Robust commonsense reasoning | Accuracy, AUC | RoBERTa, BERT, GPT-2 | Human ~94% | @article{sakaguchi2019winogrande,...} |
