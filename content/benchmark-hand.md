Partial progress on hand-converting the benchmark YAML table into Markdown format. Reece Shiraishi
| date | expiration | valid | name | url | domain | focus | keyword | description | task_types | ai_capability_measured | metrics | models | notes | cite |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 2020-09-07 | none | yes | MMLU (Massive Multitask Language Understanding) | https://paperswithcode.com/dataset/mmlu | multidomain | Academic knowledge and reasoning across 57 subjects | multiple-choice, zero-shot, few-shot, knowledge probing | Measuring Massive Multitask Language Understanding (MMLU) is a benchmark of 57 multiple-choice tasks covering elementary mathematics, US history, computer science, law, and more, designed to evaluate a model’s breadth and depth of knowledge in zero-shot and few-shot settings. | Multiple-choice | General reasoning, subject-matter understanding | Accuracy | GPT-4o, Gemini 1.5 Pro, o1, DeepSeek-R1 | Good | "@article{hendrycks2021measuring, title={Measuring Massive Multitask Language Understanding}, author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and others}, journal={arXiv preprint arXiv:2009.03300}, year={2021}, url={https://arxiv.org/abs/2009.03300}}"
| 2023-11-20 | none | yes | GPQA Diamond | https://arxiv.org/abs/2311.12022 | science | Graduate-level scientific reasoning | Google-proof, graduate-level, science QA, chemistry, physics | GPQA is a dataset of 448 challenging, multiple-choice questions in biology, physics,and chemistry, written by domain experts. It is “Google-proof”—experts score 65% (74% after error correction) while skilled non-experts with web access score only 34%. State-of-the-art LLMs like GPT-4 reach around 39% accuracy. | Multiple choice, Multi-step QA | Scientific reasoning, deep knowledge | Accuracy | o1, DeepSeek-R1 | Good | "@misc{rein2023gpqagraduatelevelgoogleproofqa, title={GPQA: A Graduate-Level Google-Proof Q&A Benchmark}, author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and others}, year={2023},url={https://arxiv.org/abs/2311.12022}}"
| 2018-03-14 | none | yes | ARC-Challenge (Advanced Reasoning Challenge) | https://allenai.org/data/arc | science | Grade-school science with reasoning emphasis | grade-school, science QA, challenge set, reasoning | The AI2 Reasoning Challenge (ARC) Challenge set comprises 7,787 natural, grade-school science questions that retrieval-based and word co-occurrence algorithms both fail, requiring advanced reasoning over a 14-million-sentence corpus. | Multiple choice | Commonsense and scientific reasoning | Accuracy | GPT-4, Claude | Good | "@inproceedings{clark2018think, title={Think you have solved question answering? Try ARC, the AI2 Reasoning Challenge}, author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and others}, booktitle={EMNLP 2018},pages={237–248}, year={2018}, url={https://allenai.org/data/arc}}"
| 2025-01-24 | none | yes | Humanity's Last Exam | https://arxiv.org/abs/2501.14249 | Multidomain | Broad cross-domain academic reasoning | cross-domain, academic exam, multiple-choice, multidisciplinary | Humanity's Last Exam is a multi-domain, multiple-choice benchmark containing 2,000 questions across diverse academic disciplines, designed to evaluate LLMs' ability to reason across domains without external resources. | Multiple-choice | Cross-domain academic reasoning | Accuracy | none | Good | "@misc{phan2025humanitys, title={Humanity's Last Exam}, author={Phan, Long and Gatti, Alice and Han, Ziwen and others}, year={2025}, url={https://arxiv.org/abs/2501.14249}}"
| 2024-11-07 | none | yes | FrontierMath | https://arxiv.org/abs/2411.04872 | Math | Challenging advanced mathematical reasoning | symbolic reasoning, number theory, algebraic geometry, category theory | FrontierMath is a benchmark of hundreds of expert-vetted mathematics problems spanning number theory, real analysis, algebraic geometry, and category theory, measuring LLMs' ability to solve problems requiring deep abstract reasoning. | Problem solving | Symbolic and abstract mathematical reasoning | Accuracy | none | Good | "@misc{glazer2024frontiermath, title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI}, author={Glazer, Elliot and Erdil, Ege and Besiroglu, Tamay and others}, year={2024}, url={https://arxiv.org/abs/2411.04872}}"
| 2024-07-18 | none | yes | SciCode | https://arxiv.org/abs/2407.13168 | scientific programming | Scientific code generation & problem solving | code synthesis,  scientific computing, programming benchmark | SciCode is a scientist-curated coding benchmark with 338 subproblems derived from 80 real research tasks across 16 scientific subfields, evaluating models on knowledge recall, reasoning, and code synthesis for scientific computing tasks. | Coding | Program synthesis, scientific computing | Solve rate (%) | Claude 3.5 Sonnet | Good | "@misc{tian2024scicode, title={SciCode: A Research Coding Benchmark Curated by Scientists}, author={Tian, Minyang and Gao, Luyu and Zhang, Shizhuo and others}, year={2024}, url={https://arxiv.org/abs/2407.13168}}"
| 2025-03-13 | none | yes | AIME (American Invitational Mathematics Examination) | https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions | Math | Pre-college advanced problem solving | algebra, combinatorics, number theory, geometry | The AIME is a 15-question, 3-hour exam for high-school students featuring challenging short-answer math problems in algebra, number theory, geometry, and combinatorics, assessing depth of problem-solving ability.