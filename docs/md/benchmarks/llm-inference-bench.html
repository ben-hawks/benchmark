<h1 id="llm-inference-bench">LLM-Inference-Bench</h1>
<p><strong>Edit:</strong> <a
href="https://github.com/mlcommons-science/benchmark/tree/main/source">edit
this entry</a></p>
<p><strong>Date</strong>: 2024-10-31</p>
<p><strong>Name</strong>: LLM-Inference-Bench</p>
<p><strong>Domain</strong>: LLM; HPC/inference</p>
<p><strong>Focus</strong>: Hardware performance benchmarking of LLMs on
AI accelerators</p>
<p><strong>Keywords</strong>: LLM, inference benchmarking, GPU,
accelerator, throughput</p>
<p><strong>Task Types</strong>: Inference Benchmarking</p>
<p><strong>Metrics</strong>: Token throughput tok/s , Latency,
Framework-hardware mix performance</p>
<p><strong>Models</strong>: LLaMA-2-7B, LLaMA-2-70B, Mistral-7B,
Qwen-7B</p>
<p><strong>Citation</strong>:</p>
<ul>
<li><p>Krishna Teja Chitty-Venkata, Siddhisanket Raskar, Bharat Kale,
Farah Ferdaus, Aditya Tanikanti, Ken Raffenetti, Valerie Taylor, Murali
Emani, and Venkatram Vishwanath. Llm-inference-bench: inference
benchmarking of large language models on ai accelerators. In SC24-W:
Workshops of the International Conference for High Performance
Computing, Networking, Storage and Analysis, volume, 1362 1379. 2024.
doi:10.1109/SCW63240.2024.00178.</p>
<ul>
<li><p>bibtex: ``` <span class="citation"
data-cites="INPROCEEDINGS">@INPROCEEDINGS</span>{10820566,</p>
<pre><code>author={Chitty-Venkata, Krishna Teja and Raskar, Siddhisanket and Kale, Bharat and Ferdaus, Farah and Tanikanti, Aditya and Raffenetti, Ken and Taylor, Valerie and Emani, Murali and Vishwanath, Venkatram},

booktitle={SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 

title={LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators}, 

year={2024},

volume={},

number={},

pages={1362-1379},

keywords={Performance evaluation;Power demand;Computational modeling;Large language models;Scalability;High performance computing;AI accelerators;Benchmark testing;Propulsion;Throughput;Large Language Models;AI Accelerators;Inference Performance Evaluation;Benchmarking},

doi={10.1109/SCW63240.2024.00178}}</code></pre>
<p>}</p>
<p>```</p></li>
</ul></li>
</ul>
<p><strong>Ratings:</strong></p>
<p>Software:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Public GitHub repository
https://github.com/argonne-lcf/LLM-Inference-Bench under BSD-3 license.
Includes scripts, configurations, and dashboards for running and
visualizing LLM inference benchmarks across multiple accelerator
platforms.</p></li>
</ul>
<p>Specification:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Benchmark scope, models, accelerator
targets, and supported frameworks are clearly specified. Input
configurations and output metrics are standardized across hardware
types.</p></li>
</ul>
<p>Dataset:</p>
<ul>
<li><p><strong>Rating:</strong> 2</p></li>
<li><p><strong>Reason:</strong> No novel dataset is introduced;
benchmark relies on pre-trained LLMs and synthetic inference inputs.
Dataset structure and FAIR considerations are minimal.</p></li>
</ul>
<p>Metrics:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Hardware-specific metrics token
throughput, latency, utilization are well-defined, consistently
measured, and aggregated in dashboards.</p></li>
</ul>
<p>Reference Solution:</p>
<ul>
<li><p><strong>Rating:</strong> 3</p></li>
<li><p><strong>Reason:</strong> Inference configurations and baseline
performance results are provided, but there are no full reference
training pipelines or model implementations.</p></li>
</ul>
<p>Documentation:</p>
<ul>
<li><p><strong>Rating:</strong> 4</p></li>
<li><p><strong>Reason:</strong> GitHub repo provides clear usage
instructions, setup guides, and interactive dashboard tooling. Some
areas like benchmarking extensions or advanced tuning are less
detailed.</p></li>
</ul>
<p><strong>Average Rating:</strong> 4.0</p>
<p><strong>Radar Plot:</strong> <img
src="../../tex/images/llm-inference-bench_radar.png"
alt="Llm-Inference-Bench radar plot" /></p>
