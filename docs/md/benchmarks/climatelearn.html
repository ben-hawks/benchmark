<h1 id="climatelearn">ClimateLearn</h1>
<p><strong>Date</strong>: 2023-07-19</p>
<p><strong>Name</strong>: ClimateLearn</p>
<p><strong>Domain</strong>: Climate Science; Forecasting</p>
<p><strong>Focus</strong>: ML for weather and climate modeling</p>
<p><strong>Keywords</strong>: medium-range forecasting, ERA5,
data-driven</p>
<p><strong>Task Types</strong>: Forecasting</p>
<p><strong>Metrics</strong>: RMSE, Anomaly correlation</p>
<p><strong>Models</strong>: CNN baselines, ResNet variants</p>
<p><strong>Citation</strong>:</p>
<ul>
<li><p>Tung Nguyen, Jason Jewik, Hritik Bansal, Prakhar Sharma, and
Aditya Grover. Climatelearn: benchmarking machine learning for weather
and climate modeling. 2023. URL: https://arxiv.org/abs/2307.01909,
arXiv:2307.01909.</p>
<ul>
<li><p>bibtex: ``` <span class="citation"
data-cites="misc">@misc</span>{nguyen2023climatelearnbenchmarkingmachinelearning,</p>
<pre><code>title={ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling}, 

author={Tung Nguyen and Jason Jewik and Hritik Bansal and Prakhar Sharma and Aditya Grover},

year={2023}, eprint={2307.01909}, 

archivePrefix={arXiv}, 

primaryClass={cs.LG},

url={https://arxiv.org/abs/2307.01909}</code></pre>
<p>}</p>
<p>```</p></li>
</ul></li>
</ul>
<p><strong>Ratings:</strong></p>
<p>Software:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Quickstart notebook makes for easy
usage</p></li>
</ul>
<p>Specification:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Task framing medium-range climate
forecasting , input/output formats, and evaluation windows are clearly
defined; benchmark supports both physical and learned models with
detailed constraints.</p></li>
</ul>
<p>Dataset:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Provides standardized access to ERA5 and
other reanalysis datasets, with ML-ready splits, metadata, and
Xarray-compatible formats; versioned and fully FAIR-compliant.</p></li>
</ul>
<p>Metrics:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> ACC and RMSE are standard, quantitative,
and appropriate for climate forecasting; well-integrated into the
benchmark, though interpretation across domains may vary.</p></li>
</ul>
<p>Reference Solution:</p>
<ul>
<li><p><strong>Rating:</strong> 0</p></li>
<li><p><strong>Reason:</strong> The benchmark is geared for CNN
architectures, but no specific model was mentioned.</p></li>
</ul>
<p>Documentation:</p>
<ul>
<li><p><strong>Rating:</strong> 5</p></li>
<li><p><strong>Reason:</strong> Explained in the benchmarkâ€™s
paper.</p></li>
</ul>
<p><strong>Average Rating:</strong> 4.167</p>
<p><strong>Radar Plot:</strong> <img
src="../../tex/images/climatelearn_radar.png"
alt="Climatelearn radar plot" /></p>
